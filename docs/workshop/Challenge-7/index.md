# Evaluation

## Content Safety Evaluation

This notebook demonstrates how to evaluate content safety using Azure AI's evaluation tools. It includes steps to:
- Simulate adversarial scenarios.
- Evaluate content for safety metrics such as violence, sexual content, hate/unfairness, and self-harm.
- Generate evaluation reports in JSON format.

### Prerequisites
- Azure AI project credentials.
- Python environment with required libraries installed (`azure-ai-evaluation`, `pandas`, etc.).
- Access to the Azure API endpoint.

Follow the steps in this notebook to perform content safety evaluations and generate detailed reports.
