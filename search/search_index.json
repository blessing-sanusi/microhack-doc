{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Microhack Challenges Hands-On Lab : Knowledge Mining","text":"<p>The Micohack event is designed to engage technical roles through a condensed, half-day hands-on hack experience. Leveraging the latest Microsoft technologies, this event provides participants with the opportunity to work on real-world problems, collaborate with peers, and explore innovative solutions. </p> <p>The Microhack event is divided into several key challenges, each carefully crafted to test and expand the participants' proficiency with Microsoft's suite of tools. These challenges are not only technical in nature but also reflect real-world scenarios that businesses face, providing a comprehensive understanding of how to apply theoretical knowledge practically. </p>"},{"location":"#hack-duration-2-hours","title":"Hack Duration: 2 hours","text":"<p>The event kicks off with an initial overview of the customer scenario for the business problem the participants will solve by leveraging cutting-edge technology and services.  </p> <p>Following this, the team will complete the setup phase, where participants ensure that their development environments are correctly configured, and all necessary tools are ready for use.  </p> <p>Finally, they will tackle the first challenge, which involves identifying key ideas that underpin the implementation of Microsoft technologies in solving predefined problems. </p>"},{"location":"workshop/","title":"Index","text":"<p>This solution accelerator enables customers with large amounts of conversational data to improve decision-making by leveraging intelligence to uncover insights, relationships, and patterns from customer interactions. It empowers users to gain valuable knowledge and drive targeted business impact. </p> <p>It leverages Azure AI Foundry, Azure AI Content Understanding, Azure OpenAI Service, and Azure AI Search to transform large volumes of conversational data into actionable insights through topic modeling, key phrase extraction, speech-to-text transcription, and interactive chat experiences.</p>"},{"location":"workshop/#use-case-scenario","title":"Use case / scenario","text":"<p>An analyst managing large volumes of conversational data needs a solution to visualize key insights and uncover patterns using natural language. An interactive dashboard enables them to explore rich, actionable insights for faster, and more informed decision-making.</p>"},{"location":"workshop/#technical-key-features","title":"Technical key features","text":""},{"location":"workshop/00-Use-Case-Scenerio/","title":"Customer Scenerio","text":""},{"location":"workshop/00-Use-Case-Scenerio/#background","title":"Background","text":"<p>Meet Alex, an analyst at InnovateTech, a leading company in the tech industry. InnovateTech prides itself on delivering exceptional customer experiences, but they are facing a significant business challenge. The company has been receiving an overwhelming amount of conversational data from various sources, including customer interactions, support tickets, and social media channels. This data holds valuable insights that could help InnovateTech understand customer sentiment, identify emerging trends, and make strategic decisions to drive growth. However, the sheer volume of data is making it nearly impossible for Alex to extract meaningful insights quickly and efficiently. </p>"},{"location":"workshop/00-Use-Case-Scenerio/#business-problem","title":"Business Problem","text":"<p>The leadership team at InnovateTech has tasked Alex with uncovering these insights to inform their decision-making process. They need to understand what customers are saying, how they feel about the company's products and services, and what trends are emerging in the market. This information is crucial for InnovateTech to stay ahead of the competition and continue delivering top-notch customer experiences. </p>"},{"location":"workshop/00-Use-Case-Scenerio/#technical-problem","title":"Technical Problem","text":"<p>However, Alex is facing a technical problem. The current tools at Alex's disposal are cumbersome and time-consuming. Traditional methods of analysis involve manually sifting through data, creating complex queries, and generating static reports. These methods are not only inefficient but also fail to provide the real-time, contextualized insights that InnovateTech needs to make informed decisions. </p>"},{"location":"workshop/00-Use-Case-Scenerio/#goals","title":"Goals","text":"<p>Enter the Interactive Insights Dashboard, a cutting-edge tool designed to transform the way analysts like Alex work. This dashboard leverages advanced natural language processing capabilities to handle large volumes of data and provide meaningful visualizations. With the Interactive Insights Dashboard, Alex can explore rich, actionable insights through an intuitive and interactive interface. The dashboard allows Alex to ask questions and receive real-time, contextualized responses, empowering Alex to make faster, more informed decisions. </p> <p>The solution streamlines problem-solving by providing a centralized platform where data-driven insights are easily accessible and shareable. It enhances collaboration among team members, fostering innovation and enabling InnovateTech to stay ahead of the competition. Additionally, the dashboard includes robust security features to ensure the protection of sensitive data, addressing any concerns about data security. </p>"},{"location":"workshop/Challenge-0/","title":"Prerequisites","text":"<p>To get started, make sure you have the following resources and permissions:</p> <ul> <li>An Azure subscription. If you don't have an Azure subscription, create a free account before you begin.</li> <li> <p>An Azure AI Foundry hub is required to manage the resources provisioned in your Content Understanding project, and it must be created in one of the following supported regions: westus, swedencentral, or australiaeast. If you're creating a hub for the first time, see How to create and manage an Azure AI Foundry hub to learn more. It's important to note you need the proper permissions to create a hub, or your admin may create one for you.</p> <ul> <li>If your role is Contributor or Owner, you can proceed with creating your own hub.</li> <li>If your role is Azure AI Developer, the hub must already be created before you can complete this quickstart. Your user role must be Azure AI Developer, Contributor, or Owner on the hub. For more information, see hubs and Azure AI roles.</li> </ul> </li> </ul>"},{"location":"workshop/Challenge-0/CU-Challenge/","title":"Create your first Content Understanding project in the AI Foundry","text":""},{"location":"workshop/Challenge-0/CU-Challenge/#step-1-create-a-content-understanding-project","title":"Step 1: Create a Content Understanding Project","text":"<ul> <li>Navigate to the AI Foundry homepage.</li> </ul> <ul> <li>Select Create a new Content Understanding Project.</li> </ul> <ul> <li>In the project creation wizard:<ul> <li>Choose the hub you created.</li> <li>Confirm that an AI Services resource and a blob storage container are provisioned (these are selected by default).</li> </ul> </li> </ul> <ul> <li>Complete the setup and click Create project.</li> </ul>"},{"location":"workshop/Challenge-0/CU-Challenge/#step-2-build-your-first-analyzer","title":"Step 2: Build Your First Analyzer","text":"<ul> <li> <p>Upload a Sample File: Begin by uploading a sample document (e.g., an invoice). </p> </li> <li> <p>Select an Analyzer Template: Based on the content type, Azure will suggest relevant analyzer templates. For this example, choose the Document analysis template. </p> </li> <li> <p>Define Your Schema:</p> <ul> <li>Add fields such as <code>vendorName</code>, <code>items</code>, and <code>price</code>.</li> <li>Specify the data type for each field (e.g., string, number).</li> <li>Optionally, provide descriptions to clarify each field\u2019s purpose.</li> </ul> </li> <li> <p>Save the Schema: Once your schema is complete, click Save schema. </p> </li> <li> <p>With the completed schema, Content Understanding now generates the output on your sample data. At this step, you can add more data to test the analyzer's accuracy or make changes to the schema if needed. </p> </li> <li> <p>Build the Analyzer: Click Build analyzer to generate your custom analyzer. The process may take a few moments. Once complete, you\u2019ll receive a unique Analyzer ID.</p> </li> </ul> <p>\ud83e\uddea Use this ID to test your analyzer or integrate it into applications via API.</p>"},{"location":"workshop/Challenge-0/CU-Challenge/#step-3-share-and-manage-access","title":"Step 3: Share and Manage Access","text":"<p>To manage access to your Content Understanding project:</p> <ul> <li> <p>Navigate to the Management Center located at the bottom of your project's navigation pane. </p> </li> <li> <p>Here, you can:</p> <ul> <li>Add or remove users.</li> <li>Assign roles to users to control their level of access.</li> </ul> </li> </ul> <p>Image: Management Center interface </p>"},{"location":"workshop/Challenge-0/CU-Challenge/#next-steps","title":"Next Steps","text":"<p>Now that you've created your first analyzer, consider the following:</p> <ul> <li>Test the Analyzer: Upload additional sample documents to evaluate the accuracy and performance of your analyzer.</li> <li>Integrate with Applications: Use the generated analyzer ID to call your analyzer via the REST API in your applications.</li> <li>Explore More Templates: The Azure AI Foundry offers a variety of analyzer templates for different content types. Explore these to find solutions tailored to your needs.</li> </ul> <p>For more detailed information and advanced configurations, refer to the official Azure AI Content Understanding documentation.</p>"},{"location":"workshop/Challenge-1/","title":"Explore Data","text":"<p>The dataset contains customer interactions categorized into various topics. Each entry includes:</p> <ul> <li>ConversationId: Unique identifier for each conversation.</li> <li>StartTime and EndTime: Timestamps for when the conversation occurred.</li> <li>Content: Full transcript of the conversation.</li> <li>Summary: A concise summary of the interaction.</li> <li>Sentiment: Sentiment analysis of the conversation (e.g., Positive, Negative).</li> <li>Topic: The main topic of the conversation.</li> <li>Key Phrases: Extracted key phrases for quick insights.</li> <li>Complaint: Specific complaints raised by the customer (if any).</li> <li>Mined Topic: Categorized topic based on the conversation.</li> </ul>"},{"location":"workshop/Challenge-1/#2-data-flow","title":"2. Data Flow","text":""},{"location":"workshop/Challenge-1/#step-1-audio-to-text","title":"Step 1: Audio to Text","text":"<ul> <li>Audio conversations are transcribed into text using speech-to-text technology.</li> <li>The transcription captures the full conversation, including timestamps and speaker identification.</li> </ul>"},{"location":"workshop/Challenge-1/#step-2-text-analysis","title":"Step 2: Text Analysis","text":"<ul> <li>Sentiment Analysis: Determines the overall sentiment of the conversation (Positive or Negative).</li> <li>Topic Mining: Identifies the main topic of the conversation (e.g., Billing Issues, Device Troubleshooting).</li> <li>Key Phrase Extraction: Highlights important phrases for quick insights.</li> <li>Complaint Identification: Extracts specific complaints raised by the customer.</li> </ul>"},{"location":"workshop/Challenge-1/#step-3-structuring-the-data","title":"Step 3: Structuring the Data","text":"<ul> <li>The analyzed data is structured into JSON format for easy querying and visualization.</li> </ul>"},{"location":"workshop/Challenge-1/#3-key-insights","title":"3. Key Insights","text":""},{"location":"workshop/Challenge-1/#frequent-topics","title":"Frequent Topics","text":"<ol> <li> <p>Account Management:</p> <ul> <li>Includes account updates, voicemail setup, and call forwarding.</li> <li>Example: Updating address and email information.</li> </ul> </li> <li> <p>Billing Issues:</p> <ul> <li>Covers billing discrepancies, payment failures, and refunds.</li> <li>Example: Resolving double charges or unexpected fees.</li> </ul> </li> <li> <p>Service Activation:</p> <ul> <li>Assistance with activating new services, SIM cards, or scheduling appointments.</li> <li>Example: Activating a new SIM card or setting up international roaming.</li> </ul> </li> <li> <p>Device Troubleshooting:</p> <ul> <li>Support for device-related issues like freezing, battery drain, or hardware malfunctions.</li> <li>Example: Factory reset for a tablet or troubleshooting a phone's battery.</li> </ul> </li> <li> <p>Internet Connectivity:</p> <ul> <li>Troubleshooting slow or unreliable internet connections.</li> <li>Example: Diagnosing slow speeds and resetting modems.</li> </ul> </li> <li> <p>Parental Controls:</p> <ul> <li>Setting up and managing parental controls for children\u2019s devices.</li> <li>Example: Configuring screen time limits and app restrictions.</li> </ul> </li> </ol>"},{"location":"workshop/Challenge-1/#sentiment-analysis","title":"Sentiment Analysis","text":"<ul> <li>Positive Sentiment: Majority of interactions indicate customer satisfaction with the support provided.</li> <li>Negative Sentiment: Often linked to unresolved billing issues or technical difficulties.</li> </ul>"},{"location":"workshop/Challenge-1/#common-complaints","title":"Common Complaints","text":"<ul> <li>Billing discrepancies (e.g., double charges, unexpected fees).</li> <li>Internet speed issues.</li> <li>Device malfunctions (e.g., freezing, battery drain).</li> </ul>"},{"location":"workshop/Challenge-1/#4-analyzer-workflow","title":"4. Analyzer Workflow","text":""},{"location":"workshop/Challenge-1/#step-1-sentiment-analysis","title":"Step 1: Sentiment Analysis","text":"<ul> <li>Uses Natural Language Processing (NLP) to classify the sentiment as Positive or Negative.</li> <li>Example: \"Thank you for your help\" \u2192 Positive sentiment.</li> </ul>"},{"location":"workshop/Challenge-1/#step-2-topic-mining","title":"Step 2: Topic Mining","text":"<ul> <li>Identifies the main topic of the conversation using keyword matching and clustering.</li> <li>Example: Keywords like \"billing,\" \"charges,\" and \"refund\" \u2192 Topic: Billing Issues.</li> </ul>"},{"location":"workshop/Challenge-1/#step-3-key-phrase-extraction","title":"Step 3: Key Phrase Extraction","text":"<ul> <li>Extracts important phrases using NLP techniques like Named Entity Recognition (NER).</li> <li>Example: \"slow Internet speed,\" \"paperless billing,\" \"factory reset.\"</li> </ul>"},{"location":"workshop/Challenge-1/#step-4-complaint-identification","title":"Step 4: Complaint Identification","text":"<ul> <li>Searches for explicit complaints in the conversation.</li> <li>Example: \"My bill was $50 higher than usual\" \u2192 Complaint: Higher bill.</li> </ul>"},{"location":"workshop/Challenge-1/Analyzer/","title":"Explore Data","text":""},{"location":"workshop/Challenge-1/Analyzer/#analyzer-configuration-summary-text-and-audio-analyzers","title":"Analyzer Configuration Summary: Text and Audio Analyzers","text":"<p>This document provides a summary of the <code>ckm-analyzer_config_text.json</code> and <code>ckm-analyzer_config_audio.json</code> files, which define configurations for analyzing both text-based and audio-based call center conversations. These analyzers extract actionable insights such as sentiment, satisfaction, topics, and more.</p>"},{"location":"workshop/Challenge-1/Analyzer/#text-analyzer-ckm-analyzer_config_textjson","title":"Text Analyzer: <code>ckm-analyzer_config_text.json</code>","text":""},{"location":"workshop/Challenge-1/Analyzer/#overview","title":"Overview","text":"<ul> <li>Analyzer ID: <code>ckm-analyzer-text</code></li> <li>Scenario: <code>text</code> (processes textual data).</li> <li>Description: \"Conversation analytics\" \u2014 focuses on analyzing call center conversations.</li> <li>Tags:</li> <li><code>templateId</code>: <code>postCallAnalytics-2024-12-01</code> (template version for post-call analytics).</li> </ul>"},{"location":"workshop/Challenge-1/Analyzer/#configuration","title":"Configuration","text":"<ul> <li><code>returnDetails</code>: <code>true</code>   Returns detailed results for each analysis.</li> </ul>"},{"location":"workshop/Challenge-1/Analyzer/#field-schema","title":"Field Schema","text":"<p>The text analyzer processes the following fields:</p> <ol> <li><code>content</code>: Full text of the conversation.</li> <li><code>Duration</code>: Duration of the conversation in seconds.</li> <li><code>summary</code>: Summarized version of the conversation.</li> <li><code>satisfied</code>: Whether the customer was satisfied (<code>Yes</code> or <code>No</code>).</li> <li><code>sentiment</code>: Overall sentiment (<code>Positive</code>, <code>Neutral</code>, <code>Negative</code>).</li> <li><code>topic</code>: Primary topic of the conversation in six words or less.</li> <li><code>keyPhrases</code>: Top 10 key phrases as a comma-separated string.</li> <li><code>complaint</code>: Primary complaint in three words or less.</li> </ol>"},{"location":"workshop/Challenge-1/Analyzer/#audio-analyzer-ckm-analyzer_config_audiojson","title":"Audio Analyzer: <code>ckm-analyzer_config_audio.json</code>","text":""},{"location":"workshop/Challenge-1/Analyzer/#overview_1","title":"Overview","text":"<ul> <li>Analyzer ID: <code>ckm-analyzer</code></li> <li>Scenario: <code>conversation</code> (processes audio-based conversations).</li> <li>Description: \"Conversation process\" \u2014 focuses on analyzing call center conversations.</li> <li>Tags:</li> <li><code>templateId</code>: <code>postCallAnalytics-2024-12-01</code> (template version for post-call analytics).</li> </ul>"},{"location":"workshop/Challenge-1/Analyzer/#configuration_1","title":"Configuration","text":"<ul> <li><code>returnDetails</code>: <code>false</code>   Returns summarized results only.</li> <li><code>locales</code>: <code>[\"en-US\"]</code>   Supports English (US) for analysis.</li> </ul>"},{"location":"workshop/Challenge-1/Analyzer/#field-schema_1","title":"Field Schema","text":"<p>The audio analyzer processes the same fields as the text analyzer:</p> <ol> <li><code>content</code>: Full text of the conversation.</li> <li><code>Duration</code>: Duration of the conversation in seconds.</li> <li><code>summary</code>: Summarized version of the conversation.</li> <li><code>satisfied</code>: Whether the customer was satisfied (<code>Yes</code> or <code>No</code>).</li> <li><code>sentiment</code>: Overall sentiment (<code>Positive</code>, <code>Neutral</code>, <code>Negative</code>).</li> <li><code>topic</code>: Primary topic of the conversation in six words or less.</li> <li><code>keyPhrases</code>: Top 10 key phrases as a comma-separated string.</li> <li><code>complaint</code>: Primary complaint in three words or less.</li> </ol>"},{"location":"workshop/Challenge-1/Analyzer/#use-cases","title":"Use Cases","text":""},{"location":"workshop/Challenge-1/Analyzer/#text-analyzer","title":"Text Analyzer","text":"<ul> <li>Purpose: Processes text-based call center conversations to extract insights.</li> <li>Use Case: Analyze chat logs or transcribed conversations to identify trends, customer satisfaction, and key topics.</li> </ul>"},{"location":"workshop/Challenge-1/Analyzer/#audio-analyzer","title":"Audio Analyzer","text":"<ul> <li>Purpose: Processes audio-based call center conversations by converting them into text for analysis.</li> <li>Use Case: Analyze recorded calls to extract insights such as sentiment, satisfaction, and complaints.</li> </ul>"},{"location":"workshop/Challenge-1/Analyzer/#how-they-fit-into-the-solution","title":"How They Fit Into the Solution","text":"<ol> <li> <p>Data Input:</p> <ul> <li>The text analyzer processes chat logs or transcribed conversations.</li> <li>The audio analyzer processes recorded calls and converts them into text.</li> </ul> </li> <li> <p>Data Output:</p> <ul> <li>Both analyzers generate structured insights (e.g., sentiment, satisfaction, topics) for visualization.</li> </ul> </li> <li> <p>Integration:</p> <ul> <li>Outputs are consumed by the backend (<code>function_app.py</code>) to populate charts.</li> <li>Insights are displayed in the frontend (<code>Chart.tsx</code>) as visualizations like Donut Charts, Word Clouds, and Tables.</li> </ul> </li> </ol>"},{"location":"workshop/Challenge-2/","title":"Explore Code","text":"<p>The Conversation Knowledge Mining Solution Accelerator is a robust application designed to extract actionable insights from conversational data. It leverages Azure AI services and provides an interactive user interface for querying and visualizing data. The solution is built with a modular architecture, combining a React-based frontend, a FastAPI backend, and Azure services for data processing and storage.</p>"},{"location":"workshop/Challenge-2/#key-features","title":"Key Features","text":""},{"location":"workshop/Challenge-2/#data-processing-and-analysis","title":"Data Processing and Analysis:","text":"<ul> <li>Processes conversational data using Azure AI Foundry, Azure AI Content Understanding, and Azure OpenAI Service.</li> <li>Extracts insights such as sentiment, key phrases, and topics from conversations.</li> <li>Supports speech-to-text transcription for audio data.</li> </ul>"},{"location":"workshop/Challenge-2/#dynamic-dashboard","title":"Dynamic Dashboard:","text":"<ul> <li>Visualizes insights through various chart types (e.g., Donut Chart, Bar Chart, Word Cloud).</li> <li>Enables filtering and customization of data views.</li> <li>Provides a responsive layout for seamless user experience.</li> </ul>"},{"location":"workshop/Challenge-2/#interactive-chat-interface","title":"Interactive Chat Interface:","text":"<ul> <li>Allows users to query data in natural language and receive real-time responses.</li> <li>Supports both text-based and chart-based responses.</li> <li>Integrates with Azure OpenAI and Azure Cognitive Search for generating responses and retrieving relevant data.</li> </ul>"},{"location":"workshop/Challenge-2/#backend-api","title":"Backend API:","text":"<ul> <li>Built with FastAPI for handling requests and integrating with Azure services.</li> <li>Includes modular routes for backend operations and conversation history management.</li> <li>Provides a health check endpoint for monitoring service status.</li> </ul>"},{"location":"workshop/Challenge-2/#scalable-deployment","title":"Scalable Deployment:","text":"<ul> <li>Supports deployment via GitHub Codespaces, VS Code Dev Containers, or local environments.</li> <li>Includes configurable deployment settings for regions, models, and resource capacities.</li> </ul>"},{"location":"workshop/Challenge-2/Api/","title":"API Overview","text":""},{"location":"workshop/Challenge-2/Api/#key-endpoints","title":"Key Endpoints","text":""},{"location":"workshop/Challenge-2/Api/#chart-data","title":"Chart Data","text":"<ol> <li> <p><code>GET /api/fetchChartData</code>:</p> <ul> <li>Fetches default chart data.</li> </ul> </li> <li> <p><code>POST /api/fetchChartDataWithFilters</code>:</p> <ul> <li>Fetches chart data based on user-selected filters.</li> </ul> </li> <li> <p><code>GET /api/fetchFilterData</code>:</p> <ul> <li>Retrieves available filter options.</li> </ul> </li> </ol>"},{"location":"workshop/Challenge-2/Api/#chatbot","title":"Chatbot","text":"<ol> <li><code>POST /api/chat</code>:<ul> <li>Processes user queries and generates responses.</li> <li>Supports dynamic chart generation.</li> </ul> </li> </ol>"},{"location":"workshop/Challenge-2/Api/#conversation-history","title":"Conversation History","text":"<ol> <li> <p><code>POST /history/generate</code>:</p> <ul> <li>Creates a new conversation or appends messages to an existing one.</li> </ul> </li> <li> <p><code>POST /history/update</code>:</p> <ul> <li>Updates an existing conversation with new messages or feedback.</li> </ul> </li> <li> <p><code>GET /history/list</code>:</p> <ul> <li>Lists all conversations for a user.</li> </ul> </li> <li> <p><code>POST /history/read</code>:</p> <ul> <li>Retrieves a specific conversation and its messages.</li> </ul> </li> <li> <p><code>DELETE /history/delete</code>:</p> <ul> <li>Deletes a specific conversation.</li> </ul> </li> </ol>"},{"location":"workshop/Challenge-2/Api/#tools-and-libraries","title":"Tools and Libraries","text":"<ul> <li>Quart: For building API endpoints.</li> <li>Azure OpenAI: For generating insights.</li> <li>CosmosDB: For managing conversation history.</li> </ul>"},{"location":"workshop/Challenge-2/Api/#how-it-fits","title":"How It Fits","text":"<p>The API layer connects the frontend and backend, enabling seamless communication and data exchange.</p>"},{"location":"workshop/Challenge-2/Backend/","title":"Backend Overview","text":""},{"location":"workshop/Challenge-2/Backend/#key-features","title":"Key Features","text":"<ol> <li> <p>Azure OpenAI Integration:</p> <ul> <li>Processes natural language queries and generates responses.</li> <li>Supports dynamic chart generation based on user queries.</li> </ul> </li> <li> <p>Structured Data Integration:</p> <ul> <li>Queries SQL databases for structured data (e.g., customer interactions, complaints).</li> <li>Uses Azure Cognitive Search for retrieving indexed call transcripts.</li> </ul> </li> <li> <p>Conversation History Management:</p> <ul> <li>Stores and retrieves conversation history using CosmosDB.</li> <li>Supports feedback and message updates.</li> </ul> </li> <li> <p>Chart Data Processing:</p> <ul> <li>Processes RAG (Retrieval-Augmented Generation) responses to generate chart-compatible JSON data.</li> </ul> </li> </ol>"},{"location":"workshop/Challenge-2/Backend/#workflow","title":"Workflow","text":"<ol> <li> <p>Initialize Services:</p> <ul> <li>Sets up Azure OpenAI, CosmosDB, and other integrations.</li> </ul> </li> <li> <p>Process Requests:</p> <ul> <li>Handles API requests for fetching chart data, filters, and conversation history.</li> </ul> </li> <li> <p>Generate Insights:</p> <ul> <li>Uses Azure OpenAI for unstructured queries.</li> <li>Executes SQL queries or Cognitive Search for structured data.</li> </ul> </li> <li> <p>Chatbot Interaction:</p> <ul> <li>Processes user queries and generates responses or charts.</li> </ul> </li> </ol>"},{"location":"workshop/Challenge-2/Backend/#tools-and-libraries","title":"Tools and Libraries","text":"<ul> <li>Quart: For building the backend application.</li> <li>Azure OpenAI: For natural language processing.</li> <li>CosmosDB: For managing conversation history.</li> <li>Azure Cognitive Search: For retrieving indexed data.</li> </ul>"},{"location":"workshop/Challenge-2/Backend/#how-it-works","title":"How It Works","text":"<p>The backend acts as the core processing engine, handling API requests, generating insights, and managing data storage.</p>"},{"location":"workshop/Challenge-2/Frontend/","title":"Frontend Overview","text":""},{"location":"workshop/Challenge-2/Frontend/#key-features","title":"Key Features","text":"<ol> <li> <p>Dynamic Chart Rendering:</p> <ul> <li>Displays charts (e.g., Donut, Bar, Word Cloud) using Chart.js.</li> <li>Updates dynamically based on user-selected filters.</li> </ul> </li> <li> <p>Chatbot Integration:</p> <ul> <li>Provides a conversational interface for users.</li> <li>Supports queries for insights, dynamic chart generation, and structured data retrieval.</li> </ul> </li> <li> <p>Filter Management:</p> <ul> <li>Allows users to refine data displayed in charts using filters (e.g., Sentiment, Topics).</li> </ul> </li> <li> <p>Responsive Design:</p> <ul> <li>Adapts layout and components to different screen sizes.</li> </ul> </li> <li> <p>Error Handling:</p> <ul> <li>Displays fallback messages for failed API calls.</li> </ul> </li> </ol>"},{"location":"workshop/Challenge-2/Frontend/#workflow","title":"Workflow","text":"<ol> <li> <p>Data Fetching:</p> <ul> <li>Calls APIs like <code>/api/fetchChartData</code> and <code>/api/fetchFilterData</code> to retrieve chart data and filters.</li> </ul> </li> <li> <p>Chatbot Interaction:</p> <ul> <li>Sends user queries to <code>/api/chat</code> for processing.</li> <li>Displays responses, including dynamically generated charts.</li> </ul> </li> <li> <p>Filter Application:</p> <ul> <li>Sends filter data to <code>/api/fetchChartDataWithFilters</code> and updates charts.</li> </ul> </li> </ol>"},{"location":"workshop/Challenge-2/Frontend/#tools-and-libraries","title":"Tools and Libraries","text":"<ul> <li>React: For building the user interface.</li> <li>Chart.js: For rendering charts.</li> <li>Axios/Fetch: For API communication.</li> </ul>"},{"location":"workshop/Challenge-2/Frontend/#how-it-works","title":"How It Works","text":"<p>The frontend provides a user-friendly interface for exploring insights, interacting with the chatbot, and visualizing data dynamically.</p>"},{"location":"workshop/Challenge-3/","title":"Explore Dashboard using Natural Language queries","text":"<p>The app allows you to interact with the dashboard using natural language queries. You can ask questions to gain insights from the data, and the system will respond with relevant charts, summaries, or structured data.</p>"},{"location":"workshop/Challenge-3/#how-it-works","title":"How It Works","text":"<ol> <li>Ask Questions: Use natural language to ask about your data.</li> <li>Get Insights: The app processes your query and provides answers, charts, or summaries.</li> <li>Explore: Dive deeper into your data with follow-up questions.</li> </ol>"},{"location":"workshop/Challenge-3/#sample-questions-to-get-you-started","title":"Sample Questions to Get You Started","text":""},{"location":"workshop/Challenge-3/#call-metrics","title":"Call Metrics","text":"<ul> <li>\ud83d\udd52 \"What is the total number of calls by date for the last 7 days?\"</li> <li>\u23f1\ufe0f \"Show me the average handling time by topics in minutes.\"</li> </ul>"},{"location":"workshop/Challenge-3/#customer-challenges","title":"Customer Challenges","text":"<ul> <li>\u2753 \"What are the top 7 challenges users reported?\"</li> <li>\ud83d\udca1 \"Give me a summary of billing issues.\"</li> </ul>"},{"location":"workshop/Challenge-3/#billing-insights","title":"Billing Insights","text":"<ul> <li>\ud83d\udcb3 \"When customers call about unexpected charges, what types of charges are they seeing?\"</li> </ul>"},{"location":"workshop/Challenge-3/#why-use-this","title":"Why Use This?","text":"<ul> <li>Interactive: No need to dig through spreadsheets\u2014just ask!</li> <li>Visual: Get charts and graphs to visualize your data.</li> <li>Efficient: Save time and get instant insights.</li> </ul>"},{"location":"workshop/Challenge-4-and-5/","title":"Orchestrator change","text":"Using your own data? <p>Incorporating your own data into the solution accelerator requires adapting the existing architecture to align with your specific data structures. Here are some recommendations:</p> <p>1. Implement Design Patterns and LangChain in Your Solution To effectively integrate AI capabilities, you need to incorporate design patterns that facilitate seamless interaction between your data and AI models. Utilizing LangChain can help in constructing these patterns, enabling efficient data processing and AI orchestration.</p> <p>2. Customize the <code>chat_functions.py</code> file The <code>chat_functions.py</code> file serves as a bridge between the user inputs and AI responses. To tailor this to your data:</p> <ul> <li>Understand the Existing Structure: Review the current implementation to comprehend how data flows and functions are structured.</li> <li>Map Your Data: Identify how your data schema aligns with the existing functions.</li> <li>Modify Functions: Adjust or rewrite functions to query and process your data appropriately, ensuring that the AI services can accurately interpret and respond based on your dataset.</li> </ul> <p>In this section, you will add an AI copilot to the Woodgrove Bank Contract Management application using Python, the GenAI capabilities of Azure Database for PostgreSQL - Flexible Server, and the Azure AI extension. Using the AI-validated data, the copilot will use RAG to provide insights and answer questions about vendor contract performance and invoicing accuracy, serving as an intelligent assistant for Woodgrove Banks users. Here's what you will accomplish:</p> <ul> <li> Explore the API codebase</li> <li> Review the RAG design</li> <li> Leverage LangChain Orchestration</li> <li> Implement and test the Chat endpoint</li> <li> Refine the copilot prompt using standard prompt engineering techniques</li> <li> Add and test the Copilot Chat UI component</li> </ul> <p>Following these steps will transform your application into a powerful AI-enhanced platform capable of executing advanced generative AI tasks and providing deeper insights from your data.</p>"},{"location":"workshop/Challenge-4-and-5/#what-are-copilots","title":"What are copilots?","text":"<p>Copilots are advanced AI assistants designed to augment human capabilities and improve productivity by providing intelligent, context-aware support, automating repetitive tasks, and enhancing decision-making processes. For instance, the Woodgrove Bank copilot will assist in data analysis, helping users identify patterns and trends in financial datasets.</p>"},{"location":"workshop/Challenge-4-and-5/#why-use-python","title":"Why use Python?","text":"<p>Python's simplicity and readability make it a popular programming language for AI and machine learning projects. Its extensive libraries and frameworks, such as LangChain, FastAPI, and many others, provide robust tools for developing sophisticated copilots. Python's versatility allows developers to iterate and experiment quickly, making it a top choice for building AI applications.</p>"},{"location":"workshop/Challenge-4-and-5/Challenge-4/","title":"Workshop Challenge: Changing the Logo in the App","text":"<p>One of the easiest and most fun changes you can make to the app is updating the logo! Follow these step-by-step instructions to replace the current logo with your own.</p>"},{"location":"workshop/Challenge-4-and-5/Challenge-4/#step-1-prepare-your-new-logo","title":"Step 1: Prepare Your New Logo","text":"<ol> <li>Create or find the logo you want to use.</li> <li>Save the logo as an image file (e.g., <code>logo.png</code>).</li> <li>Ensure the image has a reasonable size (e.g., 100x100 pixels) for better display.</li> </ol>"},{"location":"workshop/Challenge-4-and-5/Challenge-4/#step-2-add-the-logo-to-the-project","title":"Step 2: Add the Logo to the Project","text":"<ol> <li>Navigate to the <code>src/components/Svg</code> folder in your project directory.</li> <li>Replace the existing logo file or add your new logo file to this folder.</li> <li>Example: Save your new logo as <code>NewLogo.png</code>.</li> </ol>"},{"location":"workshop/Challenge-4-and-5/Challenge-4/#step-3-update-the-logo-component","title":"Step 3: Update the Logo Component","text":"<ol> <li>Open the <code>AppLogo</code> component file:</li> <li>Filepath: <code>src/components/Svg/Svg.tsx</code>.</li> <li>Locate the current logo implementation. It might look like this:</li> </ol> <p>```tsx export const AppLogo: React.FC = () =&gt; (      );</p>"},{"location":"workshop/Challenge-6/","title":"Video Processing Using Azure AI Content Understanding and Azure OpenAI","text":"<p>Content Understanding is an innovative solution designed to analyze and interpret diverse media types, including documents, images, audio, and video. It transforms this content into structured, organized, and searchable data. In this sample, we will demonstrate how to extract semantic information from you file, and send these information to Azure OpenAI to achive complex works.</p> <ul> <li>The samples in this repository default to the latest preview API version: (2024-12-01-preview).</li> </ul>"},{"location":"workshop/Challenge-6/#samples","title":"Samples","text":"File Description video_chapter_generation.ipynb Extract semantic descriptions using content understanding API, and then leverage OpenAI to group into video chapters. video_tag_generation.ipynb Generate video tags based on Azure Content Understanding and Azure OpenAI."},{"location":"workshop/Challenge-6/#getting-started","title":"Getting started","text":"<ol> <li>Identify your Azure AI Services resource, suggest to use Sweden Central region for the availability of the content understanding API.</li> <li>Go to <code>Access Control (IAM)</code> in resource, grant yourself role <code>Cognitive Services User</code></li> <li>Identify your Azure OpenAI resource</li> <li>Go to <code>Access Control (IAM)</code> in resource, grant yourself role <code>Cognitive Services OpenAI User</code></li> <li>Copy <code>notebooks/.env.sample</code> to <code>notebooks/.env</code> Bash<pre><code>cp notebooks/.env.example notebooks/.env\n</code></pre></li> <li>Fill required information into .env from the resources that you alredy have created, remember that your model is gpt-4o-mini, you should have something like this:    Bash<pre><code>AZURE_AI_SERVICE_ENDPOINT=\"https://kmyfeztrgpktwf-aiservices-cu.cognitiveservices.azure.com\"\nAZURE_AI_SERVICE_API_VERSION=2024-12-01-preview\nAZURE_OPENAI_ENDPOINT=\"https://kmyfeztrgpktwf-aiservices.openai.azure.com\"\nAZURE_OPENAI_API_VERSION=2024-08-01-preview\nAZURE_OPENAI_CHAT_DEPLOYMENT_NAME=gpt-4o-mini\n</code></pre></li> <li>Login Azure    Bash<pre><code>az login\n</code></pre></li> </ol>"},{"location":"workshop/Challenge-6/#open-a-jupyter-notebook-and-follow-the-step-by-step-guidance","title":"Open a Jupyter notebook and follow the step-by-step guidance","text":"<p>Navigate to the <code>notebooks</code> directory and select the sample notebook you are interested in. Since Codespaces is pre-configured with the necessary environment, you can directly execute each step in the notebook.</p>"},{"location":"workshop/Challenge-6/#more-samples-using-azure-content-understanding","title":"More Samples using Azure Content Understanding","text":"<p>Azure Content Understanding Basic Usecase</p> <p>Azure Search with Content Understanding</p>"},{"location":"workshop/Challenge-6/video_chapter_generation/","title":"Video Chapters Generation","text":"<p>Generate video chapters based on Azure Content Understanding and Azure OpenAI.</p> In\u00a0[\u00a0]: Copied! <pre>%pip install -r ../requirements.txt\n</pre> %pip install -r ../requirements.txt In\u00a0[\u00a0]: Copied! <pre>from dotenv import load_dotenv\nimport os\n\nload_dotenv(dotenv_path=\".env\", override=True)\n\nAZURE_AI_SERVICE_ENDPOINT = os.getenv(\"AZURE_AI_SERVICE_ENDPOINT\")\nAZURE_AI_SERVICE_API_VERSION = os.getenv(\"AZURE_AI_SERVICE_API_VERSION\", \"2024-12-01-preview\")\n\nAZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\nAZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-08-01-preview\")\nAZURE_OPENAI_CHAT_DEPLOYMENT_NAME = os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\")\n</pre> from dotenv import load_dotenv import os  load_dotenv(dotenv_path=\".env\", override=True)  AZURE_AI_SERVICE_ENDPOINT = os.getenv(\"AZURE_AI_SERVICE_ENDPOINT\") AZURE_AI_SERVICE_API_VERSION = os.getenv(\"AZURE_AI_SERVICE_API_VERSION\", \"2024-12-01-preview\")  AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\") AZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-08-01-preview\") AZURE_OPENAI_CHAT_DEPLOYMENT_NAME = os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\") In\u00a0[\u00a0]: Copied! <pre>from pathlib import Path\nVIDEO_LOCATION = Path(\"../data/FlightSimulator.mp4\")\n</pre> from pathlib import Path VIDEO_LOCATION = Path(\"../data/FlightSimulator.mp4\") In\u00a0[\u00a0]: Copied! <pre>import sys\nfrom pathlib import Path\nimport json\nimport uuid\n\n\n# add the parent directory to the path to use shared modules\nparent_dir = Path(Path.cwd()).parent\nsys.path.append(\n    str(parent_dir)\n)\nfrom python.content_understanding_client import AzureContentUnderstandingClient\n\nfrom azure.identity import DefaultAzureCredential, get_bearer_token_provider\ncredential = DefaultAzureCredential()\ntoken_provider = get_bearer_token_provider(credential, \"https://cognitiveservices.azure.com/.default\")\n\n# The analyzer template is used to define the schema of the output\nANALYZER_TEMPLATE_PATH = \"../analyzer_templates/video_content_understanding.json\"\nANALYZER_ID = \"video_scene_chapter\" + \"_\" + str(uuid.uuid4())  # Unique identifier for the analyzer\n\n# Create the Content Understanding (CU) client\ncu_client = AzureContentUnderstandingClient(\n    endpoint=AZURE_AI_SERVICE_ENDPOINT,\n    api_version=AZURE_AI_SERVICE_API_VERSION,\n    token_provider=token_provider,\n    x_ms_useragent=\"azure-ai-content-understanding-python/video_chapter_generation\", # This header is used for sample usage telemetry, please comment out this line if you want to opt out.\n)\n\n# Use the client to create an analyzer\nresponse = cu_client.begin_create_analyzer(\n    ANALYZER_ID, analyzer_template_path=ANALYZER_TEMPLATE_PATH)\nresult = cu_client.poll_result(response)\n\nprint(json.dumps(result, indent=2))\n</pre> import sys from pathlib import Path import json import uuid   # add the parent directory to the path to use shared modules parent_dir = Path(Path.cwd()).parent sys.path.append(     str(parent_dir) ) from python.content_understanding_client import AzureContentUnderstandingClient  from azure.identity import DefaultAzureCredential, get_bearer_token_provider credential = DefaultAzureCredential() token_provider = get_bearer_token_provider(credential, \"https://cognitiveservices.azure.com/.default\")  # The analyzer template is used to define the schema of the output ANALYZER_TEMPLATE_PATH = \"../analyzer_templates/video_content_understanding.json\" ANALYZER_ID = \"video_scene_chapter\" + \"_\" + str(uuid.uuid4())  # Unique identifier for the analyzer  # Create the Content Understanding (CU) client cu_client = AzureContentUnderstandingClient(     endpoint=AZURE_AI_SERVICE_ENDPOINT,     api_version=AZURE_AI_SERVICE_API_VERSION,     token_provider=token_provider,     x_ms_useragent=\"azure-ai-content-understanding-python/video_chapter_generation\", # This header is used for sample usage telemetry, please comment out this line if you want to opt out. )  # Use the client to create an analyzer response = cu_client.begin_create_analyzer(     ANALYZER_ID, analyzer_template_path=ANALYZER_TEMPLATE_PATH) result = cu_client.poll_result(response)  print(json.dumps(result, indent=2)) In\u00a0[\u00a0]: Copied! <pre># Submit the video for content analysis\nresponse = cu_client.begin_analyze(ANALYZER_ID, file_location=VIDEO_LOCATION)\n\n# Wait for the analysis to complete and get the content analysis result\nvideo_cu_result = cu_client.poll_result(\n    response, timeout_seconds=3600)  # 1 hour timeout for long videos\n\n# Print the content analysis result\nprint(f\"Video Content Understanding result: \", video_cu_result)\n\n# Optional - Delete the analyzer if it is no longer needed\ncu_client.delete_analyzer(ANALYZER_ID)\n</pre> # Submit the video for content analysis response = cu_client.begin_analyze(ANALYZER_ID, file_location=VIDEO_LOCATION)  # Wait for the analysis to complete and get the content analysis result video_cu_result = cu_client.poll_result(     response, timeout_seconds=3600)  # 1 hour timeout for long videos  # Print the content analysis result print(f\"Video Content Understanding result: \", video_cu_result)  # Optional - Delete the analyzer if it is no longer needed cu_client.delete_analyzer(ANALYZER_ID) In\u00a0[\u00a0]: Copied! <pre>from python.utility import OpenAIAssistant, generate_scenes\n\n# Create an OpenAI Assistant to interact with Azure OpenAI\nopenai_assistant = OpenAIAssistant(\n    aoai_end_point=AZURE_OPENAI_ENDPOINT,\n    aoai_api_version=AZURE_OPENAI_API_VERSION,\n    deployment_name=AZURE_OPENAI_CHAT_DEPLOYMENT_NAME,\n    aoai_api_key=None,\n)\n\n# Generate the scenes using the video segment result from Azure Content Understanding\nscene_result = generate_scenes(video_cu_result, openai_assistant)\n\n# Write the scene result to a json file\nscene_output_json_file = \"./scene_results.json\"\nwith open(scene_output_json_file, \"w\") as f:\n    f.write(scene_result.model_dump_json(indent=2))\n    print(f\"Scene result is saved to {scene_output_json_file}\")\n\n# Print the scene result for the debugging purpose\nprint(scene_result.model_dump_json(indent=2))\n</pre> from python.utility import OpenAIAssistant, generate_scenes  # Create an OpenAI Assistant to interact with Azure OpenAI openai_assistant = OpenAIAssistant(     aoai_end_point=AZURE_OPENAI_ENDPOINT,     aoai_api_version=AZURE_OPENAI_API_VERSION,     deployment_name=AZURE_OPENAI_CHAT_DEPLOYMENT_NAME,     aoai_api_key=None, )  # Generate the scenes using the video segment result from Azure Content Understanding scene_result = generate_scenes(video_cu_result, openai_assistant)  # Write the scene result to a json file scene_output_json_file = \"./scene_results.json\" with open(scene_output_json_file, \"w\") as f:     f.write(scene_result.model_dump_json(indent=2))     print(f\"Scene result is saved to {scene_output_json_file}\")  # Print the scene result for the debugging purpose print(scene_result.model_dump_json(indent=2)) In\u00a0[\u00a0]: Copied! <pre>from python.utility import generate_chapters\n\n\n# Generate the chapters using the scenes result\nchapter_result = generate_chapters(scene_result, openai_assistant)\n\n# Write the chapter result to a json file\nchapter_output_json_file = \"./chapter_results.json\"\nwith open(chapter_output_json_file, \"w\") as f:\n    f.write(chapter_result.model_dump_json(indent=2))\n    print(f\"Chapter result is saved to {chapter_output_json_file}\")\n\n# Print out the chapter result for the debugging purpose\nprint(chapter_result)\n</pre> from python.utility import generate_chapters   # Generate the chapters using the scenes result chapter_result = generate_chapters(scene_result, openai_assistant)  # Write the chapter result to a json file chapter_output_json_file = \"./chapter_results.json\" with open(chapter_output_json_file, \"w\") as f:     f.write(chapter_result.model_dump_json(indent=2))     print(f\"Chapter result is saved to {chapter_output_json_file}\")  # Print out the chapter result for the debugging purpose print(chapter_result)"},{"location":"workshop/Challenge-6/video_chapter_generation/#video-chapters-generation","title":"Video Chapters Generation\u00b6","text":""},{"location":"workshop/Challenge-6/video_chapter_generation/#pre-requisites","title":"Pre-requisites\u00b6","text":"<ol> <li>Follow README to create essential resource that will be used in this sample.</li> <li>Install required packages</li> </ol>"},{"location":"workshop/Challenge-6/video_chapter_generation/#load-environment-variables","title":"Load environment variables\u00b6","text":""},{"location":"workshop/Challenge-6/video_chapter_generation/#file-to-analyze","title":"File to Analyze\u00b6","text":""},{"location":"workshop/Challenge-6/video_chapter_generation/#create-a-custom-analyzer-and-submit-the-video-to-generate-the-description","title":"Create a custom analyzer and submit the video to generate the description\u00b6","text":"<p>The custom analyzer schema is defined in ../analyzer_templates/video_content_understanding.json. The main custom field is <code>segmentDescription</code> as we need to get the descriptions of video segments and feed them into chatGPT to generate the scenes and chapters. Adding transcripts will help to increase the accuracy of scenes/chapters segmentation results. To get transcripts, we will need to set the<code>returnDetails</code> parameter in the <code>config</code> field to <code>True</code>.</p> <p>In this example, we will use the utility class <code>AzureContentUnderstandingClient</code> to load the analyzer schema from the template file and submit it to Azure Content Understanding service. Then, we will analyze the video to get the segment descriptions and transcripts.</p>"},{"location":"workshop/Challenge-6/video_chapter_generation/#use-the-created-analyzer-to-extract-video-content","title":"Use the created analyzer to extract video content\u00b6","text":"<p>It might take some time depending on the video length. Try with short videos to get results faster</p>"},{"location":"workshop/Challenge-6/video_chapter_generation/#aggregate-video-segments-to-generate-video-scenes","title":"Aggregate video segments to generate video scenes\u00b6","text":"<p>ChatGPT will be used to combine segment descriptions and transcripts into scenes and provide concise descriptions for each scene.</p> <p>After running this step, you will have a metadata json file of video scenes that can be used to generate video chapters. Each scene has start and end timestamps, short description and corresponding transcripts if available</p>"},{"location":"workshop/Challenge-6/video_chapter_generation/#create-video-chapters","title":"Create video chapters\u00b6","text":"<p>Create video chapters by combining the video scenes with chatGPT. After running this step, you will have a video chapters json file. Each chapter has start and end timestamps, a title and list of scenes that belong to the chapter.</p>"},{"location":"workshop/Challenge-6/video_tag_generation/","title":"Video Chapters Generation","text":"<p>Generate video tags based on Azure Content Understanding and Azure OpenAI.</p> In\u00a0[\u00a0]: Copied! <pre>%pip install -r ../requirements.txt\n</pre> %pip install -r ../requirements.txt In\u00a0[\u00a0]: Copied! <pre>from dotenv import load_dotenv\nimport os\n\nload_dotenv(dotenv_path=\".env\", override=True)\n\nAZURE_AI_SERVICE_ENDPOINT = os.getenv(\"AZURE_AI_SERVICE_ENDPOINT\")\nAZURE_AI_SERVICE_API_VERSION = os.getenv(\"AZURE_AI_SERVICE_API_VERSION\", \"2024-12-01-preview\")\n\nAZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\nAZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-08-01-preview\")\nAZURE_OPENAI_CHAT_DEPLOYMENT_NAME = os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\")\n</pre> from dotenv import load_dotenv import os  load_dotenv(dotenv_path=\".env\", override=True)  AZURE_AI_SERVICE_ENDPOINT = os.getenv(\"AZURE_AI_SERVICE_ENDPOINT\") AZURE_AI_SERVICE_API_VERSION = os.getenv(\"AZURE_AI_SERVICE_API_VERSION\", \"2024-12-01-preview\")  AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\") AZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-08-01-preview\") AZURE_OPENAI_CHAT_DEPLOYMENT_NAME = os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\") In\u00a0[\u00a0]: Copied! <pre>from pathlib import Path\nVIDEO_LOCATION = Path(\"../data/FlightSimulator.mp4\")\n</pre> from pathlib import Path VIDEO_LOCATION = Path(\"../data/FlightSimulator.mp4\") In\u00a0[\u00a0]: Copied! <pre>import sys\nfrom pathlib import Path\nimport json\nimport uuid\n\n\n# add the parent directory to the path to use shared modules\nparent_dir = Path(Path.cwd()).parent\nsys.path.append(\n    str(parent_dir)\n)\nfrom python.content_understanding_client import AzureContentUnderstandingClient\n\nfrom azure.identity import DefaultAzureCredential, get_bearer_token_provider\ncredential = DefaultAzureCredential()\ntoken_provider = get_bearer_token_provider(credential, \"https://cognitiveservices.azure.com/.default\")\n\n# The analyzer template is used to define the schema of the output\nANALYZER_TEMPLATE_PATH = \"../analyzer_templates/video_tag.json\"\nANALYZER_ID = \"video_tag\" + \"_\" + str(uuid.uuid4())  # Unique identifier for the analyzer\n\n# Create the Content Understanding (CU) client\ncu_client = AzureContentUnderstandingClient(\n    endpoint=AZURE_AI_SERVICE_ENDPOINT,\n    api_version=AZURE_AI_SERVICE_API_VERSION,\n    token_provider=token_provider,\n#    x_ms_useragent=\"azure-ai-tag-with-content-understanding-python/video_tag\", # This header is used for sample usage telemetry, please comment out this line if you want to opt out.\n)\n\n# Use the client to create an analyzer\nresponse = cu_client.begin_create_analyzer(\n    ANALYZER_ID, analyzer_template_path=ANALYZER_TEMPLATE_PATH)\nresult = cu_client.poll_result(response)\n\nprint(json.dumps(result, indent=2))\n</pre> import sys from pathlib import Path import json import uuid   # add the parent directory to the path to use shared modules parent_dir = Path(Path.cwd()).parent sys.path.append(     str(parent_dir) ) from python.content_understanding_client import AzureContentUnderstandingClient  from azure.identity import DefaultAzureCredential, get_bearer_token_provider credential = DefaultAzureCredential() token_provider = get_bearer_token_provider(credential, \"https://cognitiveservices.azure.com/.default\")  # The analyzer template is used to define the schema of the output ANALYZER_TEMPLATE_PATH = \"../analyzer_templates/video_tag.json\" ANALYZER_ID = \"video_tag\" + \"_\" + str(uuid.uuid4())  # Unique identifier for the analyzer  # Create the Content Understanding (CU) client cu_client = AzureContentUnderstandingClient(     endpoint=AZURE_AI_SERVICE_ENDPOINT,     api_version=AZURE_AI_SERVICE_API_VERSION,     token_provider=token_provider, #    x_ms_useragent=\"azure-ai-tag-with-content-understanding-python/video_tag\", # This header is used for sample usage telemetry, please comment out this line if you want to opt out. )  # Use the client to create an analyzer response = cu_client.begin_create_analyzer(     ANALYZER_ID, analyzer_template_path=ANALYZER_TEMPLATE_PATH) result = cu_client.poll_result(response)  print(json.dumps(result, indent=2)) In\u00a0[\u00a0]: Copied! <pre># Submit the video for content analysis\nresponse = cu_client.begin_analyze(ANALYZER_ID, file_location=VIDEO_LOCATION)\n\n# Wait for the analysis to complete and get the content analysis result\nvideo_cu_result = cu_client.poll_result(\n    response, timeout_seconds=3600)  # 1 hour timeout for long videos\n\n# Print the content analysis result\nprint(f\"Video Content Understanding result: \", video_cu_result)\n\n# Optional - Delete the analyzer if it is no longer needed\ncu_client.delete_analyzer(ANALYZER_ID)\n</pre> # Submit the video for content analysis response = cu_client.begin_analyze(ANALYZER_ID, file_location=VIDEO_LOCATION)  # Wait for the analysis to complete and get the content analysis result video_cu_result = cu_client.poll_result(     response, timeout_seconds=3600)  # 1 hour timeout for long videos  # Print the content analysis result print(f\"Video Content Understanding result: \", video_cu_result)  # Optional - Delete the analyzer if it is no longer needed cu_client.delete_analyzer(ANALYZER_ID) In\u00a0[\u00a0]: Copied! <pre>from python.utility import OpenAIAssistant, aggregate_tags\n\n# Create an OpenAI Assistant to interact with Azure OpenAI\nopenai_assistant = OpenAIAssistant(\n    aoai_end_point=AZURE_OPENAI_ENDPOINT,\n    aoai_api_version=AZURE_OPENAI_API_VERSION,\n    deployment_name=AZURE_OPENAI_CHAT_DEPLOYMENT_NAME,\n    aoai_api_key=None,\n)\n\n# Aggregate tags using the video segment result from Azure Content Understanding\ntag_result = aggregate_tags(video_cu_result, openai_assistant)\n\ntag_result.tags\n</pre> from python.utility import OpenAIAssistant, aggregate_tags  # Create an OpenAI Assistant to interact with Azure OpenAI openai_assistant = OpenAIAssistant(     aoai_end_point=AZURE_OPENAI_ENDPOINT,     aoai_api_version=AZURE_OPENAI_API_VERSION,     deployment_name=AZURE_OPENAI_CHAT_DEPLOYMENT_NAME,     aoai_api_key=None, )  # Aggregate tags using the video segment result from Azure Content Understanding tag_result = aggregate_tags(video_cu_result, openai_assistant)  tag_result.tags"},{"location":"workshop/Challenge-6/video_tag_generation/#video-chapters-generation","title":"Video Chapters Generation\u00b6","text":""},{"location":"workshop/Challenge-6/video_tag_generation/#pre-requisites","title":"Pre-requisites\u00b6","text":"<ol> <li>Follow README to create essential resource that will be used in this sample.</li> <li>Install required packages</li> </ol>"},{"location":"workshop/Challenge-6/video_tag_generation/#load-environment-variables","title":"Load environment variables\u00b6","text":""},{"location":"workshop/Challenge-6/video_tag_generation/#file-to-analyze","title":"File to Analyze\u00b6","text":""},{"location":"workshop/Challenge-6/video_tag_generation/#create-a-custom-analyzer-and-submit-the-video-to-generate-tags","title":"Create a custom analyzer and submit the video to generate tags\u00b6","text":"<p>The custom analyzer schema is defined in ../analyzer_templates/video_tag.json. The custom fields are <code>segmentDescription</code>, <code>transcript</code> and <code>tags</code>. Adding description and transcripts helps to increase the accuracy of tag generation results. To get transcripts, we will need to set the<code>returnDetails</code> parameter in the <code>config</code> field to <code>True</code>.</p> <p>In this example, we will use the utility class <code>AzureContentUnderstandingClient</code> to load the analyzer schema from the template file and submit it to Azure Content Understanding service. Then, we will analyze the video to get the segment tags.</p>"},{"location":"workshop/Challenge-6/video_tag_generation/#use-the-created-analyzer-to-extract-video-content","title":"Use the created analyzer to extract video content\u00b6","text":"<p>It might take some time depending on the video length. Try with short videos to get results faster</p>"},{"location":"workshop/Challenge-6/video_tag_generation/#aggregate-tags-from-each-segment-to-generate-video-tags","title":"Aggregate tags from each segment to generate video tags\u00b6","text":"<p>ChatGPT will be used to remove duplicate tags which are semantically similar across segments.</p>"},{"location":"workshop/Challenge-7/","title":"Evaluation","text":""},{"location":"workshop/Challenge-7/#content-safety-evaluation","title":"Content Safety Evaluation","text":"<p>This notebook demonstrates how to evaluate content safety using Azure AI's evaluation tools. It includes steps to: - Simulate adversarial scenarios. - Evaluate content for safety metrics such as violence, sexual content, hate/unfairness, and self-harm. - Generate evaluation reports in JSON format.</p>"},{"location":"workshop/Challenge-7/#prerequisites","title":"Prerequisites","text":"<ul> <li>Azure AI project credentials.</li> <li>Python environment with required libraries installed (<code>azure-ai-evaluation</code>, <code>pandas</code>, etc.).</li> <li>Access to the Azure API endpoint.</li> </ul> <p>Follow the steps in this notebook to perform content safety evaluations and generate detailed reports.</p>"},{"location":"workshop/Challenge-7/Content_safety_evaluation/","title":"Content Safety Evaluation","text":"In\u00a0[1]: Copied! <pre>from azure.ai.evaluation.simulator import AdversarialSimulator\n</pre> from azure.ai.evaluation.simulator import AdversarialSimulator <pre>[INFO] Could not import AIAgentConverter. Please install the dependency with `pip install azure-ai-projects`.\n</pre> In\u00a0[2]: Copied! <pre>import time\nimport uuid\nimport json\nimport pandas as pd\nfrom pathlib import Path\n\n# Define folder paths\noutput_folder = \"output\"\nPath(output_folder).mkdir(parents=True, exist_ok=True)  # Ensure output folder exists\n\ncount = 10\n</pre> import time import uuid import json import pandas as pd from pathlib import Path  # Define folder paths output_folder = \"output\" Path(output_folder).mkdir(parents=True, exist_ok=True)  # Ensure output folder exists  count = 10 In\u00a0[\u00a0]: Copied! <pre>from azure.identity import DefaultAzureCredential\n\nazure_ai_project = {\n    \"subscription_id\": '',\n    \"resource_group_name\": '',\n    \"project_name\": ''\n}\n\n# your azure api endpoint\napi_url = ''\n</pre> from azure.identity import DefaultAzureCredential  azure_ai_project = {     \"subscription_id\": '',     \"resource_group_name\": '',     \"project_name\": '' }  # your azure api endpoint api_url = '' In\u00a0[\u00a0]: Copied! <pre>from pathlib import Path\nimport requests\n\ndef call_streaming_url(url):\n    full_response = \"\"\n    try:\n        response = requests.get(url, stream=True)    \n    except:\n         time.sleep(10)\n         response = requests.get(url, stream=True)\n    for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n            full_response += chunk.decode('utf-8')  # Concatenate each chunk to the full response\n\n    return full_response\n</pre> from pathlib import Path import requests  def call_streaming_url(url):     full_response = \"\"     try:         response = requests.get(url, stream=True)         except:          time.sleep(10)          response = requests.get(url, stream=True)     for chunk in response.iter_content(chunk_size=8192):         if chunk:             full_response += chunk.decode('utf-8')  # Concatenate each chunk to the full response      return full_response In\u00a0[\u00a0]: Copied! <pre>from typing import List, Dict, Any, Optional\nasync def callback(\n    messages: List[Dict],\n    stream: bool = False,\n    session_state: Any = None,\n) -&gt; dict:\n    query = messages[\"messages\"][0][\"content\"]\n    context = None\n\n    # Add file contents for summarization or re-write\n    if 'file_content' in messages[\"template_parameters\"]:\n        query += messages[\"template_parameters\"]['file_content']\n    \n    # Call your own endpoint and pass your query as input. Make sure to handle your function_call_to_your_endpoint's error responses.\n    km_api_url = api_url + query\n    # print(km_api_url)\n    response = call_streaming_url(km_api_url) \n   \n    # Format responses in OpenAI message protocol\n    try:\n        r = json.loads(response)['answer']\n    except:\n        r = response #'I cannot answer this question from the data available. Please rephrase or add more details.'\n    formatted_response = {\n        \"content\": r,\n        \"role\": \"assistant\",\n        \"context\": {},\n    }\n\n    messages[\"messages\"].append(formatted_response)\n\n    return {\n        \"messages\": messages[\"messages\"],\n        \"stream\": stream,\n        \"session_state\": session_state\n    }\n</pre> from typing import List, Dict, Any, Optional async def callback(     messages: List[Dict],     stream: bool = False,     session_state: Any = None, ) -&gt; dict:     query = messages[\"messages\"][0][\"content\"]     context = None      # Add file contents for summarization or re-write     if 'file_content' in messages[\"template_parameters\"]:         query += messages[\"template_parameters\"]['file_content']          # Call your own endpoint and pass your query as input. Make sure to handle your function_call_to_your_endpoint's error responses.     km_api_url = api_url + query     # print(km_api_url)     response = call_streaming_url(km_api_url)          # Format responses in OpenAI message protocol     try:         r = json.loads(response)['answer']     except:         r = response #'I cannot answer this question from the data available. Please rephrase or add more details.'     formatted_response = {         \"content\": r,         \"role\": \"assistant\",         \"context\": {},     }      messages[\"messages\"].append(formatted_response)      return {         \"messages\": messages[\"messages\"],         \"stream\": stream,         \"session_state\": session_state     } In\u00a0[\u00a0]: Copied! <pre>from azure.ai.evaluation.simulator import AdversarialScenario\nfrom azure.identity import DefaultAzureCredential\ncredential = DefaultAzureCredential()\n\nscenario = AdversarialScenario.ADVERSARIAL_QA\nadversarial_simulator = AdversarialSimulator(azure_ai_project=azure_ai_project, credential=credential)\n\noutputs = await adversarial_simulator(\n        scenario=scenario, # required adversarial scenario to simulate\n        target=callback, # callback function to simulate against\n        max_conversation_turns=1, #optional, applicable only to conversation scenario\n        max_simulation_results=count, #optional\n    )\n\noutput_file_adversarial = Path(output_folder) / f\"adversarial_output_{count}.jsonl\"\nwith output_file_adversarial.open(\"w\") as f:\n    f.write(outputs.to_eval_qr_json_lines())\n</pre> from azure.ai.evaluation.simulator import AdversarialScenario from azure.identity import DefaultAzureCredential credential = DefaultAzureCredential()  scenario = AdversarialScenario.ADVERSARIAL_QA adversarial_simulator = AdversarialSimulator(azure_ai_project=azure_ai_project, credential=credential)  outputs = await adversarial_simulator(         scenario=scenario, # required adversarial scenario to simulate         target=callback, # callback function to simulate against         max_conversation_turns=1, #optional, applicable only to conversation scenario         max_simulation_results=count, #optional     )  output_file_adversarial = Path(output_folder) / f\"adversarial_output_{count}.jsonl\" with output_file_adversarial.open(\"w\") as f:     f.write(outputs.to_eval_qr_json_lines()) In\u00a0[\u00a0]: Copied! <pre>from pathlib import Path\nfrom azure.ai.evaluation import ContentSafetyEvaluator\nimport base64\nimport pandas as pd\n\ncredential = DefaultAzureCredential()\n# instantiate an evaluator with image and multi-modal support\nsafety_evaluator = ContentSafetyEvaluator(credential=credential, azure_ai_project=azure_ai_project)\n\ndf = pd.read_json(output_file_adversarial, lines=True)\njson_string = []\nfor index, row in df.iterrows(): \n    safety_score = safety_evaluator(query=row['query'], response=row['response'])\n    json_object = {\n        \"query\": row['query'],\n        \"response\": row['response'],\n        \"violance\": safety_score['violence'],\n        \"violence_score\": safety_score['violence_score'],\n        \"violence_reason\": safety_score['violence_reason'],\n        \"violence_threshold\": safety_score['violence_threshold'],\n        \"violence_result\": safety_score['violence_result'],\n        \"sexual\": safety_score['sexual'],\n        \"sexual_score\": safety_score['sexual_score'],\n        \"sexual_reason\": safety_score['sexual_reason'],\n        \"sexual_threshold\": safety_score['sexual_threshold'],\n        \"sexual_result\": safety_score['sexual_result'],\n        \"hate_unfairness\": safety_score['hate_unfairness'],\n        \"hate_unfairness_score\": safety_score['hate_unfairness_score'],\n        \"hate_unfairness_reason\": safety_score['hate_unfairness_reason'],\n        \"hate_unfairness_threshold\": safety_score['hate_unfairness_threshold'],\n        \"hate_unfairness_result\": safety_score['hate_unfairness_result'],\n        \"self_harm\": safety_score['self_harm'],\n        \"self_harm_score\": safety_score['self_harm_score'],\n        \"self_harm_reason\": safety_score['self_harm_reason'],\n        \"self_harm_threshold\": safety_score['self_harm_threshold'],\n        \"self_harm_result\": safety_score['self_harm_result']\n    }\n    json_string.append(json_object)\nfile_eval = Path(output_folder) / f\"adversarial_output_eval_scores_{count}.jsonl\"\nwith Path(file_eval).open(\"w\") as f:\n  json.dump(json_string, f, indent=4)\n</pre> from pathlib import Path from azure.ai.evaluation import ContentSafetyEvaluator import base64 import pandas as pd  credential = DefaultAzureCredential() # instantiate an evaluator with image and multi-modal support safety_evaluator = ContentSafetyEvaluator(credential=credential, azure_ai_project=azure_ai_project)  df = pd.read_json(output_file_adversarial, lines=True) json_string = [] for index, row in df.iterrows():      safety_score = safety_evaluator(query=row['query'], response=row['response'])     json_object = {         \"query\": row['query'],         \"response\": row['response'],         \"violance\": safety_score['violence'],         \"violence_score\": safety_score['violence_score'],         \"violence_reason\": safety_score['violence_reason'],         \"violence_threshold\": safety_score['violence_threshold'],         \"violence_result\": safety_score['violence_result'],         \"sexual\": safety_score['sexual'],         \"sexual_score\": safety_score['sexual_score'],         \"sexual_reason\": safety_score['sexual_reason'],         \"sexual_threshold\": safety_score['sexual_threshold'],         \"sexual_result\": safety_score['sexual_result'],         \"hate_unfairness\": safety_score['hate_unfairness'],         \"hate_unfairness_score\": safety_score['hate_unfairness_score'],         \"hate_unfairness_reason\": safety_score['hate_unfairness_reason'],         \"hate_unfairness_threshold\": safety_score['hate_unfairness_threshold'],         \"hate_unfairness_result\": safety_score['hate_unfairness_result'],         \"self_harm\": safety_score['self_harm'],         \"self_harm_score\": safety_score['self_harm_score'],         \"self_harm_reason\": safety_score['self_harm_reason'],         \"self_harm_threshold\": safety_score['self_harm_threshold'],         \"self_harm_result\": safety_score['self_harm_result']     }     json_string.append(json_object) file_eval = Path(output_folder) / f\"adversarial_output_eval_scores_{count}.jsonl\" with Path(file_eval).open(\"w\") as f:   json.dump(json_string, f, indent=4) In\u00a0[\u00a0]: Copied! <pre># import asyncio\n# from azure.ai.evaluation.simulator import Simulator\n\n# import importlib.resources as pkg_resources\n# model_config = {\n#         \"azure_endpoint\": api_url, #\"&lt;your_azure_endpoint&gt;\",\n#         \"azure_deployment\": \"gpt-4o-mini\" #\"&lt;deployment_name&gt;\"\n# }\n# grounding_simulator = Simulator(model_config=model_config)\n\n# package = \"azure.ai.evaluation.simulator._data_sources\"\n# resource_name = \"grounding.json\"\n# conversation_turns = []\n\n# with pkg_resources.path(package, resource_name) as grounding_file:\n#     with open(grounding_file, \"r\") as file:\n#         data = json.load(file)\n\n# for item in data:\n#     conversation_turns.append([item])\n\n# outputs = await grounding_simulator(\n#     target=callback,\n#     conversation_turns=conversation_turns, #generates 287 rows of data\n#     max_conversation_turns=1,\n# )\n\n# output_file = \"grounding_simulation_output.jsonl\"\n# with open(output_file, \"w\") as file:\n#     for output in outputs:\n#         file.write(output.to_eval_qr_json_lines())\n</pre> # import asyncio # from azure.ai.evaluation.simulator import Simulator  # import importlib.resources as pkg_resources # model_config = { #         \"azure_endpoint\": api_url, #\"\", #         \"azure_deployment\": \"gpt-4o-mini\" #\"\" # } # grounding_simulator = Simulator(model_config=model_config)  # package = \"azure.ai.evaluation.simulator._data_sources\" # resource_name = \"grounding.json\" # conversation_turns = []  # with pkg_resources.path(package, resource_name) as grounding_file: #     with open(grounding_file, \"r\") as file: #         data = json.load(file)  # for item in data: #     conversation_turns.append([item])  # outputs = await grounding_simulator( #     target=callback, #     conversation_turns=conversation_turns, #generates 287 rows of data #     max_conversation_turns=1, # )  # output_file = \"grounding_simulation_output.jsonl\" # with open(output_file, \"w\") as file: #     for output in outputs: #         file.write(output.to_eval_qr_json_lines())  In\u00a0[\u00a0]: Copied! <pre># # Then you can pass it into our Groundedness evaluator to evaluate it for groundedness\n# groundedness_evaluator = GroundednessEvaluator(model_config=model_config)\n# eval_output = evaluate(\n#     data=output_file,\n#     evaluators={\n#         \"groundedness\": groundedness_evaluator\n#     },\n#     output_path=\"groundedness_eval_output.json\",\n#     # azure_ai_project=project_scope # Optional for uploading to your Azure AI Project\n# )\n</pre> # # Then you can pass it into our Groundedness evaluator to evaluate it for groundedness # groundedness_evaluator = GroundednessEvaluator(model_config=model_config) # eval_output = evaluate( #     data=output_file, #     evaluators={ #         \"groundedness\": groundedness_evaluator #     }, #     output_path=\"groundedness_eval_output.json\", #     # azure_ai_project=project_scope # Optional for uploading to your Azure AI Project # ) In\u00a0[\u00a0]: Copied! <pre># import importlib.resources as pkg_resources\n\n# grounding_simulator = Simulator(model_config=model_config)\n\n# package = \"azure.ai.evaluation.simulator._data_sources\"\n# resource_name = \"grounding.json\"\n# conversation_turns = []\n\n# with pkg_resources.path(package, resource_name) as grounding_file:\n#     with open(grounding_file, \"r\") as file:\n#         data = json.load(file)\n\n# for item in data:\n#     conversation_turns.append([item])\n\n# outputs = asyncio.run(grounding_simulator(\n#     target=callback,\n#     conversation_turns=conversation_turns, #generates 287 rows of data\n#     max_conversation_turns=1,\n# ))\n\n# output_file = \"grounding_simulation_output.jsonl\"\n# with open(output_file, \"w\") as file:\n#     for output in outputs:\n#         file.write(output.to_eval_qr_json_lines())\n\n# # Then you can pass it into our Groundedness evaluator to evaluate it for groundedness\n# groundedness_evaluator = GroundednessEvaluator(model_config=model_config)\n# eval_output = evaluate(\n#     data=output_file,\n#     evaluators={\n#         \"groundedness\": groundedness_evaluator\n#     },\n#     output_path=\"groundedness_eval_output.json\",\n#     azure_ai_project=project_scope # Optional for uploading to your Azure AI Project\n# )\n</pre> # import importlib.resources as pkg_resources  # grounding_simulator = Simulator(model_config=model_config)  # package = \"azure.ai.evaluation.simulator._data_sources\" # resource_name = \"grounding.json\" # conversation_turns = []  # with pkg_resources.path(package, resource_name) as grounding_file: #     with open(grounding_file, \"r\") as file: #         data = json.load(file)  # for item in data: #     conversation_turns.append([item])  # outputs = asyncio.run(grounding_simulator( #     target=callback, #     conversation_turns=conversation_turns, #generates 287 rows of data #     max_conversation_turns=1, # ))  # output_file = \"grounding_simulation_output.jsonl\" # with open(output_file, \"w\") as file: #     for output in outputs: #         file.write(output.to_eval_qr_json_lines())  # # Then you can pass it into our Groundedness evaluator to evaluate it for groundedness # groundedness_evaluator = GroundednessEvaluator(model_config=model_config) # eval_output = evaluate( #     data=output_file, #     evaluators={ #         \"groundedness\": groundedness_evaluator #     }, #     output_path=\"groundedness_eval_output.json\", #     azure_ai_project=project_scope # Optional for uploading to your Azure AI Project # )"},{"location":"workshop/Deployment/","title":"Prerequisites","text":"<p>We will set up the initial environment for you to build on top of during your Microhack. This comprehensive setup includes configuring essential Azure services and ensuring access to all necessary resources. Participants will familiarize themselves with the architecture, gaining insights into how various components interact to create a cohesive solution. With the foundational environment in place, the focus will shift seamlessly to the first Microhack Challenge endeavor. </p>"},{"location":"workshop/Deployment/#prerequisites","title":"Prerequisites","text":"<ul> <li>To deploy this solution accelerator, ensure you have access to an Azure subscription with the necessary permissions to create resource groups and resources. Follow the steps in  Azure Account Set Up</li> <li>VS Code installed locally</li> </ul> <p>Check the Azure Products by Region page and select a region where the following services are available:  </p> <ul> <li>Azure AI Foundry </li> <li>Azure OpenAI Service </li> <li>Azure AI Search</li> <li>Azure AI Content Understanding</li> <li>Embedding Deployment Capacity  </li> <li>GPT Model Capacity</li> <li>Azure Semantic Search </li> </ul> <p>Here are some example regions where the services are available: East US, East US2, Australia East, UK South, France Central.</p>"},{"location":"workshop/Deployment/#important-check-azure-openai-quota-availability","title":"\u26a0\ufe0f Important: Check Azure OpenAI Quota Availability","text":"<p>\u27a1\ufe0f To ensure sufficient quota is available in your subscription, please follow Quota check instructions guide before you deploy the solution.</p>"},{"location":"workshop/Deployment/#optional-quota-recommendations","title":"[Optional] Quota Recommendations","text":"<p>By default, the GPT model capacity in deployment is set to 30k tokens.  </p> <p>We recommend increasing the capacity to 100k tokens for optimal performance. </p> <p>To adjust quota settings, follow these steps </p>"},{"location":"workshop/Deployment/#deploying","title":"Deploying","text":"<p>Once you've opened the project in Codespaces or in Dev Containers or locally, you can deploy it to Azure following the following steps. </p> <p>To change the azd parameters from the default values, follow the steps here. </p> <ol> <li> <p>Login to Azure:</p> Bash<pre><code>azd auth login\n</code></pre> </li> <li> <p>Provision and deploy all the resources:</p> Bash<pre><code>azd up\n</code></pre> </li> <li> <p>Provide an <code>azd</code> environment name (like \"ckmapp\")</p> </li> <li> <p>Select a subscription from your Azure account, and select a location which has quota for all the resources. </p> <ul> <li>This deployment will take 7-10 minutes to provision the resources in your account and set up the solution with sample data. </li> <li>If you get an error or timeout with deployment, changing the location can help, as there may be availability constraints for the resources.</li> </ul> </li> <li> <p>Once the deployment has completed successfully, open the Azure Portal, go to the deployed resource group, find the App Service and get the app URL from <code>Default domain</code>.</p> </li> <li> <p>You can now delete the resources by running <code>azd down</code>, if you are done trying out the application. </p> </li> </ol>  Additional Steps  <ol> <li> <p>Add App Authentication</p> <p>Follow steps in App Authentication to configure authenitcation in app service.</p> <p>Note: Authentication changes can take up to 10 minutes </p> </li> </ol>"},{"location":"workshop/Deployment/#to-authenticate-with-azure-developer-cli-azd-use-the-following-command-with-your-tenant-id","title":"To authenticate with Azure Developer CLI (<code>azd</code>), use the following command with your Tenant ID:","text":"<p><code>sh azd auth login --tenant-id &lt;tenant-id&gt;</code></p>"},{"location":"workshop/Deployment/App-Authentication/","title":"App Authentication","text":""},{"location":"workshop/Deployment/App-Authentication/#add-authentication-in-azure-app-service-configuration","title":"Add Authentication in Azure App Service configuration","text":"<ol> <li>Click on <code>Authentication</code> from left menu.</li> </ol> <ol> <li>Click on <code>+ Add Provider</code> to see a list of identity providers.</li> </ol> <ol> <li>Click on <code>+ Add Provider</code> to see a list of identity providers.</li> </ol> <ol> <li>Select the first option <code>Microsoft Entra Id</code> from the drop-down list.</li> </ol> <ol> <li>Accept the default values and click on <code>Add</code> button to go back to the previous page with the identity provider added.</li> </ol> <p>### Creating a new App Registration 1. Click on <code>Home</code> and select <code>Microsoft Entra ID</code>.</p> <p></p> <ol> <li>Click on <code>App registrations</code>.</li> </ol> <p></p> <ol> <li>Click on <code>+ New registration</code>.</li> </ol> <p></p> <ol> <li>Provide the <code>Name</code>, select supported account types as <code>Accounts in this organizational directory only(Contoso only - Single tenant)</code>, select platform as <code>Web</code>, enter/select the <code>URL</code> and register.</li> </ol> <p></p> <ol> <li>After application is created sucessfully, then click on <code>Add a Redirect URL</code>.</li> </ol> <p></p> <ol> <li>Click on <code>+ Add a platform</code>.</li> </ol> <p></p> <ol> <li>Click on <code>Web</code>.</li> </ol> <p></p> <ol> <li>Enter the <code>web app URL</code> (Provide the app service name in place of XXXX) and Save. Then go back to [Step 4] and follow from Point 4 choose <code>Pick an existing app registration in this directory</code> from the Add an Identity Provider page and provide the newly registered App Name.</li> </ol> <p></p>"},{"location":"workshop/Tear-Down/","title":"Cleanup Resources","text":""},{"location":"workshop/Tear-Down/#give-us-a-on-github","title":"Give us a \u2b50\ufe0f on GitHub","text":"<p>FOUND THIS WORKSHOP AND SAMPLE USEFUL? MAKE SURE YOU GET UPDATES.</p> <p>The Build Your Own Advanced AI Copilot with Postgres sample is an actively updated project that will reflect the latest features and best practices for code-first development of RAG-based copilots on the Azure AI platform. Visit the repo or click the button below, to give us a \u2b50\ufe0f.</p> <p> Give the PostgreSQL Solution Accelerator a Star!</p>"},{"location":"workshop/Tear-Down/#provide-feedback","title":"Provide Feedback","text":"<p>Have feedback that can help us make this lab better for others? Open an issue and let us know.</p>"},{"location":"workshop/Tear-Down/#clean-up","title":"Clean-up","text":"<p>Once you have completed this workshop, delete the Azure resources you created. You are charged for the configured capacity, not how much the resources are used. Follow these instructions to delete your resource group and all resources you created for this solution accelerator.</p> <ol> <li> <p>In VS Code, open a new integrated terminal prompt.</p> </li> <li> <p>At the terminal prompt, execute the following command to delete the resources created by the deployment script:</p> <p>Execute the following Azure Developer CLI command to delete resources!</p> <pre><code>azd down --purge\n</code></pre> <p>The <code>--purge</code> flag purges the resources that provide soft-delete functionality in Azure, including Azure KeyVault and Azure OpenAI. This flag is required to remove all resources completely.</p> </li> <li> <p>In the terminal window, you will be shown a list of the resources that will be deleted and prompted about continuing. Enter \"y\" at the prompt to being the resource deletion.</p> </li> </ol>"},{"location":"workshop/Tear-Down/#persist-changes-to-github","title":"Persist changes to GitHub","text":"<p>If you want to save any changes you have made to files, use the Source Control tool in VS Code to commit and push your changes to your fork of the GitHub repo.</p>"},{"location":"workshop/support-docs/AppAuthentication/","title":"AppAuthentication","text":""},{"location":"workshop/support-docs/AppAuthentication/#add-authentication-in-azure-app-service-configuration","title":"Add Authentication in Azure App Service configuration","text":"<ol> <li>Click on <code>Authentication</code> from left menu.</li> </ol> <ol> <li>Click on <code>+ Add Provider</code> to see a list of identity providers.</li> </ol> <ol> <li>Click on <code>+ Add Provider</code> to see a list of identity providers.</li> </ol> <ol> <li>Select the first option <code>Microsoft Entra Id</code> from the drop-down list.</li> </ol> <ol> <li>Accept the default values and click on <code>Add</code> button to go back to the previous page with the identity provider added.</li> </ol> <p>### Creating a new App Registration 1. Click on <code>Home</code> and select <code>Microsoft Entra ID</code>.</p> <p></p> <ol> <li>Click on <code>App registrations</code>.</li> </ol> <p></p> <ol> <li>Click on <code>+ New registration</code>.</li> </ol> <p></p> <ol> <li>Provide the <code>Name</code>, select supported account types as <code>Accounts in this organizational directory only(Contoso only - Single tenant)</code>, select platform as <code>Web</code>, enter/select the <code>URL</code> and register.</li> </ol> <p></p> <ol> <li>After application is created sucessfully, then click on <code>Add a Redirect URL</code>.</li> </ol> <p></p> <ol> <li>Click on <code>+ Add a platform</code>.</li> </ol> <p></p> <ol> <li>Click on <code>Web</code>.</li> </ol> <p></p> <ol> <li>Enter the <code>web app URL</code> (Provide the app service name in place of XXXX) and Save. Then go back to [Step 4] and follow from Point 4 choose <code>Pick an existing app registration in this directory</code> from the Add an Identity Provider page and provide the newly registered App Name.</li> </ol> <p></p>"},{"location":"workshop/support-docs/AzureAccountSetUp/","title":"AzureAccountSetUp","text":""},{"location":"workshop/support-docs/AzureAccountSetUp/#azure-account-setup","title":"Azure account setup","text":"<ol> <li>Sign up for a free Azure account and create an Azure Subscription.</li> <li>Check that you have the necessary permissions:<ul> <li>Your Azure account must have <code>Microsoft.Authorization/roleAssignments/write</code> permissions, such as Role Based Access Control Administrator, User Access Administrator, or Owner.</li> <li>Your Azure account also needs <code>Microsoft.Resources/deployments/write</code> permissions on the subscription level.</li> </ul> </li> </ol> <p>You can view the permissions for your account and subscription by following the steps below:  - Navigate to the Azure Portal and click on <code>Subscriptions</code> under 'Navigation'  - Select the subscription you are using for this accelerator from the list.      - If you try to search for your subscription and it does not come up, make sure no filters are selected. - Select <code>Access control (IAM)</code> and you can see the roles that are assigned to your account for this subscription.      - If you want to see more information about the roles, you can go to the <code>Role assignments</code>      tab and search by your account name and then click the role you want to view more information about.</p>"},{"location":"workshop/support-docs/AzureGPTQuotaSettings/","title":"AzureGPTQuotaSettings","text":""},{"location":"workshop/support-docs/AzureGPTQuotaSettings/#how-to-check-update-quota","title":"How to Check &amp; Update Quota","text":"<ol> <li>Navigate to the Azure AI Foundry portal.  </li> <li>Select the AI Project associated with this accelerator.  </li> <li>Go to the <code>Management Center</code> from the bottom-left navigation menu.  </li> <li>Select <code>Quota</code> </li> <li>Click on the <code>GlobalStandard</code> dropdown.  </li> <li>Select the required GPT model (<code>GPT-4, GPT-4o, GPT-4o Mini</code>) or Embeddings model (<code>text-embedding-ada-002</code>).  </li> <li>Choose the region where the deployment is hosted.  </li> <li>Request More Quota or delete any unused model deployments as needed.  </li> </ol>"},{"location":"workshop/support-docs/AzureSemanticSearchRegion/","title":"AzureSemanticSearchRegion","text":""},{"location":"workshop/support-docs/AzureSemanticSearchRegion/#select-a-region-where-semantic-search-availability-is-available-before-proceeding-with-the-deployment","title":"Select a region where Semantic Search Availability is available before proceeding with the deployment.","text":"<p>Steps to Check Semantic Search Availability 1. Open the Semantic Search Availability page. 2. Scroll down to the \"Availability by Region\" section. 3. Use the table to find supported regions for Azure AI Search and its Semantic Search feature. 4. If your target region is not listed, choose a supported region for deployment.</p>"},{"location":"workshop/support-docs/ConversationalDataFormat/","title":"ConversationalDataFormat","text":""},{"location":"workshop/support-docs/ConversationalDataFormat/#data-upload-format","title":"Data upload format","text":""},{"location":"workshop/support-docs/ConversationalDataFormat/#audio-file-format","title":"Audio File Format","text":"<p>Azure AI Speech Service is utilized for transcription of conversation audio files. The code is currently configured to support WAV files only, but the code can be modified to support other formats supported by Azure Speech Service. Full details can be found here.</p> <p>We have seen successful transcription of files up to 15MB in size but some very large files may experience processing challenges.</p> <p>Contact center conversations may be uploaded directly as audio to the <code>cu_audio_files_all</code> folder in the Fabric lakehouse. o ensure proper processing, all audio files must follow the specified naming convention. Below is an example of the required format:</p> Text Only<pre><code>convo_03b0e193-5b55-42d3-a258-b0ff9336ae18_2024-12-05 18_00_00.wav\n</code></pre>"},{"location":"workshop/support-docs/ConversationalDataFormat/#naming-convention-breakdown","title":"Naming Convention Breakdown","text":"<ol> <li><code>convo</code>: The prefix that indicates the file contains a contact center conversation.</li> <li>Conversation ID (GUID): A unique identifier for the conversation, represented as a globally unique identifier (GUID).</li> <li>Date and Timestamp: The date and time of the conversation, formatted as <code>YYYY-MM-DD HH_MM_SS</code>.</li> </ol>"},{"location":"workshop/support-docs/ConversationalDataFormat/#json-file-format","title":"JSON File Format","text":"<p>Below is a sample structure of a conversation file. Each sentence or phrase is an individual node followed by summary information for the entire call. These formatted conversation files are smaller size, less costly to process, and faster to process. JSON<pre><code>{\n    \"ClientId\": \"10003\",\n    \"ConversationId\": \"0a7b112e-3e59-4132-82a9-b399f782e859\",\n    \"StartTime\": \"2024-11-22 03:00:00\",\n    \"EndTime\": \"2024-11-22 03:14:00\",\n    \"Content\": \" Agent: Good day, thank you for calling Contoso Inc. This is Chris speaking, how can I assist you today?\\n\\nCustomer (Susan): Hi Chris, I've been having some network coverage and connectivity issues lately. It's been a bit frustrating.\\n\\nAgent: I'm sorry to hear that, Susan. I understand how frustrating it can be when you have connectivity issues. Can you tell me more about the issue you are facing and when it first started?\\n\\nCustomer (Susan): Sure. It's been happening for about a week now. I mostly experience lose of signal when I'm at home. However, the issue doesn't affect my mobile data.\\n\\nAgent: Thank you for sharing that information, Susan. Just to confirm, this issue only occurs when you are at home, correct?\\n\\nCustomer (Susan): Yes, that's correct.\\n\\nAgent: Alright, let's try and see if we can find a resolution for you. Firstly, can you tell me if you have checked your router and modem? Sometimes, network issues can be due to router configuration or problems with the modem.\\n\\nCustomer (Susan): I haven't really checked on that. I'm not really tech-savvy, to be honest.\\n\\nAgent: No problem at all, Susan. I'll guide you through the steps to check your router and modem. Firstly, could you please ensure that your router and modem are properly plugged in and switched on?\\n\\nCustomer (Susan): Just a moment... Okay, everything seems to be plugged in properly, and both the modem and the router are turned on.\\n\\nAgent: Great! Now, could you try unplugging both the modem and router for 10 seconds and then plugging them back in? This process is called power cycling, and it can help reset your devices.\\n\\nCustomer (Susan): Alright, I've done that. Let me check if the internet is working... No, unfortunately, the situation hasn't improved.\\n\\nAgent: Thank you for trying that. Since the issue still persists, I will schedule a technician visit for you. Can you please provide your address?\\n\\nCustomer (Susan): My address is 123 Elm Street.\\n\\nAgent: Thank you, Susan. I have scheduled a technician to visit your location tomorrow between 9am and 11am. They will contact you 15 minutes before arrival.\\n\\nCustomer (Susan): Thank you for arranging that quickly, Chris.\\n\\nAgent: You're welcome, Susan. I apologize for the inconvenience caused. Is there anything else I can assist you with today?\\n\\nCustomer (Susan): No, that covers it. Thank you for your help, Chris.\\n\\nAgent: It's been a pleasure assisting you, Susan. We at Contoso Inc. value your satisfaction and apologize again for the inconvenience. Feel free to call us again if you need any more help. Have a great day!\\n\\nCustomer (Susan): Thank you, you too!\\n\\nAgent: This call is now complete. Thank you for choosing Contoso Inc. Goodbye, Susan!\"\n}\n</code></pre></p>"},{"location":"workshop/support-docs/CustomizingAzdParameters/","title":"CustomizingAzdParameters","text":""},{"location":"workshop/support-docs/CustomizingAzdParameters/#optional-customizing-resource-names","title":"[Optional]: Customizing resource names","text":"<p>By default this template will use the environment name as the prefix to prevent naming collisions within Azure. The parameters below show the default values. You only need to run the statements below if you need to change the values. </p> <p>To override any of the parameters, run <code>azd env set &lt;key&gt; &lt;value&gt;</code> before running <code>azd up</code>. On the first azd command, it will prompt you for the environment name. Be sure to choose 3-20 charaters alphanumeric unique name. </p> <p>Change the Content Understanding Location (allowed values: Sweden Central, Australia East)</p> Bash<pre><code>azd env set AZURE_ENV_CU_LOCATION 'swedencentral'\n</code></pre> <p>Change the Secondary Location (example: eastus2, westus2, etc.)</p> Bash<pre><code>azd env set AZURE_ENV_SECONDARY_LOCATION eastus2\n</code></pre> <p>Change the Model Deployment Type (allowed values: Standard, GlobalStandard)</p> Bash<pre><code>azd env set AZURE_ENV_MODEL_DEPLOYMENT_TYPE GlobalStandard\n</code></pre> <p>Set the Model Name (allowed values: gpt-4o-mini, gpt-4o, gpt-4)</p> Bash<pre><code>azd env set AZURE_ENV_MODEL_NAME gpt-4o-mini\n</code></pre> <p>Change the Model Capacity (choose a number based on available GPT model capacity in your subscription)</p> Bash<pre><code>azd env set AZURE_ENV_MODEL_CAPACITY 30\n</code></pre> <p>Change the Embedding Model </p> Bash<pre><code>azd env set AZURE_ENV_EMBEDDING_MODEL_NAME text-embedding-ada-002\n</code></pre> <p>Change the Embedding Deployment Capacity (choose a number based on available embedding model capacity in your subscription)</p> Bash<pre><code>azd env set AZURE_ENV_EMBEDDING_MODEL_CAPACITY 80\n</code></pre>"},{"location":"workshop/support-docs/DeleteResourceGroup/","title":"Deleting Resources After a Failed Deployment in Azure Portal","text":"<p>If your deployment fails and you need to clean up the resources manually, follow these steps in the Azure Portal.</p>"},{"location":"workshop/support-docs/DeleteResourceGroup/#1-navigate-to-the-azure-portal","title":"1. Navigate to the Azure Portal","text":"<ol> <li>Open Azure Portal.</li> <li>Sign in with your Azure account.</li> </ol>"},{"location":"workshop/support-docs/DeleteResourceGroup/#2-find-the-resource-group","title":"2. Find the Resource Group","text":"<ol> <li>In the search bar at the top, type \"Resource groups\" and select it.</li> <li>Locate the resource group associated with the failed deployment.</li> </ol>"},{"location":"workshop/support-docs/DeleteResourceGroup/#3-delete-the-resource-group","title":"3. Delete the Resource Group","text":"<ol> <li>Click on the resource group name to open it.</li> <li>Click the Delete resource group button at the top.</li> </ol> <ol> <li>Type the resource group name in the confirmation box and click Delete.</li> </ol> <p>\ud83d\udccc Note: Deleting a resource group will remove all resources inside it.</p>"},{"location":"workshop/support-docs/DeleteResourceGroup/#4-delete-individual-resources-if-needed","title":"4. Delete Individual Resources (If Needed)","text":"<p>If you don\u2019t want to delete the entire resource group, follow these steps:</p> <ol> <li>Open Azure Portal and go to the Resource groups section.</li> <li>Click on the specific resource group.</li> <li>Select the resource you want to delete (e.g., App Service, Storage Account).</li> <li>Click Delete at the top.</li> </ol> <p></p>"},{"location":"workshop/support-docs/DeleteResourceGroup/#5-verify-deletion","title":"5. Verify Deletion","text":"<ul> <li>After a few minutes, refresh the Resource groups page.</li> <li>Ensure the deleted resource or group no longer appears.</li> </ul> <p>\ud83d\udccc Tip: If a resource fails to delete, check if it's locked under the Locks section and remove the lock.</p>"},{"location":"workshop/support-docs/Fabric_deployment/","title":"Fabric deployment","text":""},{"location":"workshop/support-docs/Fabric_deployment/#how-to-customize","title":"How to customize","text":"<p>If you'd like to customize the solution accelerator, here are some ways you might do that: - Ingest your own audio conversation files by uploading them into the <code>cu_audio_files_all</code> lakehouse folder and run the data pipeline - Deploy with Microsoft Fabric by following the steps in Fabric_deployment.md</p> <ol> <li> <p>Create Fabric workspace</p> <ol> <li>Navigate to (Fabric Workspace)</li> <li>Click on Data Engineering experience</li> <li>Click on Workspaces from left Navigation</li> <li>Click on + New Workspace<ol> <li>Provide Name of Workspace </li> <li>Provide Description of Workspace (optional)</li> <li>Click Apply</li> </ol> </li> <li>Open Workspace</li> <li>Create Environment<ol> <li>Click <code>+ New Item</code> (in Workspace)</li> <li>Select Environment from list</li> <li>Provide name for Environment and click Create</li> <li>Select Public libraries in left panel</li> <li>Click Add from .yml</li> <li>Upload .yml from here</li> <li>Click Publish</li> </ol> </li> <li>Retrieve Workspace ID from URL, refer to documentation additional assistance (here)</li> </ol> <p>***Note: Wait until the Environment is finished publishing prior to proceeding witht the next steps.</p> </li> <li> <p>Deploy Fabric resources and artifacts</p> <ol> <li>Navigate to (Azure Portal)</li> <li>Click on Azure Cloud Shell in the top right of navigation Menu (add image)</li> <li>Run the run the following commands:  <ol> <li><code>az login</code> ***Follow instructions in Azure Cloud Shell for login instructions</li> <li><code>rm -rf ./Conversation-Knowledge-Mining-Solution-Accelerator</code></li> <li><code>git clone https://github.com/microsoft/Conversation-Knowledge-Mining-Solution-Accelerator</code></li> <li><code>cd ./Conversation-Knowledge-Mining-Solution-Accelerator/Deployment/scripts/fabric_scripts</code></li> <li><code>sh ./run_fabric_items_scripts.sh keyvault_param workspaceid_param solutionprefix_param</code><ol> <li>keyvault_param - the name of the keyvault that was created in Step 1</li> <li>workspaceid_param - the workspaceid created in Step 2</li> <li>solutionprefix_param - prefix used to append to lakehouse upon creation</li> </ol> </li> </ol> </li> </ol> </li> <li> <p>Add App Authentication</p> <p>Follow steps in App Authentication to configure authenitcation in app service.</p> </li> </ol>"},{"location":"workshop/support-docs/Fabric_deployment/#upload-additional-files","title":"Upload additional files","text":"<p>All files WAV files can be uploaded in the corresponding Lakehouse in the data/Files folder:</p> <ul> <li>Audio (WAV files):   Upload Audio files in the cu_audio_files_all folder.</li> </ul>"},{"location":"workshop/support-docs/Fabric_deployment/#post-deployment","title":"Post-deployment","text":"<ul> <li>To process additional files, manually execute the pipeline_notebook after uploading new files.</li> <li>The OpenAI prompt can be modified within the Fabric notebooks.</li> </ul>"},{"location":"workshop/support-docs/quota_check/","title":"Quota check","text":""},{"location":"workshop/support-docs/quota_check/#check-quota-availability-before-deployment","title":"Check Quota Availability Before Deployment","text":"<p>Before deploying the accelerator, ensure sufficient quota availability for the required model. Use one of the following scripts based on your needs:  </p> <ul> <li><code>quota_check_params.sh</code> \u2192 If you know the model and capacity required.  </li> <li><code>quota_check_all_regions.sh</code> \u2192 If you want to check available capacity across all regions for supported models.  </li> </ul>"},{"location":"workshop/support-docs/quota_check/#if-using-azure-portal-and-cloud-shell","title":"If using Azure Portal and Cloud Shell","text":"<ol> <li>Navigate to the Azure Portal.</li> <li>Click on Azure Cloud Shell in the top right navigation menu.</li> <li>Run the appropriate command based on your requirement:  </li> </ol> <p>To check quota for a specific model and capacity: </p> Text Only<pre><code>```sh\ncurl -L -o quota_check_params.sh \"https://raw.githubusercontent.com/microsoft/Conversation-Knowledge-Mining-Solution-Accelerator/main/infra/scripts/quota_check_params.sh\"\nchmod +x quota_check_params.sh\n./quota_check_params.sh &lt;model_name:capacity&gt; [&lt;model_region&gt;] (e.g., gpt-4o-mini:30,text-embedding-ada-002:20 eastus)\n```\n</code></pre> <p>To check available quota across all regions for supported models: </p> Text Only<pre><code>```sh\ncurl -L -o quota_check_all_regions.sh \"https://raw.githubusercontent.com/microsoft/Conversation-Knowledge-Mining-Solution-Accelerator/main/infra/scripts/quota_check_all_regions.sh\"\nchmod +x quota_check_all_regions.sh\n./quota_check_all_regions.sh\n```\n</code></pre>"},{"location":"workshop/support-docs/quota_check/#if-using-vs-code-or-codespaces","title":"If using VS Code or Codespaces","text":"<ol> <li>Run the appropriate script based on your requirement:  </li> </ol> <p>To check quota for a specific model and capacity: </p> Text Only<pre><code>```sh\n./quota_check_params.sh &lt;model_name:capacity&gt; [&lt;model_region&gt;] (e.g., gpt-4o-mini:30,text-embedding-ada-002:20 eastus)\n```\n</code></pre> <p>To check available quota across all regions for supported models: </p> Text Only<pre><code>```sh\n./quota_check_all_regions.sh\n```\n</code></pre> <ol> <li> <p>If you see the error <code>_bash: az: command not found_</code>, install Azure CLI:  </p> <p>Bash<pre><code>curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash\naz login\n</code></pre> 3. Rerun the script after installing Azure CLI.</p> <p>Parameters - <code>&lt;model_name:capacity&gt;</code>: The name and required capacity for each model, in the format model_name:capacity (e.g., gpt-4o-mini:30,text-embedding-ada-002:20). - <code>[&lt;model_region&gt;] (optional)</code>: The Azure region to check first. If not provided, all supported regions will be checked (e.g., eastus).</p> </li> </ol>"}]}