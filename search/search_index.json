{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Knowledge Mining Microhack: Hands-on Workshop","text":""},{"location":"#overview","title":"Overview","text":"<p>This solution accelerator enables customers with large amounts of conversational data to improve decision-making by leveraging intelligence to uncover insights, relationships, and patterns from customer interactions. It empowers users to gain valuable knowledge and drive targeted business impact. </p> <p>It leverages Azure AI Foundry, Azure AI Content Understanding, Azure OpenAI Service, and Azure AI Search to transform large volumes of conversational data into actionable insights through topic modeling, key phrase extraction, speech-to-text transcription, and interactive chat experiences.</p>"},{"location":"#technical-key-features","title":"Technical key features","text":""},{"location":"#use-case-scenario","title":"Use case / scenario","text":"<p>An analyst managing large volumes of conversational data needs a solution to visualize key insights and uncover patterns using natural language. An interactive dashboard enables them to explore rich, actionable insights for faster, and more informed decision-making.</p>"},{"location":"workshop/docs/","title":"Introduction","text":"<p>The Micohack event is designed to engage technical roles through a condensed, half-day hands-on hack experience. Leveraging the latest Microsoft technologies, this event provides participants with the opportunity to work on real-world problems, collaborate with peers, and explore innovative solutions. </p> <p>The Microhack event is divided into several key challenges, each carefully crafted to test and expand the participants' proficiency with Microsoft's suite of tools. These challenges are not only technical in nature but also reflect real-world scenarios that businesses face, providing a comprehensive understanding of how to apply theoretical knowledge practically. </p>"},{"location":"workshop/docs/#hack-duration-2-hours","title":"Hack Duration: 2 hours","text":"<p>The event kicks off with an initial overview of the customer scenario for the business problem the participants will solve by leveraging cutting-edge technology and services.  </p> <p>Following this, the team will complete the setup phase, where participants ensure that their development environments are correctly configured, and all necessary tools are ready for use.  </p> <p>Finally, they will tackle the first challenge, which involves identifying key ideas that underpin the implementation of Microsoft technologies in solving predefined problems. </p>"},{"location":"workshop/docs/00-Use-Case-Scenerio/","title":"Scenerio","text":"<p>An analyst managing large volumes of conversational data needs a solution to visualize key insights and uncover patterns using natural language. An interactive dashboard enables them to explore rich, actionable insights for faster, and more informed decision-making.</p> <p>This solution empowers analysts with tools to ask questions and receive real-time, contextualized responses. It streamlines problem-solving, enhances collaboration, and fosters innovation by making data-driven insights accessible and shareable.</p> <p>The sample data used in this repository is synthetic and generated using Azure OpenAI service. The data is intended for use as sample data only.</p>"},{"location":"workshop/docs/00-Use-Case-Scenerio/#knowledge-mining","title":"Knowledge Mining","text":"<p>Knowledge Mining enables customers with large amounts of conversational data to improve decision-making by leveraging intelligence to uncover insights, relationships, and patterns from customer interactions. It empowers users to gain valuable knowledge and drive targeted business impact. </p> <p>This template leverages Azure AI Foundry, Azure AI Content Understanding, Azure OpenAI Service, and Azure AI Search to transform large volumes of conversational data into actionable insights through topic modeling, key phrase extraction, speech-to-text transcription, and interactive chat experiences.</p>"},{"location":"workshop/docs/Challenge-0/","title":"Enviroment Setup","text":""},{"location":"workshop/docs/Challenge-0/#prerequisites","title":"Prerequisites","text":"<p>To deploy this solution accelerator, ensure you have access to an Azure subscription with the necessary permissions to create resource groups and resources. Follow the steps in  Azure Account Set Up</p> <p>Check the Azure Products by Region page and select a region where the following services are available:  </p> <ul> <li>Azure AI Foundry </li> <li>Azure OpenAI Service </li> <li>Azure AI Search</li> <li>Azure AI Content Understanding</li> <li>Embedding Deployment Capacity  </li> <li>GPT Model Capacity</li> <li>Azure Semantic Search </li> </ul> <p>Here are some example regions where the services are available: East US, East US2, Australia East, UK South, France Central.</p>"},{"location":"workshop/docs/Challenge-0/#important-check-azure-openai-quota-availability","title":"\u26a0\ufe0f Important: Check Azure OpenAI Quota Availability","text":"<p>\u27a1\ufe0f To ensure sufficient quota is available in your subscription, please follow Quota check instructions guide before you deploy the solution.</p>"},{"location":"workshop/docs/Challenge-0/#deploying","title":"Deploying","text":"<p>Once you've opened the project in Codespaces or in Dev Containers or locally, you can deploy it to Azure following the following steps. </p> <p>To change the azd parameters from the default values, follow the steps here. </p> <ol> <li> <p>Login to Azure:</p> Bash<pre><code>azd auth login\n</code></pre> </li> <li> <p>Provision and deploy all the resources:</p> Bash<pre><code>azd up\n</code></pre> </li> <li> <p>Provide an <code>azd</code> environment name (like \"ckmapp\")</p> </li> <li> <p>Select a subscription from your Azure account, and select a location which has quota for all the resources. </p> <ul> <li>This deployment will take 7-10 minutes to provision the resources in your account and set up the solution with sample data. </li> <li>If you get an error or timeout with deployment, changing the location can help, as there may be availability constraints for the resources.</li> </ul> </li> <li> <p>Once the deployment has completed successfully, open the Azure Portal, go to the deployed resource group, find the App Service and get the app URL from <code>Default domain</code>.</p> </li> <li> <p>You can now delete the resources by running <code>azd down</code>, if you are done trying out the application. </p> </li> </ol>  Additional Steps  <ol> <li> <p>Add App Authentication</p> <p>Follow steps in App Authentication to configure authenitcation in app service.</p> <p>Note: Authentication changes can take up to 10 minutes </p> </li> </ol>"},{"location":"workshop/docs/Challenge-0/#to-authenticate-with-azure-developer-cli-azd-use-the-following-command-with-your-tenant-id","title":"To authenticate with Azure Developer CLI (<code>azd</code>), use the following command with your Tenant ID:","text":"<p><code>sh azd auth login --tenant-id &lt;tenant-id&gt;</code></p>"},{"location":"workshop/docs/Challenge-0/App-Authentication/","title":"App Authentication","text":""},{"location":"workshop/docs/Challenge-0/App-Authentication/#add-authentication-in-azure-app-service-configuration","title":"Add Authentication in Azure App Service configuration","text":"<ol> <li>Click on <code>Authentication</code> from left menu.</li> </ol> <ol> <li>Click on <code>+ Add Provider</code> to see a list of identity providers.</li> </ol> <ol> <li>Click on <code>+ Add Provider</code> to see a list of identity providers.</li> </ol> <ol> <li>Select the first option <code>Microsoft Entra Id</code> from the drop-down list.</li> </ol> <ol> <li>Accept the default values and click on <code>Add</code> button to go back to the previous page with the identity provider added.</li> </ol> <p>### Creating a new App Registration 1. Click on <code>Home</code> and select <code>Microsoft Entra ID</code>.</p> <p></p> <ol> <li>Click on <code>App registrations</code>.</li> </ol> <p></p> <ol> <li>Click on <code>+ New registration</code>.</li> </ol> <p></p> <ol> <li>Provide the <code>Name</code>, select supported account types as <code>Accounts in this organizational directory only(Contoso only - Single tenant)</code>, select platform as <code>Web</code>, enter/select the <code>URL</code> and register.</li> </ol> <p></p> <ol> <li>After application is created sucessfully, then click on <code>Add a Redirect URL</code>.</li> </ol> <p></p> <ol> <li>Click on <code>+ Add a platform</code>.</li> </ol> <p></p> <ol> <li>Click on <code>Web</code>.</li> </ol> <p></p> <ol> <li>Enter the <code>web app URL</code> (Provide the app service name in place of XXXX) and Save. Then go back to [Step 4] and follow from Point 4 choose <code>Pick an existing app registration in this directory</code> from the Add an Identity Provider page and provide the newly registered App Name.</li> </ol> <p></p>"},{"location":"workshop/docs/Challenge-1/","title":"Explore Data","text":"<p>Before using the dashboard, explore the existing raw data manually. What challenges do you encounter in identifying trends and patterns? How does the manual exploration compare to using the Interactive Insights Dashboard? </p>"},{"location":"workshop/docs/Challenge-1/#steps","title":"Steps","text":"Text Only<pre><code>1. Check challenge sucess criteria before starting\n2. in a private web browser, open your deployment\n</code></pre>"},{"location":"workshop/docs/Challenge-2/","title":"Explore the code","text":"<p>The Conversation Knowledge Mining Solution Accelerator is a robust application designed to extract actionable insights from conversational data. It leverages Azure AI services and provides an interactive user interface for querying and visualizing data. The solution is built with a modular architecture, combining a React-based frontend, a FastAPI backend, and Azure services for data processing and storage.</p>"},{"location":"workshop/docs/Challenge-2/#key-features","title":"Key Features","text":""},{"location":"workshop/docs/Challenge-2/#data-processing-and-analysis","title":"Data Processing and Analysis:","text":"<ul> <li>Processes conversational data using Azure AI Foundry, Azure AI Content Understanding, and Azure OpenAI Service.</li> <li>Extracts insights such as sentiment, key phrases, and topics from conversations.</li> <li>Supports speech-to-text transcription for audio data.</li> </ul>"},{"location":"workshop/docs/Challenge-2/#dynamic-dashboard","title":"Dynamic Dashboard:","text":"<ul> <li>Visualizes insights through various chart types (e.g., Donut Chart, Bar Chart, Word Cloud).</li> <li>Enables filtering and customization of data views.</li> <li>Provides a responsive layout for seamless user experience.</li> </ul>"},{"location":"workshop/docs/Challenge-2/#interactive-chat-interface","title":"Interactive Chat Interface:","text":"<ul> <li>Allows users to query data in natural language and receive real-time responses.</li> <li>Supports both text-based and chart-based responses.</li> <li>Integrates with Azure OpenAI and Azure Cognitive Search for generating responses and retrieving relevant data.</li> </ul>"},{"location":"workshop/docs/Challenge-2/#backend-api","title":"Backend API:","text":"<ul> <li>Built with FastAPI for handling requests and integrating with Azure services.</li> <li>Includes modular routes for backend operations and conversation history management.</li> <li>Provides a health check endpoint for monitoring service status.</li> </ul>"},{"location":"workshop/docs/Challenge-2/#scalable-deployment","title":"Scalable Deployment:","text":"<ul> <li>Supports deployment via GitHub Codespaces, VS Code Dev Containers, or local environments.</li> <li>Includes configurable deployment settings for regions, models, and resource capacities.</li> </ul>"},{"location":"workshop/docs/Challenge-2/Chart-Component/","title":"Chart Component","text":""},{"location":"workshop/docs/Challenge-2/Chart-Component/#chart-component-walkthrough","title":"Chart Component Walkthrough","text":"<p>This document provides a comprehensive overview of the <code>Chart</code> component in the Conversation Knowledge Mining Solution Accelerator. It explains how the component works, its interaction with other parts of the application, and its relevance to both technical and non-technical users.</p>"},{"location":"workshop/docs/Challenge-2/Chart-Component/#overview-of-the-chart-component","title":"Overview of the Chart Component","text":"<p>The <code>Chart</code> component is a dynamic and interactive visualization module that displays insights derived from conversation data. It supports multiple chart types (e.g., Donut Chart, Bar Chart, Word Cloud, etc.) and allows users to filter and explore data interactively.</p>"},{"location":"workshop/docs/Challenge-2/Chart-Component/#key-features","title":"Key Features","text":"<ol> <li>Dynamic Chart Rendering:</li> <li>Supports multiple chart types such as Donut Charts, Bar Charts, Word Clouds, and Tables.</li> <li> <p>Automatically adjusts layout and size based on the window dimensions.</p> </li> <li> <p>Interactive Filtering:</p> </li> <li> <p>Users can apply filters (e.g., Topics, Sentiments, Satisfaction levels) to refine the data displayed in the charts.</p> </li> <li> <p>Real-Time Data Updates:</p> </li> <li> <p>Fetches data dynamically from the backend based on user interactions and updates the charts in real-time.</p> </li> <li> <p>Responsive Design:</p> </li> <li> <p>Automatically adjusts chart layouts and dimensions to fit the screen size.</p> </li> <li> <p>Error Handling:</p> </li> <li>Gracefully handles errors during data fetching and displays fallback messages when necessary.</li> </ol>"},{"location":"workshop/docs/Challenge-2/Chart-Component/#how-it-works","title":"How It Works","text":""},{"location":"workshop/docs/Challenge-2/Chart-Component/#1-data-flow","title":"1. Data Flow","text":"<p>The <code>Chart</code> component interacts with the backend and other frontend modules to fetch, process, and display data:</p> <ol> <li>Frontend (<code>Chart</code> Component):</li> <li>Fetches chart data and filter options using API utility functions (<code>api.ts</code>).</li> <li> <p>Dynamically renders charts based on the data and configuration.</p> </li> <li> <p>Backend (<code>function_app.py</code>):</p> </li> <li>Provides APIs (<code>/api/fetchChartData</code>, <code>/api/fetchChartDataWithFilters</code>, <code>/api/fetchFilterData</code>) to serve chart data and filter options.</li> <li> <p>Dynamically generates SQL queries based on user-selected filters.</p> </li> <li> <p>API Utility (<code>api.ts</code>):</p> </li> <li>Acts as a bridge between the frontend and backend.</li> <li>Provides reusable functions to fetch chart data, filter data, and layout configurations.</li> </ol>"},{"location":"workshop/docs/Challenge-2/Chart-Component/#2-key-components","title":"2. Key Components","text":""},{"location":"workshop/docs/Challenge-2/Chart-Component/#frontend-chart-component","title":"Frontend: Chart Component","text":"<p>Responsibilities: - Fetches chart data using <code>getChartData</code> and <code>fetchChartDataWithFilters</code>. - Dynamically renders charts based on their type (e.g., Donut Chart, Word Cloud). - Handles user interactions such as applying filters and resizing the window.</p>"},{"location":"workshop/docs/Challenge-2/Chart-Component/#backend-azure-function","title":"Backend: Azure Function","text":"<p>Responsibilities: - Connects to the SQL database using secure credentials. - Provides APIs to fetch chart data and filter options. - Dynamically builds SQL queries based on user-selected filters.</p>"},{"location":"workshop/docs/Challenge-2/Chart-Component/#api-utility","title":"API Utility","text":"<p>Responsibilities: - Provides reusable functions to interact with backend APIs. - Handles API calls for fetching chart data, filter data, and layout configurations. - Manages error handling and response parsing.</p>"},{"location":"workshop/docs/Challenge-2/Chart-Component/#3-technical-details","title":"3. Technical Details","text":""},{"location":"workshop/docs/Challenge-2/Chart-Component/#dynamic-chart-rendering","title":"Dynamic Chart Rendering","text":"<p>The <code>Chart</code> component uses the <code>renderChart</code> function to render different chart types dynamically: <code>``tsx const renderChart = (chart: ChartConfigItem, heightInPixes: number) =&gt; {   switch (chart.type) {     case \"card\":       return &lt;Card ... /&gt;;     case \"donutchart\":       return &lt;DonutChart ... /&gt;;     case \"bar\":       return &lt;BarChart ... /&gt;;     case \"table\":       return &lt;TopicTable ... /&gt;;     case \"wordcloud\":       return &lt;WordCloudChart ... /&gt;;     default:       console.warn(</code>Unknown chart type: ${chart.type}`);       return null;   } };</p>"},{"location":"workshop/docs/Challenge-2/Chat-Component/","title":"Chat Component","text":"<p>The Chat component integrates seamlessly with the backend to provide a robust and dynamic chat experience. It supports:</p>"},{"location":"workshop/docs/Challenge-2/Chat-Component/#general-chat","title":"General Chat:","text":"Text Only<pre><code>- The greeting function handles general queries and greetings.\n- It uses Azure OpenAI to generate responses, either via the AIProjectClient or directly using the openai library.\n</code></pre>"},{"location":"workshop/docs/Challenge-2/Chat-Component/#sql-query-generation","title":"SQL Query Generation:","text":"Text Only<pre><code>- The get_SQL_Response function dynamically generates SQL queries based on user input.\n- It uses Azure OpenAI to create the SQL query and executes it against the database using the execute_sql_query function.\n</code></pre>"},{"location":"workshop/docs/Challenge-2/Chat-Component/#azure-cognitive-search","title":"Azure Cognitive Search:","text":"Text Only<pre><code>- The get_answers_from_calltranscripts function retrieves answers from indexed call transcripts using Azure Cognitive Search.\n- It uses a hybrid query approach (vector_simple_hybrid) to combine semantic and vector-based search.\n</code></pre>"},{"location":"workshop/docs/Challenge-2/Chat-Component/#chart-data-generation","title":"Chart Data Generation:","text":"Text Only<pre><code>- The process_rag_response function in chat_helper.py dynamically generates chart data from RAG responses.\n- It uses Azure OpenAI to process the RAG response and generate valid JSON data compatible with Chart.js.\n</code></pre>"},{"location":"workshop/docs/Challenge-3/","title":"Overview","text":""},{"location":"workshop/docs/Challenge-3/#explore-dashboard-using-natural-language-queries","title":"Explore Dashboard using Natural Language queries","text":"<p>To help you get started, here are some Sample Questions you can ask in the app:</p> <ul> <li>Total number of calls by date for the last 7 days</li> <li>Show average handling time by topics in minutes</li> <li>What are the top 7 challenges users reported?</li> <li>Give a summary of billing issues</li> <li>When customers call in about unexpected charges, what types of charges are they seeing?</li> </ul> <p>These questions serve as a great starting point to explore insights from the data.</p>"},{"location":"workshop/docs/Challenge-4-and-5/","title":"Orchestrator change","text":"Using your own data? <p>Incorporating your own data into the solution accelerator requires adapting the existing architecture to align with your specific data structures. Here are some recommendations:</p> <p>1. Implement Design Patterns and LangChain in Your Solution To effectively integrate AI capabilities, you need to incorporate design patterns that facilitate seamless interaction between your data and AI models. Utilizing LangChain can help in constructing these patterns, enabling efficient data processing and AI orchestration.</p> <p>2. Customize the <code>chat_functions.py</code> file The <code>chat_functions.py</code> file serves as a bridge between the user inputs and AI responses. To tailor this to your data:</p> <ul> <li>Understand the Existing Structure: Review the current implementation to comprehend how data flows and functions are structured.</li> <li>Map Your Data: Identify how your data schema aligns with the existing functions.</li> <li>Modify Functions: Adjust or rewrite functions to query and process your data appropriately, ensuring that the AI services can accurately interpret and respond based on your dataset.</li> </ul> <p>In this section, you will add an AI copilot to the Woodgrove Bank Contract Management application using Python, the GenAI capabilities of Azure Database for PostgreSQL - Flexible Server, and the Azure AI extension. Using the AI-validated data, the copilot will use RAG to provide insights and answer questions about vendor contract performance and invoicing accuracy, serving as an intelligent assistant for Woodgrove Banks users. Here's what you will accomplish:</p> <ul> <li> Explore the API codebase</li> <li> Review the RAG design</li> <li> Leverage LangChain Orchestration</li> <li> Implement and test the Chat endpoint</li> <li> Refine the copilot prompt using standard prompt engineering techniques</li> <li> Add and test the Copilot Chat UI component</li> </ul> <p>Following these steps will transform your application into a powerful AI-enhanced platform capable of executing advanced generative AI tasks and providing deeper insights from your data.</p>"},{"location":"workshop/docs/Challenge-4-and-5/#what-are-copilots","title":"What are copilots?","text":"<p>Copilots are advanced AI assistants designed to augment human capabilities and improve productivity by providing intelligent, context-aware support, automating repetitive tasks, and enhancing decision-making processes. For instance, the Woodgrove Bank copilot will assist in data analysis, helping users identify patterns and trends in financial datasets.</p>"},{"location":"workshop/docs/Challenge-4-and-5/#why-use-python","title":"Why use Python?","text":"<p>Python's simplicity and readability make it a popular programming language for AI and machine learning projects. Its extensive libraries and frameworks, such as LangChain, FastAPI, and many others, provide robust tools for developing sophisticated copilots. Python's versatility allows developers to iterate and experiment quickly, making it a top choice for building AI applications.</p>"},{"location":"workshop/docs/Challenge-6/","title":"Video Processing","text":""},{"location":"workshop/docs/Challenge-7/","title":"Application Evaluation","text":""},{"location":"workshop/docs/Tear-Down/","title":"Cleanup Resources","text":""},{"location":"workshop/docs/Tear-Down/#give-us-a-on-github","title":"Give us a \u2b50\ufe0f on GitHub","text":"<p>FOUND THIS WORKSHOP AND SAMPLE USEFUL? MAKE SURE YOU GET UPDATES.</p> <p>The Build Your Own Advanced AI Copilot with Postgres sample is an actively updated project that will reflect the latest features and best practices for code-first development of RAG-based copilots on the Azure AI platform. Visit the repo or click the button below, to give us a \u2b50\ufe0f.</p> <p> Give the PostgreSQL Solution Accelerator a Star!</p>"},{"location":"workshop/docs/Tear-Down/#provide-feedback","title":"Provide Feedback","text":"<p>Have feedback that can help us make this lab better for others? Open an issue and let us know.</p>"},{"location":"workshop/docs/Tear-Down/#clean-up","title":"Clean-up","text":"<p>Once you have completed this workshop, delete the Azure resources you created. You are charged for the configured capacity, not how much the resources are used. Follow these instructions to delete your resource group and all resources you created for this solution accelerator.</p> <ol> <li> <p>In VS Code, open a new integrated terminal prompt.</p> </li> <li> <p>At the terminal prompt, execute the following command to delete the resources created by the deployment script:</p> <p>Execute the following Azure Developer CLI command to delete resources!</p> <pre><code>azd down --purge\n</code></pre> <p>The <code>--purge</code> flag purges the resources that provide soft-delete functionality in Azure, including Azure KeyVault and Azure OpenAI. This flag is required to remove all resources completely.</p> </li> <li> <p>In the terminal window, you will be shown a list of the resources that will be deleted and prompted about continuing. Enter \"y\" at the prompt to being the resource deletion.</p> </li> </ol>"},{"location":"workshop/docs/Tear-Down/#persist-changes-to-github","title":"Persist changes to GitHub","text":"<p>If you want to save any changes you have made to files, use the Source Control tool in VS Code to commit and push your changes to your fork of the GitHub repo.</p>"},{"location":"workshop/support-docs/AppAuthentication/","title":"AppAuthentication","text":""},{"location":"workshop/support-docs/AppAuthentication/#add-authentication-in-azure-app-service-configuration","title":"Add Authentication in Azure App Service configuration","text":"<ol> <li>Click on <code>Authentication</code> from left menu.</li> </ol> <ol> <li>Click on <code>+ Add Provider</code> to see a list of identity providers.</li> </ol> <ol> <li>Click on <code>+ Add Provider</code> to see a list of identity providers.</li> </ol> <ol> <li>Select the first option <code>Microsoft Entra Id</code> from the drop-down list.</li> </ol> <ol> <li>Accept the default values and click on <code>Add</code> button to go back to the previous page with the identity provider added.</li> </ol> <p>### Creating a new App Registration 1. Click on <code>Home</code> and select <code>Microsoft Entra ID</code>.</p> <p></p> <ol> <li>Click on <code>App registrations</code>.</li> </ol> <p></p> <ol> <li>Click on <code>+ New registration</code>.</li> </ol> <p></p> <ol> <li>Provide the <code>Name</code>, select supported account types as <code>Accounts in this organizational directory only(Contoso only - Single tenant)</code>, select platform as <code>Web</code>, enter/select the <code>URL</code> and register.</li> </ol> <p></p> <ol> <li>After application is created sucessfully, then click on <code>Add a Redirect URL</code>.</li> </ol> <p></p> <ol> <li>Click on <code>+ Add a platform</code>.</li> </ol> <p></p> <ol> <li>Click on <code>Web</code>.</li> </ol> <p></p> <ol> <li>Enter the <code>web app URL</code> (Provide the app service name in place of XXXX) and Save. Then go back to [Step 4] and follow from Point 4 choose <code>Pick an existing app registration in this directory</code> from the Add an Identity Provider page and provide the newly registered App Name.</li> </ol> <p></p>"},{"location":"workshop/support-docs/AzureAccountSetUp/","title":"AzureAccountSetUp","text":""},{"location":"workshop/support-docs/AzureAccountSetUp/#azure-account-setup","title":"Azure account setup","text":"<ol> <li>Sign up for a free Azure account and create an Azure Subscription.</li> <li>Check that you have the necessary permissions:<ul> <li>Your Azure account must have <code>Microsoft.Authorization/roleAssignments/write</code> permissions, such as Role Based Access Control Administrator, User Access Administrator, or Owner.</li> <li>Your Azure account also needs <code>Microsoft.Resources/deployments/write</code> permissions on the subscription level.</li> </ul> </li> </ol> <p>You can view the permissions for your account and subscription by following the steps below:  - Navigate to the Azure Portal and click on <code>Subscriptions</code> under 'Navigation'  - Select the subscription you are using for this accelerator from the list.      - If you try to search for your subscription and it does not come up, make sure no filters are selected. - Select <code>Access control (IAM)</code> and you can see the roles that are assigned to your account for this subscription.      - If you want to see more information about the roles, you can go to the <code>Role assignments</code>      tab and search by your account name and then click the role you want to view more information about.</p>"},{"location":"workshop/support-docs/AzureGPTQuotaSettings/","title":"AzureGPTQuotaSettings","text":""},{"location":"workshop/support-docs/AzureGPTQuotaSettings/#how-to-check-update-quota","title":"How to Check &amp; Update Quota","text":"<ol> <li>Navigate to the Azure AI Foundry portal.  </li> <li>Select the AI Project associated with this accelerator.  </li> <li>Go to the <code>Management Center</code> from the bottom-left navigation menu.  </li> <li>Select <code>Quota</code> </li> <li>Click on the <code>GlobalStandard</code> dropdown.  </li> <li>Select the required GPT model (<code>GPT-4, GPT-4o, GPT-4o Mini</code>) or Embeddings model (<code>text-embedding-ada-002</code>).  </li> <li>Choose the region where the deployment is hosted.  </li> <li>Request More Quota or delete any unused model deployments as needed.  </li> </ol>"},{"location":"workshop/support-docs/AzureSemanticSearchRegion/","title":"AzureSemanticSearchRegion","text":""},{"location":"workshop/support-docs/AzureSemanticSearchRegion/#select-a-region-where-semantic-search-availability-is-available-before-proceeding-with-the-deployment","title":"Select a region where Semantic Search Availability is available before proceeding with the deployment.","text":"<p>Steps to Check Semantic Search Availability 1. Open the Semantic Search Availability page. 2. Scroll down to the \"Availability by Region\" section. 3. Use the table to find supported regions for Azure AI Search and its Semantic Search feature. 4. If your target region is not listed, choose a supported region for deployment.</p>"},{"location":"workshop/support-docs/ConversationalDataFormat/","title":"ConversationalDataFormat","text":""},{"location":"workshop/support-docs/ConversationalDataFormat/#data-upload-format","title":"Data upload format","text":""},{"location":"workshop/support-docs/ConversationalDataFormat/#audio-file-format","title":"Audio File Format","text":"<p>Azure AI Speech Service is utilized for transcription of conversation audio files. The code is currently configured to support WAV files only, but the code can be modified to support other formats supported by Azure Speech Service. Full details can be found here.</p> <p>We have seen successful transcription of files up to 15MB in size but some very large files may experience processing challenges.</p> <p>Contact center conversations may be uploaded directly as audio to the <code>cu_audio_files_all</code> folder in the Fabric lakehouse. o ensure proper processing, all audio files must follow the specified naming convention. Below is an example of the required format:</p> Text Only<pre><code>convo_03b0e193-5b55-42d3-a258-b0ff9336ae18_2024-12-05 18_00_00.wav\n</code></pre>"},{"location":"workshop/support-docs/ConversationalDataFormat/#naming-convention-breakdown","title":"Naming Convention Breakdown","text":"<ol> <li><code>convo</code>: The prefix that indicates the file contains a contact center conversation.</li> <li>Conversation ID (GUID): A unique identifier for the conversation, represented as a globally unique identifier (GUID).</li> <li>Date and Timestamp: The date and time of the conversation, formatted as <code>YYYY-MM-DD HH_MM_SS</code>.</li> </ol>"},{"location":"workshop/support-docs/ConversationalDataFormat/#json-file-format","title":"JSON File Format","text":"<p>Below is a sample structure of a conversation file. Each sentence or phrase is an individual node followed by summary information for the entire call. These formatted conversation files are smaller size, less costly to process, and faster to process. JSON<pre><code>{\n    \"ClientId\": \"10003\",\n    \"ConversationId\": \"0a7b112e-3e59-4132-82a9-b399f782e859\",\n    \"StartTime\": \"2024-11-22 03:00:00\",\n    \"EndTime\": \"2024-11-22 03:14:00\",\n    \"Content\": \" Agent: Good day, thank you for calling Contoso Inc. This is Chris speaking, how can I assist you today?\\n\\nCustomer (Susan): Hi Chris, I've been having some network coverage and connectivity issues lately. It's been a bit frustrating.\\n\\nAgent: I'm sorry to hear that, Susan. I understand how frustrating it can be when you have connectivity issues. Can you tell me more about the issue you are facing and when it first started?\\n\\nCustomer (Susan): Sure. It's been happening for about a week now. I mostly experience lose of signal when I'm at home. However, the issue doesn't affect my mobile data.\\n\\nAgent: Thank you for sharing that information, Susan. Just to confirm, this issue only occurs when you are at home, correct?\\n\\nCustomer (Susan): Yes, that's correct.\\n\\nAgent: Alright, let's try and see if we can find a resolution for you. Firstly, can you tell me if you have checked your router and modem? Sometimes, network issues can be due to router configuration or problems with the modem.\\n\\nCustomer (Susan): I haven't really checked on that. I'm not really tech-savvy, to be honest.\\n\\nAgent: No problem at all, Susan. I'll guide you through the steps to check your router and modem. Firstly, could you please ensure that your router and modem are properly plugged in and switched on?\\n\\nCustomer (Susan): Just a moment... Okay, everything seems to be plugged in properly, and both the modem and the router are turned on.\\n\\nAgent: Great! Now, could you try unplugging both the modem and router for 10 seconds and then plugging them back in? This process is called power cycling, and it can help reset your devices.\\n\\nCustomer (Susan): Alright, I've done that. Let me check if the internet is working... No, unfortunately, the situation hasn't improved.\\n\\nAgent: Thank you for trying that. Since the issue still persists, I will schedule a technician visit for you. Can you please provide your address?\\n\\nCustomer (Susan): My address is 123 Elm Street.\\n\\nAgent: Thank you, Susan. I have scheduled a technician to visit your location tomorrow between 9am and 11am. They will contact you 15 minutes before arrival.\\n\\nCustomer (Susan): Thank you for arranging that quickly, Chris.\\n\\nAgent: You're welcome, Susan. I apologize for the inconvenience caused. Is there anything else I can assist you with today?\\n\\nCustomer (Susan): No, that covers it. Thank you for your help, Chris.\\n\\nAgent: It's been a pleasure assisting you, Susan. We at Contoso Inc. value your satisfaction and apologize again for the inconvenience. Feel free to call us again if you need any more help. Have a great day!\\n\\nCustomer (Susan): Thank you, you too!\\n\\nAgent: This call is now complete. Thank you for choosing Contoso Inc. Goodbye, Susan!\"\n}\n</code></pre></p>"},{"location":"workshop/support-docs/CustomizingAzdParameters/","title":"CustomizingAzdParameters","text":""},{"location":"workshop/support-docs/CustomizingAzdParameters/#optional-customizing-resource-names","title":"[Optional]: Customizing resource names","text":"<p>By default this template will use the environment name as the prefix to prevent naming collisions within Azure. The parameters below show the default values. You only need to run the statements below if you need to change the values. </p> <p>To override any of the parameters, run <code>azd env set &lt;key&gt; &lt;value&gt;</code> before running <code>azd up</code>. On the first azd command, it will prompt you for the environment name. Be sure to choose 3-20 charaters alphanumeric unique name. </p> <p>Change the Content Understanding Location (allowed values: Sweden Central, Australia East)</p> Bash<pre><code>azd env set AZURE_ENV_CU_LOCATION 'swedencentral'\n</code></pre> <p>Change the Secondary Location (example: eastus2, westus2, etc.)</p> Bash<pre><code>azd env set AZURE_ENV_SECONDARY_LOCATION eastus2\n</code></pre> <p>Change the Model Deployment Type (allowed values: Standard, GlobalStandard)</p> Bash<pre><code>azd env set AZURE_ENV_MODEL_DEPLOYMENT_TYPE GlobalStandard\n</code></pre> <p>Set the Model Name (allowed values: gpt-4o-mini, gpt-4o, gpt-4)</p> Bash<pre><code>azd env set AZURE_ENV_MODEL_NAME gpt-4o-mini\n</code></pre> <p>Change the Model Capacity (choose a number based on available GPT model capacity in your subscription)</p> Bash<pre><code>azd env set AZURE_ENV_MODEL_CAPACITY 30\n</code></pre> <p>Change the Embedding Model </p> Bash<pre><code>azd env set AZURE_ENV_EMBEDDING_MODEL_NAME text-embedding-ada-002\n</code></pre> <p>Change the Embedding Deployment Capacity (choose a number based on available embedding model capacity in your subscription)</p> Bash<pre><code>azd env set AZURE_ENV_EMBEDDING_MODEL_CAPACITY 80\n</code></pre>"},{"location":"workshop/support-docs/DeleteResourceGroup/","title":"Deleting Resources After a Failed Deployment in Azure Portal","text":"<p>If your deployment fails and you need to clean up the resources manually, follow these steps in the Azure Portal.</p>"},{"location":"workshop/support-docs/DeleteResourceGroup/#1-navigate-to-the-azure-portal","title":"1. Navigate to the Azure Portal","text":"<ol> <li>Open Azure Portal.</li> <li>Sign in with your Azure account.</li> </ol>"},{"location":"workshop/support-docs/DeleteResourceGroup/#2-find-the-resource-group","title":"2. Find the Resource Group","text":"<ol> <li>In the search bar at the top, type \"Resource groups\" and select it.</li> <li>Locate the resource group associated with the failed deployment.</li> </ol>"},{"location":"workshop/support-docs/DeleteResourceGroup/#3-delete-the-resource-group","title":"3. Delete the Resource Group","text":"<ol> <li>Click on the resource group name to open it.</li> <li>Click the Delete resource group button at the top.</li> </ol> <ol> <li>Type the resource group name in the confirmation box and click Delete.</li> </ol> <p>\ud83d\udccc Note: Deleting a resource group will remove all resources inside it.</p>"},{"location":"workshop/support-docs/DeleteResourceGroup/#4-delete-individual-resources-if-needed","title":"4. Delete Individual Resources (If Needed)","text":"<p>If you don\u2019t want to delete the entire resource group, follow these steps:</p> <ol> <li>Open Azure Portal and go to the Resource groups section.</li> <li>Click on the specific resource group.</li> <li>Select the resource you want to delete (e.g., App Service, Storage Account).</li> <li>Click Delete at the top.</li> </ol> <p></p>"},{"location":"workshop/support-docs/DeleteResourceGroup/#5-verify-deletion","title":"5. Verify Deletion","text":"<ul> <li>After a few minutes, refresh the Resource groups page.</li> <li>Ensure the deleted resource or group no longer appears.</li> </ul> <p>\ud83d\udccc Tip: If a resource fails to delete, check if it's locked under the Locks section and remove the lock.</p>"},{"location":"workshop/support-docs/Fabric_deployment/","title":"Fabric deployment","text":""},{"location":"workshop/support-docs/Fabric_deployment/#how-to-customize","title":"How to customize","text":"<p>If you'd like to customize the solution accelerator, here are some ways you might do that: - Ingest your own audio conversation files by uploading them into the <code>cu_audio_files_all</code> lakehouse folder and run the data pipeline - Deploy with Microsoft Fabric by following the steps in Fabric_deployment.md</p> <ol> <li> <p>Create Fabric workspace</p> <ol> <li>Navigate to (Fabric Workspace)</li> <li>Click on Data Engineering experience</li> <li>Click on Workspaces from left Navigation</li> <li>Click on + New Workspace<ol> <li>Provide Name of Workspace </li> <li>Provide Description of Workspace (optional)</li> <li>Click Apply</li> </ol> </li> <li>Open Workspace</li> <li>Create Environment<ol> <li>Click <code>+ New Item</code> (in Workspace)</li> <li>Select Environment from list</li> <li>Provide name for Environment and click Create</li> <li>Select Public libraries in left panel</li> <li>Click Add from .yml</li> <li>Upload .yml from here</li> <li>Click Publish</li> </ol> </li> <li>Retrieve Workspace ID from URL, refer to documentation additional assistance (here)</li> </ol> <p>***Note: Wait until the Environment is finished publishing prior to proceeding witht the next steps.</p> </li> <li> <p>Deploy Fabric resources and artifacts</p> <ol> <li>Navigate to (Azure Portal)</li> <li>Click on Azure Cloud Shell in the top right of navigation Menu (add image)</li> <li>Run the run the following commands:  <ol> <li><code>az login</code> ***Follow instructions in Azure Cloud Shell for login instructions</li> <li><code>rm -rf ./Conversation-Knowledge-Mining-Solution-Accelerator</code></li> <li><code>git clone https://github.com/microsoft/Conversation-Knowledge-Mining-Solution-Accelerator</code></li> <li><code>cd ./Conversation-Knowledge-Mining-Solution-Accelerator/Deployment/scripts/fabric_scripts</code></li> <li><code>sh ./run_fabric_items_scripts.sh keyvault_param workspaceid_param solutionprefix_param</code><ol> <li>keyvault_param - the name of the keyvault that was created in Step 1</li> <li>workspaceid_param - the workspaceid created in Step 2</li> <li>solutionprefix_param - prefix used to append to lakehouse upon creation</li> </ol> </li> </ol> </li> </ol> </li> <li> <p>Add App Authentication</p> <p>Follow steps in App Authentication to configure authenitcation in app service.</p> </li> </ol>"},{"location":"workshop/support-docs/Fabric_deployment/#upload-additional-files","title":"Upload additional files","text":"<p>All files WAV files can be uploaded in the corresponding Lakehouse in the data/Files folder:</p> <ul> <li>Audio (WAV files):   Upload Audio files in the cu_audio_files_all folder.</li> </ul>"},{"location":"workshop/support-docs/Fabric_deployment/#post-deployment","title":"Post-deployment","text":"<ul> <li>To process additional files, manually execute the pipeline_notebook after uploading new files.</li> <li>The OpenAI prompt can be modified within the Fabric notebooks.</li> </ul>"},{"location":"workshop/support-docs/quota_check/","title":"Quota check","text":""},{"location":"workshop/support-docs/quota_check/#check-quota-availability-before-deployment","title":"Check Quota Availability Before Deployment","text":"<p>Before deploying the accelerator, ensure sufficient quota availability for the required model. Use one of the following scripts based on your needs:  </p> <ul> <li><code>quota_check_params.sh</code> \u2192 If you know the model and capacity required.  </li> <li><code>quota_check_all_regions.sh</code> \u2192 If you want to check available capacity across all regions for supported models.  </li> </ul>"},{"location":"workshop/support-docs/quota_check/#if-using-azure-portal-and-cloud-shell","title":"If using Azure Portal and Cloud Shell","text":"<ol> <li>Navigate to the Azure Portal.</li> <li>Click on Azure Cloud Shell in the top right navigation menu.</li> <li>Run the appropriate command based on your requirement:  </li> </ol> <p>To check quota for a specific model and capacity: </p> Text Only<pre><code>```sh\ncurl -L -o quota_check_params.sh \"https://raw.githubusercontent.com/microsoft/Conversation-Knowledge-Mining-Solution-Accelerator/main/infra/scripts/quota_check_params.sh\"\nchmod +x quota_check_params.sh\n./quota_check_params.sh &lt;model_name:capacity&gt; [&lt;model_region&gt;] (e.g., gpt-4o-mini:30,text-embedding-ada-002:20 eastus)\n```\n</code></pre> <p>To check available quota across all regions for supported models: </p> Text Only<pre><code>```sh\ncurl -L -o quota_check_all_regions.sh \"https://raw.githubusercontent.com/microsoft/Conversation-Knowledge-Mining-Solution-Accelerator/main/infra/scripts/quota_check_all_regions.sh\"\nchmod +x quota_check_all_regions.sh\n./quota_check_all_regions.sh\n```\n</code></pre>"},{"location":"workshop/support-docs/quota_check/#if-using-vs-code-or-codespaces","title":"If using VS Code or Codespaces","text":"<ol> <li>Run the appropriate script based on your requirement:  </li> </ol> <p>To check quota for a specific model and capacity: </p> Text Only<pre><code>```sh\n./quota_check_params.sh &lt;model_name:capacity&gt; [&lt;model_region&gt;] (e.g., gpt-4o-mini:30,text-embedding-ada-002:20 eastus)\n```\n</code></pre> <p>To check available quota across all regions for supported models: </p> Text Only<pre><code>```sh\n./quota_check_all_regions.sh\n```\n</code></pre> <ol> <li> <p>If you see the error <code>_bash: az: command not found_</code>, install Azure CLI:  </p> <p>Bash<pre><code>curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash\naz login\n</code></pre> 3. Rerun the script after installing Azure CLI.</p> <p>Parameters - <code>&lt;model_name:capacity&gt;</code>: The name and required capacity for each model, in the format model_name:capacity (e.g., gpt-4o-mini:30,text-embedding-ada-002:20). - <code>[&lt;model_region&gt;] (optional)</code>: The Azure region to check first. If not provided, all supported regions will be checked (e.g., eastus).</p> </li> </ol>"}]}