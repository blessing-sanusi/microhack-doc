{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"workshop/","title":"Knowledge Mining Microhack: Hands-on Workshop","text":""},{"location":"workshop/#overview","title":"Overview","text":"<p>This solution accelerator enables customers with large amounts of conversational data to improve decision-making by leveraging intelligence to uncover insights, relationships, and patterns from customer interactions. It empowers users to gain valuable knowledge and drive targeted business impact. </p> <p>It leverages Azure AI Foundry, Azure AI Content Understanding, Azure OpenAI Service, and Azure AI Search to transform large volumes of conversational data into actionable insights through topic modeling, key phrase extraction, speech-to-text transcription, and interactive chat experiences.</p>"},{"location":"workshop/#technical-key-features","title":"Technical key features","text":""},{"location":"workshop/#use-case-scenario","title":"Use case / scenario","text":"<p>An analyst managing large volumes of conversational data needs a solution to visualize key insights and uncover patterns using natural language. An interactive dashboard enables them to explore rich, actionable insights for faster, and more informed decision-making.</p>"},{"location":"workshop/docs/","title":"Introduction","text":"<p>The Micohack event is designed to engage technical roles through a condensed, half-day hands-on hack experience. Leveraging the latest Microsoft technologies, this event provides participants with the opportunity to work on real-world problems, collaborate with peers, and explore innovative solutions. </p> <p>The Microhack event is divided into several key challenges, each carefully crafted to test and expand the participants' proficiency with Microsoft's suite of tools. These challenges are not only technical in nature but also reflect real-world scenarios that businesses face, providing a comprehensive understanding of how to apply theoretical knowledge practically. </p>"},{"location":"workshop/docs/#hack-duration-2-hours","title":"Hack Duration: 2 hours","text":"<p>The event kicks off with an initial overview of the customer scenario for the business problem the participants will solve by leveraging cutting-edge technology and services.  </p> <p>Following this, the team will complete the setup phase, where participants ensure that their development environments are correctly configured, and all necessary tools are ready for use.  </p> <p>Finally, they will tackle the first challenge, which involves identifying key ideas that underpin the implementation of Microsoft technologies in solving predefined problems. </p>"},{"location":"workshop/docs/00-Use-Case-Scenerio/","title":"Scenerio","text":"<p>An analyst managing large volumes of conversational data needs a solution to visualize key insights and uncover patterns using natural language. An interactive dashboard enables them to explore rich, actionable insights for faster, and more informed decision-making.</p> <p>This solution empowers analysts with tools to ask questions and receive real-time, contextualized responses. It streamlines problem-solving, enhances collaboration, and fosters innovation by making data-driven insights accessible and shareable.</p> <p>The sample data used in this repository is synthetic and generated using Azure OpenAI service. The data is intended for use as sample data only.</p>"},{"location":"workshop/docs/00-Use-Case-Scenerio/#knowledge-mining","title":"Knowledge Mining","text":"<p>Knowledge Mining enables customers with large amounts of conversational data to improve decision-making by leveraging intelligence to uncover insights, relationships, and patterns from customer interactions. It empowers users to gain valuable knowledge and drive targeted business impact. </p> <p>This template leverages Azure AI Foundry, Azure AI Content Understanding, Azure OpenAI Service, and Azure AI Search to transform large volumes of conversational data into actionable insights through topic modeling, key phrase extraction, speech-to-text transcription, and interactive chat experiences.</p>"},{"location":"workshop/docs/00-Use-Case-Scenerio/#technical-key-features","title":"Technical key features","text":""},{"location":"workshop/docs/01-Environment-setup/","title":"Enviroment Setup","text":""},{"location":"workshop/docs/01-Environment-setup/#prerequisites","title":"Prerequisites","text":"<p>To deploy this solution accelerator, ensure you have access to an Azure subscription with the necessary permissions to create resource groups and resources. Follow the steps in  Azure Account Set Up </p> <p>Check the Azure Products by Region page and select a region where the following services are available:  </p> <ul> <li>Azure AI Foundry </li> <li>Azure OpenAI Service </li> <li>Azure AI Search</li> <li>Azure AI Content Understanding</li> <li>Embedding Deployment Capacity  </li> <li>GPT Model Capacity</li> <li>Azure Semantic Search </li> </ul> <p>Here are some example regions where the services are available: East US, East US2, Australia East, UK South, France Central.</p>"},{"location":"workshop/docs/01-Environment-setup/#important-check-azure-openai-quota-availability","title":"\u26a0\ufe0f Important: Check Azure OpenAI Quota Availability","text":"<p>\u27a1\ufe0f To ensure sufficient quota is available in your subscription, please follow Quota check instructions guide before you deploy the solution.</p>"},{"location":"workshop/docs/01-Environment-setup/#deploying","title":"Deploying","text":"<p>Once you've opened the project in Codespaces or in Dev Containers or locally, you can deploy it to Azure following the following steps. </p> <p>To change the azd parameters from the default values, follow the steps here. </p> <ol> <li> <p>Login to Azure:</p> Bash<pre><code>azd auth login\n</code></pre> </li> <li> <p>Provision and deploy all the resources:</p> Bash<pre><code>azd up\n</code></pre> </li> <li> <p>Provide an <code>azd</code> environment name (like \"ckmapp\")</p> </li> <li> <p>Select a subscription from your Azure account, and select a location which has quota for all the resources. </p> <ul> <li>This deployment will take 7-10 minutes to provision the resources in your account and set up the solution with sample data. </li> <li>If you get an error or timeout with deployment, changing the location can help, as there may be availability constraints for the resources.</li> </ul> </li> <li> <p>Once the deployment has completed successfully, open the Azure Portal, go to the deployed resource group, find the App Service and get the app URL from <code>Default domain</code>.</p> </li> <li> <p>You can now delete the resources by running <code>azd down</code>, if you are done trying out the application. </p> </li> </ol>  Additional Steps  <ol> <li> <p>Add App Authentication</p> <p>Follow steps in App Authentication to configure authenitcation in app service.</p> <p>Note: Authentication changes can take up to 10 minutes </p> </li> </ol>"},{"location":"workshop/docs/01-Environment-setup/#to-authenticate-with-azure-developer-cli-azd-use-the-following-command-with-your-tenant-id","title":"To authenticate with Azure Developer CLI (<code>azd</code>), use the following command with your Tenant ID:","text":"<p><code>sh azd auth login --tenant-id &lt;tenant-id&gt;</code></p>"},{"location":"workshop/docs/01-Environment-setup/AppAuthentication/","title":"AppAuthentication","text":""},{"location":"workshop/docs/01-Environment-setup/AppAuthentication/#add-authentication-in-azure-app-service-configuration","title":"Add Authentication in Azure App Service configuration","text":"<ol> <li>Click on <code>Authentication</code> from left menu.</li> </ol> <ol> <li>Click on <code>+ Add Provider</code> to see a list of identity providers.</li> </ol> <ol> <li>Click on <code>+ Add Provider</code> to see a list of identity providers.</li> </ol> <ol> <li>Select the first option <code>Microsoft Entra Id</code> from the drop-down list.</li> </ol> <ol> <li>Accept the default values and click on <code>Add</code> button to go back to the previous page with the identity provider added.</li> </ol> <p>### Creating a new App Registration 1. Click on <code>Home</code> and select <code>Microsoft Entra ID</code>.</p> <p></p> <ol> <li>Click on <code>App registrations</code>.</li> </ol> <p></p> <ol> <li>Click on <code>+ New registration</code>.</li> </ol> <p></p> <ol> <li>Provide the <code>Name</code>, select supported account types as <code>Accounts in this organizational directory only(Contoso only - Single tenant)</code>, select platform as <code>Web</code>, enter/select the <code>URL</code> and register.</li> </ol> <p></p> <ol> <li>After application is created sucessfully, then click on <code>Add a Redirect URL</code>.</li> </ol> <p></p> <ol> <li>Click on <code>+ Add a platform</code>.</li> </ol> <p></p> <ol> <li>Click on <code>Web</code>.</li> </ol> <p></p> <ol> <li>Enter the <code>web app URL</code> (Provide the app service name in place of XXXX) and Save. Then go back to [Step 4] and follow from Point 4 choose <code>Pick an existing app registration in this directory</code> from the Add an Identity Provider page and provide the newly registered App Name.</li> </ol> <p></p>"},{"location":"workshop/docs/02-%20Explore-Data/","title":"Overview","text":"<p>Before using the dashboard, explore the existing raw data manually. What challenges do you encounter in identifying trends and patterns? How does the manual exploration compare to using the Interactive Insights Dashboard? </p>"},{"location":"workshop/docs/02-%20Explore-Data/#steps","title":"Steps","text":"Text Only<pre><code>1. Check challenge sucess criteria before starting\n2. in a private web browser, open your deployment\n</code></pre>"},{"location":"workshop/docs/03-Explore-the-code/","title":"Overview of the Application","text":"<p>The Conversation Knowledge Mining Solution Accelerator is a robust application designed to extract actionable insights from conversational data. It leverages Azure AI services and provides an interactive user interface for querying and visualizing data. The solution is built with a modular architecture, combining a React-based frontend, a FastAPI backend, and Azure services for data processing and storage.</p>"},{"location":"workshop/docs/03-Explore-the-code/#key-features","title":"Key Features","text":""},{"location":"workshop/docs/03-Explore-the-code/#data-processing-and-analysis","title":"Data Processing and Analysis:","text":"<ul> <li>Processes conversational data using Azure AI Foundry, Azure AI Content Understanding, and Azure OpenAI Service.</li> <li>Extracts insights such as sentiment, key phrases, and topics from conversations.</li> <li>Supports speech-to-text transcription for audio data.</li> </ul>"},{"location":"workshop/docs/03-Explore-the-code/#dynamic-dashboard","title":"Dynamic Dashboard:","text":"<ul> <li>Visualizes insights through various chart types (e.g., Donut Chart, Bar Chart, Word Cloud).</li> <li>Enables filtering and customization of data views.</li> <li>Provides a responsive layout for seamless user experience.</li> </ul>"},{"location":"workshop/docs/03-Explore-the-code/#interactive-chat-interface","title":"Interactive Chat Interface:","text":"<ul> <li>Allows users to query data in natural language and receive real-time responses.</li> <li>Supports both text-based and chart-based responses.</li> <li>Integrates with Azure OpenAI and Azure Cognitive Search for generating responses and retrieving relevant data.</li> </ul>"},{"location":"workshop/docs/03-Explore-the-code/#backend-api","title":"Backend API:","text":"<ul> <li>Built with FastAPI for handling requests and integrating with Azure services.</li> <li>Includes modular routes for backend operations and conversation history management.</li> <li>Provides a health check endpoint for monitoring service status.</li> </ul>"},{"location":"workshop/docs/03-Explore-the-code/#scalable-deployment","title":"Scalable Deployment:","text":"<ul> <li>Supports deployment via GitHub Codespaces, VS Code Dev Containers, or local environments.</li> <li>Includes configurable deployment settings for regions, models, and resource capacities.</li> </ul>"},{"location":"workshop/docs/03-Explore-the-code/00-Chart-Component/","title":"00 Chart Component","text":""},{"location":"workshop/docs/03-Explore-the-code/00-Chart-Component/#chart-component","title":"Chart Component","text":"<p>The Chart component is a dynamic and responsive charting module that:</p> Text Only<pre><code>\u2022   Fetches chart data and filter metadata from APIs.\n\u2022   Dynamically renders various chart types based on configuration.\n\u2022   Supports filtering to update chart data in real-time.\n\u2022   Manages layout and responsiveness for a seamless user experience.\n</code></pre>"},{"location":"workshop/docs/03-Explore-the-code/00-Chart-Component/#key-functionalities","title":"Key Functionalities","text":"<p>Data Fetching     Chart Data:         The getChartData function fetches chart data from the backend.         o   If filters are applied, it uses fetchChartDataWithFilters to fetch filtered data.         o   If no filters are applied, it uses fetchChartData to fetch default data. Chart Rendering     The renderChart function dynamically renders charts based on their type:         o   Card: Displays a single value with a description.         o   Donut Chart: Displays data as a donut chart.         o   Bar Chart: Displays data as a horizontal bar chart.         o   Table: Displays data in a tabular format.         o   Word Cloud: Displays data as a word cloud.</p> <ol> <li>Filtering \u2022   Applying Filters: o   The applyFilters function is triggered when the user applies new filters via the ChartFilter component. o   It calls getChartData with the updated filters to fetch filtered chart data.</li> </ol>"},{"location":"workshop/docs/03-Explore-the-code/01-Chat-Component/","title":"Chat Component","text":"<p>The Chat component integrates seamlessly with the backend to provide a robust and dynamic chat experience. It supports:</p> Text Only<pre><code>1. General Chat:\n   \u2022    The greeting function handles general queries and greetings.\n   \u2022    It uses Azure OpenAI to generate responses, either via the AIProjectClient or directly using the openai library.\n\n\n2. SQL Query Generation:\n    \u2022   The get_SQL_Response function dynamically generates SQL queries based on user input.\n    \u2022   It uses Azure OpenAI to create the SQL query and executes it against the database using the execute_sql_query function.\n\n\n3. Azure Cognitive Search:\n    \u2022   The get_answers_from_calltranscripts function retrieves answers from indexed call transcripts using Azure Cognitive Search.\n    \u2022   It uses a hybrid query approach (vector_simple_hybrid) to combine semantic and vector-based search.\n\n\n4. Chart Data Generation:\n    \u2022   The process_rag_response function in chat_helper.py dynamically generates chart data from RAG responses.\n    \u2022   It uses Azure OpenAI to process the RAG response and generate valid JSON data compatible with Chart.js.\n</code></pre>"},{"location":"workshop/docs/04-Explore-Application/","title":"Index","text":""},{"location":"workshop/docs/04-Explore-Application/#sample-questions","title":"Sample Questions","text":"<p>To help you get started, here are some Sample Questions you can ask in the app:</p> <ul> <li>Total number of calls by date for the last 7 days</li> <li>Show average handling time by topics in minutes</li> <li>What are the top 7 challenges users reported?</li> <li>Give a summary of billing issues</li> <li>When customers call in about unexpected charges, what types of charges are they seeing?</li> </ul> <p>These questions serve as a great starting point to explore insights from the data.</p>"},{"location":"workshop/docs/05-Customize-Solution/","title":"Implement a Copilot","text":"Using your own data? <p>Incorporating your own data into the solution accelerator requires adapting the existing architecture to align with your specific data structures. Here are some recommendations:</p> <p>1. Implement Design Patterns and LangChain in Your Solution To effectively integrate AI capabilities, you need to incorporate design patterns that facilitate seamless interaction between your data and AI models. Utilizing LangChain can help in constructing these patterns, enabling efficient data processing and AI orchestration.</p> <p>2. Customize the <code>chat_functions.py</code> file The <code>chat_functions.py</code> file serves as a bridge between the user inputs and AI responses. To tailor this to your data:</p> <ul> <li>Understand the Existing Structure: Review the current implementation to comprehend how data flows and functions are structured.</li> <li>Map Your Data: Identify how your data schema aligns with the existing functions.</li> <li>Modify Functions: Adjust or rewrite functions to query and process your data appropriately, ensuring that the AI services can accurately interpret and respond based on your dataset.</li> </ul> <p>In this section, you will add an AI copilot to the Woodgrove Bank Contract Management application using Python, the GenAI capabilities of Azure Database for PostgreSQL - Flexible Server, and the Azure AI extension. Using the AI-validated data, the copilot will use RAG to provide insights and answer questions about vendor contract performance and invoicing accuracy, serving as an intelligent assistant for Woodgrove Banks users. Here's what you will accomplish:</p> <ul> <li> Explore the API codebase</li> <li> Review the RAG design</li> <li> Leverage LangChain Orchestration</li> <li> Implement and test the Chat endpoint</li> <li> Refine the copilot prompt using standard prompt engineering techniques</li> <li> Add and test the Copilot Chat UI component</li> </ul> <p>Following these steps will transform your application into a powerful AI-enhanced platform capable of executing advanced generative AI tasks and providing deeper insights from your data.</p>"},{"location":"workshop/docs/05-Customize-Solution/#what-are-copilots","title":"What are copilots?","text":"<p>Copilots are advanced AI assistants designed to augment human capabilities and improve productivity by providing intelligent, context-aware support, automating repetitive tasks, and enhancing decision-making processes. For instance, the Woodgrove Bank copilot will assist in data analysis, helping users identify patterns and trends in financial datasets.</p>"},{"location":"workshop/docs/05-Customize-Solution/#why-use-python","title":"Why use Python?","text":"<p>Python's simplicity and readability make it a popular programming language for AI and machine learning projects. Its extensive libraries and frameworks, such as LangChain, FastAPI, and many others, provide robust tools for developing sophisticated copilots. Python's versatility allows developers to iterate and experiment quickly, making it a top choice for building AI applications.</p>"},{"location":"workshop/docs/05-Customize-Solution/01/","title":"01","text":""},{"location":"workshop/docs/05-Customize-Solution/01/#how-to-customize","title":"How to customize","text":"<p>If you'd like to customize the solution accelerator, here are some ways you might do that: - Ingest your own audio conversation files by uploading them into the <code>cu_audio_files_all</code> lakehouse folder and run the data pipeline - Deploy with Microsoft Fabric by following the steps in Fabric_deployment.md</p> <ol> <li> <p>Create Fabric workspace</p> <ol> <li>Navigate to (Fabric Workspace)</li> <li>Click on Data Engineering experience</li> <li>Click on Workspaces from left Navigation</li> <li>Click on + New Workspace<ol> <li>Provide Name of Workspace </li> <li>Provide Description of Workspace (optional)</li> <li>Click Apply</li> </ol> </li> <li>Open Workspace</li> <li>Create Environment<ol> <li>Click <code>+ New Item</code> (in Workspace)</li> <li>Select Environment from list</li> <li>Provide name for Environment and click Create</li> <li>Select Public libraries in left panel</li> <li>Click Add from .yml</li> <li>Upload .yml from here</li> <li>Click Publish</li> </ol> </li> <li>Retrieve Workspace ID from URL, refer to documentation additional assistance (here)</li> </ol> <p>***Note: Wait until the Environment is finished publishing prior to proceeding witht the next steps.</p> </li> <li> <p>Deploy Fabric resources and artifacts</p> <ol> <li>Navigate to (Azure Portal)</li> <li>Click on Azure Cloud Shell in the top right of navigation Menu (add image)</li> <li>Run the run the following commands:  <ol> <li><code>az login</code> ***Follow instructions in Azure Cloud Shell for login instructions</li> <li><code>rm -rf ./Conversation-Knowledge-Mining-Solution-Accelerator</code></li> <li><code>git clone https://github.com/microsoft/Conversation-Knowledge-Mining-Solution-Accelerator</code></li> <li><code>cd ./Conversation-Knowledge-Mining-Solution-Accelerator/Deployment/scripts/fabric_scripts</code></li> <li><code>sh ./run_fabric_items_scripts.sh keyvault_param workspaceid_param solutionprefix_param</code><ol> <li>keyvault_param - the name of the keyvault that was created in Step 1</li> <li>workspaceid_param - the workspaceid created in Step 2</li> <li>solutionprefix_param - prefix used to append to lakehouse upon creation</li> </ol> </li> </ol> </li> </ol> </li> <li> <p>Add App Authentication</p> <p>Follow steps in App Authentication to configure authenitcation in app service.</p> </li> </ol>"},{"location":"workshop/docs/05-Customize-Solution/01/#upload-additional-files","title":"Upload additional files","text":"<p>All files WAV files can be uploaded in the corresponding Lakehouse in the data/Files folder:</p> <ul> <li>Audio (WAV files):   Upload Audio files in the cu_audio_files_all folder.</li> </ul>"},{"location":"workshop/docs/05-Customize-Solution/01/#post-deployment","title":"Post-deployment","text":"<ul> <li>To process additional files, manually execute the pipeline_notebook after uploading new files.</li> <li>The OpenAI prompt can be modified within the Fabric notebooks.</li> </ul>"},{"location":"workshop/docs/05-Customize-Solution/02/","title":"5.2 Review RAG Design","text":"<p>The solution leverages the Retrieval Augmented Generation (RAG) design pattern to ensure the copilot's responses are grounded in the (private) data maintained by Woodgrove Bank. RAG works by retrieving relevant information from your data stores and augmenting the model prompt to create a composite prompt. This enhanced prompt is used by the LLM to generate the final response or completion.</p> <p></p> <p>To understand how the RAG design pattern works in the context of the Woodgrove Bank Contract Management application, select each tab in order and review the sequence of events shown in the figure above.</p> 1. User Query2. Vectorize Query3. Retrieve Data4. Augment Query5. Generate Response <p>The user query arrives at our copilot implementation via the chat API endpoint.</p> <p>User queries are submitted via the copilot interface of the Woodgrove Bank Contract Management portal. These queries are sent to the backend API's <code>/chat</code> endpoint. The incoming user query has three components:</p> <ol> <li>The user's question (text input)</li> <li>An optional chat history (object array)</li> <li>And, a max history setting, which indicates the number of chat history messages to use in the prompt</li> </ol> <p>The API extracts these parameters from the incoming request and invokes the <code>/chat</code> endpoint, starting the workflow that reflects this RAG design pattern.</p> <p>Embeddings representing the user query are generated.</p> <p>When performing similarity searches to find data, the <code>/chat</code> endpoint sends the text of the user's request to Azure OpenAI, where it is vectorized using a large language \"Embedding\" model (e.g., Azure Open AI <code>text-embedding-ada-002</code>). This vector is then used in the query to retrieve similar records in the next step.</p> Generate Embeddings <p>The below <code>__create_query_embeddings</code> function in the Woodgrove Bank API uses the LangChain AzureOpenAIEmbeddings class to generate embeddings for the provided user query.</p> src/api/app/functions/chat_functions.py<pre><code>async def __create_query_embeddings(self, user_query: str):\n    \"\"\"\n    Generates vector embeddings for the user query.\n    \"\"\"\n    # Create embeddings using the LangChain Azure OpenAI Embeddings client\n    # This makes an API call to the Azure OpenAI service to generate embeddings,\n    # which can be used to compare the user query with vectorized data in the database.\n    query_embeddings = await self.embedding_client.aembed_query(user_query)\n    return query_embeddings\n</code></pre> <p>This step may be skipped in cases where raw data is requested, such as a list of vendors.</p> <p>Queries are executed against the database to retrieve (private) data.</p> <p>This step retrieves data from the PostgreSQL database to \"augment\" the prompt. Depending on the user query, hybrid search and direct data retrieval techniques may be used to obtain relevant records.</p> Improve RAG accuracy <p>The accuracy of the RAG pattern can be improved by using techniques like semantic ranking to order the returned results and GraphRAG to identify relationships between data. You will learn about these techniques in the next task.</p> <p>Select each tab below to learn more about the implementation of Hybrid Search and Direct Data Retrieval in the context of the Woodgrove Bank Contract Management application!</p> Hybrid SearchDirect Data Retrieval <p>Hybrid search in Azure Database for PostgreSQL combines traditional full-text search functionality with the vector similarity search capabilities enabled by the <code>azure_ai</code> and <code>vector</code> extensions to deliver highly relevant results. This dual approach leverages the precision of keyword matching with full-text search and the contextual understanding of vector search, ensuring that users obtain exact matches and semantically related content. This synergy enhances search efficiency, provides a richer user experience, and supports diverse use cases\u2014from technical document retrieval to broad content discovery\u2014making it an invaluable tool for modern copilots</p> Hybrid Search Example Code <p>The <code>find_invoice_validation_results</code> function below provides an example of the hybrid search technique used in the Woodgrove Bank API.</p> src/api/app/functions/chat_functions.py<pre><code>async def find_invoice_validation_results(self, user_query: str, invoice_id: int = None, vendor_id: int = None, sow_id: int = None):\n    \"\"\"\n    Retrieves invoice accuracy and performance validation results similar to the user query for specified invoice, vendor, or SOW.\n    If no invoice_id, vendor_id, or sow_id is provided, return all similar validation results.\n    \"\"\"\n    # Define the columns to retrieve from the table\n    # Exclude the embedding column in results\n    columns = [\"invoice_id\", \"datestamp\", \"result\", \"validation_passed\"]\n\n    # Get the embeddings for the user query\n    query_embeddings = await self.__create_query_embeddings(user_query)\n\n    # Use hybrid search to rank records, with exact matches ranked highest\n    columns.append(f\"\"\"CASE\n                        WHEN result ILIKE '%{user_query}%' THEN 0\n                        ELSE (embedding &lt;=&gt; '{query_embeddings}')::real\n                    END as rank\"\"\")\n\n    query = f'SELECT {\", \".join(columns)} FROM invoice_validation_results'\n\n    # Filter the validation results by invoice_id, vendor_id, or sow_id, if provided\n    if invoice_id is not None:\n        query += f' WHERE invoice_id = {invoice_id}'\n    else:\n        if vendor_id is not None:\n            query += f' WHERE vendor_id = {vendor_id}'\n            if sow_id is not None:\n                query += f' AND sow_id = {sow_id}'\n        elif sow_id is not None:\n            query += f' WHERE sow_id = {sow_id}'\n\n    query += f' ORDER BY rank ASC'\n\n    rows = await self.__execute_query(f'{query};')\n    return [dict(row) for row in rows]\n</code></pre> <p>In the code above: </p> <ol> <li> <p>The <code>CASE</code> statement on lines 14-17 handles the Hybrid Search.</p> <ol> <li> <p><code>WHEN result ILIKE '%{user_query}%'</code> performs a case-insensitive search for the exact text of the user query. If found, a <code>rank</code> of <code>0</code> is assigned to the record, which ranks these records as the highest or most similar matches.</p> </li> <li> <p>When an exact match is not found, the <code>ELSE</code> statement executes a vector similarity search using the cosine distance function, as indicated by the <code>&lt;=&gt;</code> vector operator, to compare the embedding representation of the user query (<code>query_embeddings</code>) from the previous step to values in the <code>embedding</code> column of the <code>invoice_validation_results</code> table. The similarity score of these matches is assigned as the <code>rank</code>. Scores closer to zero indicate a more semantically similar result.</p> </li> </ol> </li> <li> <p>If values are provided, the query is further refined to filter on a specific <code>invoice_id</code> or <code>vendor_id</code> and <code>sow_id</code>, as shown in lines 73-81.</p> </li> <li> <p>Finally, the results are ordered by <code>rank</code> to ensure the most relevant search results appear first in the rows returned.</p> </li> </ol> <p>In other cases, such as getting a list of all vendors, query vectorization is unnecessary. Direct data retrieval is handled via simple <code>SELECT</code> queries against the database to avoid the overhead of generating embeddings and querying vector fields.</p> Direct Data Retrieval Example Code <p>The <code>get_vendors</code> function below provides an example of a direct data retrieval technique used in the Woodgrove Bank API.</p> src/api/app/functions/chat_functions.py<pre><code>async def get_vendors(self):\n    \"\"\"Retrieves a list of vendors from the database.\"\"\"\n    rows = await self.__execute_query('SELECT * FROM vendors;')\n    return [dict(row) for row in rows]\n</code></pre> <p>The copilot creates a composite prompt with the retrieved data.</p> <p>The <code>/chat</code> API combines the user's original question with results returned from the database to create an enhanced or composite model prompt, augmenting the model with additional data to use when generating a response.</p> <p>The chat model uses the prompt to generate a grounded response.</p> <p>The composite prompt, grounded with (\"private\") data, is sent to a Large Language \"chat\" completion model, such as Azure OpenAI's <code>gpt-4o</code>. The completion model sees the enhanced prompt (hybrid search results and chat history) as the grounding context for generating the completion, improving the quality (e.g., relevance, groundedness) of the response returned from the Woodgrove Bank copilot.</p>"},{"location":"workshop/docs/05-Customize-Solution/03/","title":"5.3 Leverage LangChain Orchestration","text":"<p>LangChain is a powerful tool that enhances the integration and coordination of multiple AI models and tools to create complex and dynamic AI applications. By leveraging orchestration capabilities, LangChain allows you to seamlessly combine various language models, APIs, and custom components into a unified workflow. This orchestration ensures that each element works together efficiently, enabling the creation of sophisticated applications capable of performing various tasks, from natural language understanding and generation to information retrieval and data analysis.</p> <p>LangChain's orchestration capabilities are particularly beneficial when building a copilot application using Python and Azure Database for PostgreSQL. Copilots must often combine natural language processing (NLP) models, knowledge retrieval systems, and custom logic to provide accurate and contextually relevant responses. LangChain facilitates this by orchestrating various NLP models and APIs, ensuring the copilot can effectively understand and generate responses to user queries.</p> <p>Moreover, integrating Azure Database for PostgreSQL with LangChain provides a scalable and flexible database solution that can handle large volumes of data with low latency. The PostgreSQL vector search features enabled by the <code>vector</code> extension allow for high-performance retrieval of relevant information based on the semantic similarity of data, which is especially useful for NLP applications. This means the copilot can perform sophisticated searches over large datasets, retrieving contextually relevant information for user queries.</p>"},{"location":"workshop/docs/05-Customize-Solution/03/#rag-with-langchain","title":"RAG with LangChain","text":"<p>By leveraging LangChain's orchestration capabilities, RAG can seamlessly combine the retrieval of relevant information with the generative power of AI models. When a user poses a query, the RAG model can retrieve contextually appropriate data from PostgreSQL using hybrid search and then generate a comprehensive, coherent response based on the grounding data. This combination of retrieval and generation significantly enhances the copilot's ability to provide accurate, context-aware answers, leading to a more robust and user-friendly experience.</p>"},{"location":"workshop/docs/05-Customize-Solution/03/#understand-function-calling-and-tools-in-langchain","title":"Understand function calling and tools in LangChain","text":"<p>Function calling in LangChain offers a more structured and flexible approach than using the Azure OpenAI client directly in Python. In LangChain, you can define and manage functions as modular components that are easily reusable and maintainable. This approach allows for more organized code, where each function encapsulates a specific task, reducing complexity and making the development process more efficient.</p> <p>When using the Azure OpenAI client in Python, function calls are typically limited to direct API interactions. While you can still build complex workflows, it often requires more manual orchestration and handling of asynchronous operations, which can become cumbersome and more challenging to maintain as the application grows.</p> <p>LangChain's tools play a crucial role in enhancing function calling. With a vast array of built-in tools and the ability to integrate external ones, LangChain allows you to create sophisticated pipelines where functions can call tools to perform specific operations, such as data retrieval, processing, or transformation. These tools can be configured to operate conditionally or in parallel, further optimizing the application's performance. Additionally, LangChain's tools simplify error handling and debugging by isolating functions and tools, making identifying and resolving issues easier.</p>"},{"location":"workshop/docs/05-Customize-Solution/04/","title":"5.4 Enable the Chat Endpoint","text":"<p>In this step, you will review the backend API code for the <code>/chat</code> endpoint in the <code>completions</code> router. You will then add the <code>completions</code> router to the FastAPI application to make the <code>/chat</code> endpoint available.</p>"},{"location":"workshop/docs/05-Customize-Solution/04/#review-chat-endpoint-implementation","title":"Review Chat Endpoint Implementation","text":"<p>The Woodgove Bank API exposes endpoints in various routers... The <code>chat</code> endpoint resides in the <code>completions</code> router, defined in the <code>src/api/app/routers/completions.py</code> file. Open it now in VS Code and explore the code in sections. You can also expand the section below to see the code inline and review explanations for each line of code.</p> Chat endpoint code src/api/app/routers/completions.py<pre><code>from app.functions.chat_functions import ChatFunctions\nfrom app.lifespan_manager import get_chat_client, get_db_connection_pool, get_embedding_client, get_prompt_service\nfrom app.models import CompletionRequest\nfrom fastapi import APIRouter, Depends\nfrom langchain.agents import AgentExecutor, create_openai_functions_agent\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.tools import StructuredTool\n\n# Initialize the router\nrouter = APIRouter(\n    prefix = \"/completions\",\n    tags = [\"Completions\"],\n    dependencies = [Depends(get_chat_client)],\n    responses = {404: {\"description\": \"Not found\"}}\n)\n\n@router.post('/chat', response_model = str)\nasync def generate_chat_completion(\n    request: CompletionRequest,\n    llm = Depends(get_chat_client),\n    db_pool = Depends(get_db_connection_pool),\n    embedding_client = Depends(get_embedding_client),\n    prompt_service = Depends(get_prompt_service)):\n    \"\"\"Generate a chat completion using the Azure OpenAI API.\"\"\"\n\n    # Retrieve the copilot prompt\n    system_prompt = prompt_service.get_prompt(\"copilot\")\n\n    # Provide the copilot with a persona using the system prompt.\n    messages = [{ \"role\": \"system\", \"content\": system_prompt }]\n\n    # Add the chat history to the messages list\n    # Chat history provides context of previous questions and responses for the copilot.\n    for message in request.chat_history[-request.max_history:]:\n        messages.append({\"role\": message.role, \"content\": message.content})\n\n    # Create a chat prompt template\n    prompt = ChatPromptTemplate.from_messages(\n        [\n            (\"system\", system_prompt),\n            MessagesPlaceholder(\"chat_history\", optional=True),\n            (\"user\", \"{input}\"),\n            MessagesPlaceholder(\"agent_scratchpad\")\n        ]\n    )\n\n    # Get the chat functions\n    cf = ChatFunctions(db_pool, embedding_client)\n\n    # Define tools for the agent to retrieve data from the database\n    tools = [\n        # Hybrid search functions\n        StructuredTool.from_function(coroutine=cf.find_invoice_line_items),\n        StructuredTool.from_function(coroutine=cf.find_invoice_validation_results),\n        StructuredTool.from_function(coroutine=cf.find_milestone_deliverables),\n        StructuredTool.from_function(coroutine=cf.find_sow_chunks),\n        StructuredTool.from_function(coroutine=cf.find_sow_validation_results),\n        # Get invoice data functions\n        StructuredTool.from_function(coroutine=cf.get_invoice_id),\n        StructuredTool.from_function(coroutine=cf.get_invoice_line_items),\n        StructuredTool.from_function(coroutine=cf.get_invoice_validation_results),\n        StructuredTool.from_function(coroutine=cf.get_invoices),\n        # Get SOW data functions\n        StructuredTool.from_function(coroutine=cf.get_sow_chunks),\n        StructuredTool.from_function(coroutine=cf.get_sow_id),\n        StructuredTool.from_function(coroutine=cf.get_sow_milestones),\n        StructuredTool.from_function(coroutine=cf.get_milestone_deliverables),\n        StructuredTool.from_function(coroutine=cf.get_sow_validation_results),\n        StructuredTool.from_function(coroutine=cf.get_sows),\n        # Get vendor data functions\n        StructuredTool.from_function(coroutine=cf.get_vendors)\n    ]\n\n    # Create an agent\n    agent = create_openai_functions_agent(llm=llm, tools=tools, prompt=prompt)\n    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, return_intermediate_steps=True)\n    completion = await agent_executor.ainvoke({\"input\": request.message, \"chat_history\": messages})\n    return completion['output']\n</code></pre> <ol> <li> <p>Import libraries (lines 1-7): Required classes and functions are imported from various libraries.</p> </li> <li> <p>Intialize the router (lines 10-15): This is the <code>completions</code> router, assigning the route prefix, dependencies, and other metadata.</p> </li> <li> <p>Define the chat endpoint (lines 17-23): The <code>/chat</code> endpoint is the entry point into the Woodgove Bank copilot implementation. It expects a <code>CompletionRequest</code>, which contains the user query, the chat history, and the maximum number of history messages to include in the prompt, and returns a text response.</p> <ul> <li>It accepts POST requests from clients and extracts required parameters.</li> <li>It invokes the get_chat_completion function with those parameters.</li> <li>It returns the LLM's response to the client.</li> </ul> </li> <li> <p>Chat endpoint implementation (lines 24-78). The \"/completions/chat\" route maps to the endpoint where we can invoke the Contoso Chat implementation.</p> <ul> <li> <p>Get the system prompt (line 27): The system prompt defines the copilot's persona, providing instructions about how the copilot should behave, respond to questions, and interact with customers. It also provides guidance about the RAG design pattern and how function calls (tools) should be used when answering questions. You will look at this in detail in the Prompt Engineering step of this section.</p> </li> <li> <p>Build messages collection (lines 30-35): The messages collection provides the LLM with system and user prompts and chat history messages. Each message consists of a <code>role</code> and message <code>content</code>. The role will be <code>system</code>, <code>assistant</code>, or <code>user</code>. After the <code>system</code> message, all subsequent messages must be <code>user</code> / <code>assistant</code> pairs, with a user query followed by an assistant response.</p> </li> <li> <p>Build the model prompt (lines 38-45): The LangChain <code>ChatPromptTemplate</code> class allows you to build a model prompt from a collection of messages.</p> <ul> <li>The system prompt is added to provide instructions to the model.</li> <li>The chat history is inserted as context about previous questions and responses.</li> <li>The user input provides the model with the current question it is attempting to answer.</li> <li>An agent scratchpad placeholder is included to allow responses from tools assigned to the agent to augment the model with grounding data.</li> <li>The resulting prompt provides a structured input for the conversational AI agent, helping it to generate a response based on the given context.</li> </ul> </li> <li> <p>Implement function calling (lines 48-72):</p> <ul> <li>Line 48 instantiates the <code>ChatFunctions</code> class, which contains the methods for interacting with the PostgreSQL database. You can review the functions in the <code>src/api/app/functions/chat_functions.py</code> file.</li> <li>The <code>tools</code> array created in lines 51-72 is the collection of functions available to the LangChain agent for performing retrieval operations to augment the model prompt during response generation.</li> <li> <p>Tools are created using the <code>StructuredTool.from_function</code> method provided by LangChain.</p> About the LangChain <code>StructuredTool</code> class <p>The <code>StructuredTool</code> class is a wrapper that allows LangChain agents to interact with functions. The <code>from_function</code> method creates a tool from the given function, describing the function using its input parameters and docstring description. To use it with async methods, you pass the function's name to the <code>coroutine</code> input parameter.</p> <p>In Python, a docstring (short for documentation string) is a special type of string used to document a function, method, class, or module. It provides a convenient way of associating documentation with Python code and is typically enclosed within triple quotes (\"\"\" or '''). Docstrings are placed immediately after the definition of the function (or method, class, or module) they document.</p> <p>Using the <code>StructuredTool.from_function</code> method automates the creation of the JSON function definitions required by Azure OpenAI function calling methods, simplifying function calling when using LangChain.</p> </li> </ul> </li> <li> <p>Create a LangChain agent (lines 75-76): The LangChain agent is responsible for interacting with the LLM to generate a response.</p> <ul> <li> <p>Using the <code>create_openai_functions_agent</code> method, a LangChain agent is instantiated. This agent handles function calling via the <code>tools</code> provided to the agent.</p> About the <code>create_openai_functions_agent</code> function <p>The <code>create_openai_functions_agent</code> function in LangChain creates an agent that can call external functions to perform tasks using a specified language model and tools. This enables the integration of various services and functionalities into the agent's workflow, providing flexibility and enhanced capabilities.</p> </li> <li> <p>LangChain's <code>AgentExecutor</code> class manages the agent's execution flow. It handles the processing of inputs, the invocation of tools or models, and the handling of outputs.</p> About LangChain's <code>AgentExecutor</code> <p>The <code>AgentExecutor</code> ensures that all the steps required to generate a response are executed in the correct order. It abstracts the complexities of execution for agents, providing an additional layer of functionality and structure, and making it easier to build, manage, and scale sophisticated agents.</p> </li> </ul> </li> <li> <p>Invoke the agent (line 77): The agent executor's async <code>invoke</code> method sends the incoming user message and chat history to the LLM.</p> <ul> <li>The <code>input</code> and <code>chat_history</code> tokens were defined in the prompt object created using the <code>ChatPromptTemplate</code>. The <code>invoke</code> method injects these into the model prompt, allowing the LLM to use that information when generating a response.</li> <li>The LangChain agent uses the LLM to determine if tool calls are necessary by evaluating the user query.</li> <li>Any tools required to answer the question are called, and the model prompt is augmented with grounding data from their results to formulate the final response.</li> </ul> </li> <li> <p>Return the response (line 78): The agent's completion response is returned to the user.</p> </li> </ul> </li> </ol>"},{"location":"workshop/docs/05-Customize-Solution/04/#enable-chat-endpoint-calls","title":"Enable Chat Endpoint Calls","text":"<p>To enable the <code>/chat</code> endpoint to be called from the Woodgrove Bank Contract Management Portal, you will add the completions router to the FastAPI app.</p> <ol> <li> <p>In the VS Code Explorer, navigate to the <code>src/api/app</code> folder and open the <code>main.py</code> file.</p> </li> <li> <p>Locate the block of code where the API endpoint routers are added (lines 44-56).</p> </li> <li> <p>Insert the following code at the start of that block (just below the <code># Add routers to API endpoints</code> comment on line 43) to add the <code>completions/chat</code> endpoint to the exposed API.</p> <p>Insert the code below onto line 43 of <code>main.py</code>!</p> Python<pre><code>app.include_router(completions.router)\n</code></pre> </li> <li> <p>The updated list of routers should look like this:</p> Python<pre><code># Add routers to API endpoints\napp.include_router(completions.router)\napp.include_router(deliverables.router)\napp.include_router(documents.router)\napp.include_router(embeddings.router)\napp.include_router(invoices.router)\napp.include_router(invoice_line_items.router)\napp.include_router(milestones.router)\napp.include_router(sows.router)\napp.include_router(status.router)\napp.include_router(statuses.router)\napp.include_router(validation.router)\napp.include_router(validation_results.router)\napp.include_router(vendors.router)\napp.include_router(webhooks.router)\n</code></pre> </li> <li> <p>Save the <code>main.py</code> file.</p> </li> </ol> <p>Congratulations! Your API is now enabled for copilot interactions!</p>"},{"location":"workshop/docs/05-Customize-Solution/05/","title":"5.5 Test the Chat API","text":"<p>Now, all you need to do is run the FastAPI server and have it listen for incoming client requests on the API's <code>completions/chat</code> route. In this next section, you will see how to do this locally for rapid prototyping and testing.</p>"},{"location":"workshop/docs/05-Customize-Solution/05/#testing-options","title":"Testing Options","text":"<p>The Chat API is deployed against the <code>/completions/chat</code> endpoint. So, how can you test this?</p> <ul> <li>You can use a third-party client to <code>POST</code> a request to the endpoint</li> <li>You can use a <code>CURL</code> command to make the request from the command line</li> <li> <p>You can use the built-in <code>/swagger</code> Swagger UI to try it out interactively</p> <p>Swagger UI</p> <p>Recall that the <code>src/api/app/main.py</code> file contained the definition for the FastAPI application. In that definition, the default documentation endpoint for the API was modified to use the more common <code>/swagger</code> path.</p> </li> </ul>"},{"location":"workshop/docs/05-Customize-Solution/05/#test-with-swagger","title":"Test with Swagger","text":"<p>The Swagger UI provides an easy and intuitive way to test your endpoints rapidly. A side benefit of this approach is it shows you the <code>curl</code> command you can use to make the same request from the terminal if you want to try that out later.</p> <ol> <li> <p>In Visual Studio Code, select the Run and Debug icon from the Activity Bar on the left-hand side.</p> <p></p> </li> <li> <p>At the top of the Run and Debug menu, select the API Debugger option for your OS from the debug configurations dropdown list.</p> <p></p> </li> <li> <p>Select the Start Debugging button (or press F5 on your keyboard).</p> <p></p> </li> <li> <p>Once the API debugging session has started, indicated by an <code>Application startup complete.</code> message in the terminal output, followed by the URL on which Uvicorn is running, open the local API URL in a web browser (http://127.0.0.1:8000/).</p> <p></p> </li> <li> <p>Append <code>/swagger</code> to the URL to get the Swagger UI interactive testing page.</p> <p></p> </li> <li> <p>Expand the <code>POST</code> section for the <code>completions/chat</code> endpoint under the Completions tag and select <code>Try it out</code>:</p> <p></p> </li> <li> <p>In the Request body section, paste the following JSON snippet to request a list of vendors, then select Execute.</p> <p>Paste the following JSON in the request body box!</p> <pre><code>{\n    \"session_id\": 0,\n    \"message\": \"What vendors are we working with?\",\n    \"chat_history\": [],\n    \"max_history\": 6\n}\n</code></pre> <p></p> </li> <li> <p>Scroll down to the Responses section for the <code>/completions/chat</code> endpoint in the Swagger UI and verify you received a valid response.</p> <p></p> View server execution traces in the VS Code console. <p>By running a debug session in VS Code, you can see the server execution traces of a request to the Chat endpoint in the Visual Studio Code console. These traces allow you to observe the agent scratchpad output from your LangChain agent, which can be useful in troubleshooting and understanding how the agent works.</p> VS Code terminal with LangChain output<pre><code>&gt; Entering new AgentExecutor chain...\n\nInvoking: `get_vendors` with `{}`\n\n[{'id': 1, 'name': 'Adatum Corporation', 'address': '789 Goldsmith Road, MainTown City', 'contact_name': 'Elizabeth Moore', 'contact_email': 'elizabeth.moore@adatum.com', 'contact_phone': '555-789-7890', 'website': 'http://www.adatum.com', 'type': 'Data Engineering'}, {'id': 2, 'name': 'Contoso Ltd.', 'address': '456 Industrial Road, Scooton City', 'contact_name': 'Nicole Wagner', 'contact_email': 'nicole@contoso.com', 'contact_phone': '555-654-3210', 'website': 'http://www.contoso.com', 'type': 'Software Engineering'}, {'id': 3, 'name': 'Lucerne Publishing', 'address': '789 Live Street, Woodgrove', 'contact_name': 'Ana Bowman', 'contact_email': 'abowman@lucernepublishing.com', 'contact_phone': '555-654-9870', 'website': 'http://www.lucernepublishing.com', 'type': 'Graphic Design'}, {'id': 4, 'name': 'VanArsdel Ltd.', 'address': '123 Innovation Drive, TechVille', 'contact_name': 'Gabriel Diaz', 'contact_email': 'gdiaz@vanarsdelltd.com', 'contact_phone': '555-321-0987', 'website': 'http://www.vanarsdelltd.com', 'type': 'Software Engineering'}, {'id': 5, 'name': 'Trey Research', 'address': '456 Research Avenue, Redmond', 'contact_name': 'Serena Davis', 'contact_email': 'serena.davis@treyresearch.net', 'contact_phone': '555-867-5309', 'website': 'http://www.treyresearch.net', 'type': 'DevOps'}, {'id': 6, 'name': 'Fabrikam Inc.', 'address': '24601 South St., Philadelphia', 'contact_name': 'Remy Morris', 'contact_email': 'remy.morris@fabrikam.com', 'contact_phone': '610-321-0987', 'website': 'http://www.fabrikam.com', 'type': 'AI Services'}, {'id': 7, 'name': 'The Phone Company', 'address': '10642 Meridian St., Indianapolis', 'contact_name': 'Ashley Schroeder', 'contact_email': 'ashley.schroeder@thephone-company.com', 'contact_phone': '719-444-2345', 'website': 'http://www.thephone-company.com', 'type': 'Communications'}]Here are the vendors we are currently working with:\n\n1. **Adatum Corporation**\n   - **Type:** Data Engineering\n   - **Address:** 789 Goldsmith Road, MainTown City\n   - **Contact:** Elizabeth Moore\n   - **Email:** [elizabeth.moore@adatum.com](mailto:elizabeth.moore@adatum.com)\n   - **Phone:** 555-789-7890\n   - **Website:** [www.adatum.com](http://www.adatum.com)\n\n2. **Contoso Ltd.**\n   - **Type:** Software Engineering\n   - **Address:** 456 Industrial Road, Scooton City\n   - **Contact:** Nicole Wagner\n   - **Email:** [nicole@contoso.com](mailto:nicole@contoso.com)\n   - **Phone:** 555-654-3210\n   - **Website:** [www.contoso.com](http://www.contoso.com)\n\n3. **Lucerne Publishing**\n   - **Type:** Graphic Design\n   - **Address:** 789 Live Street, Woodgrove\n   - **Contact:** Ana Bowman\n   - **Email:** [abowman@lucernepublishing.com](mailto:abowman@lucernepublishing.com)\n   - **Phone:** 555-654-9870\n   - **Website:** [www.lucernepublishing.com](http://www.lucernepublishing.com)\n\n4. **VanArsdel Ltd.**\n   - **Type:** Software Engineering\n   - **Address:** 123 Innovation Drive, TechVille\n   - **Contact:** Gabriel Diaz\n   - **Email:** [gdiaz@vanarsdelltd.com](mailto:gdiaz@vanarsdelltd.com)\n   - **Phone:** 555-321-0987\n   - **Website:** [www.vanarsdelltd.com](http://www.vanarsdelltd.com)\n\n5. **Trey Research**\n   - **Type:** DevOps\n   - **Address:** 456 Research Avenue, Redmond\n   - **Contact:** Serena Davis\n   - **Email:** [serena.davis@treyresearch.net](mailto:serena.davis@treyresearch.net)\n   - **Phone:** 555-867-5309\n   - **Website:** [www.treyresearch.net](http://www.treyresearch.net)\n\n6. **Fabrikam Inc.**\n   - **Type:** AI Services\n   - **Address:** 24601 South St., Philadelphia\n   - **Contact:** Remy Morris\n   - **Email:** [remy.morris@fabrikam.com](mailto:remy.morris@fabrikam.com)\n   - **Phone:** 610-321-0987\n   - **Website:** [www.fabrikam.com](http://www.fabrikam.com)\n\n7. **The Phone Company**\n   - **Type:** Communications\n   - **Address:** 10642 Meridian St., Indianapolis\n   - **Contact:** Ashley Schroeder\n   - **Email:** [ashley.schroeder@thephone-company.com](mailto:ashley.schroeder@thephone-company.com)\n   - **Phone:** 719-444-2345\n   - **Website:** [www.thephone-company.com](http://www.thephone-company.com)\n\n&gt; Finished chain.\n</code></pre> <p>In the trace above:</p> <ol> <li> <p>Line 1 shows the <code>AgentExecutor</code> chain starting.</p> </li> <li> <p>Lines 3-5 show the agent invoking the <code>get_vendors</code> method and outputting the results into an array of vendor data.</p> </li> <li> <p>Lines 7-61 show the response from the LLM in markdown format.</p> </li> <li> <p>Line 63 shows the <code>AgentExecutor</code> chain has finished.</p> </li> </ol> </li> <li> <p>Stop the API debugger in VS Code.</p> </li> </ol> <p>You have successfully tested your <code>/chat</code> API endpoint!</p>"},{"location":"workshop/docs/05-Customize-Solution/06/","title":"5.6 Add Copilot Chat To UI","text":"<p>In this step, you will review the <code>CopilotChat</code> component of the front-end application and enable it so you can chat with your copilot via the Portal UI. You can create an interactive and responsive chat interface that enhances user engagement by leveraging React's dynamic component-based architecture and its seamless integration with real-time messaging APIs. The copilot chat feature empowers the application to provide real-time conversation capabilities, handle complex state management, and deliver intuitive interactions that make the user experience more engaging and efficient.</p>"},{"location":"workshop/docs/05-Customize-Solution/06/#review-copilot-chat-ui-component","title":"Review Copilot Chat UI Component","text":"<p>The Woodgrove Bank Contract Management Portal is a single-page application (SPA) built using React.js, a popular JavaScript framework for interactive user interfaces.</p> <p>A React component, <code>CopilotChat</code>, has been provided to allow you to easily integrate the copilot capability into the UI of the portal application. This component is implemented in the <code>src/userportal/src/components/CopilotChat.jsx</code> file. Open it now in VS Code and explore the code in sections. You can also expand the section below to see the code inline and review the explanations for each line of code.</p> Copilot Chat REACT component code. src/userportal/src/components/CopilotChat.jsx<pre><code>import React, { useState, useEffect, useRef } from 'react';\nimport ReactMarkdown from 'react-markdown';\nimport { Row, Col, Button, OverlayTrigger, Tooltip } from 'react-bootstrap';\nimport ConfirmModal from './ConfirmModal'; \nimport api from '../api/Api'; // Adjust the path as necessary\nimport './CopilotChat.css';\n\nconst CopilotChat = () =&gt; {\n  const [sessionId, setSessionId] = useState(-1);\n  const [messages, setMessages] = useState([]);\n  const [input, setInput] = useState('');\n  const messagesEndRef = useRef(null);\n  const [error, setError] = useState('');\n  const [isThinking, setIsThinking] = useState(false);\n\n  const [sessions, setSessions] = useState([]);\n  const [sessionToDelete, setSessionToDelete] = useState(null);\n  const [showDeleteModal, setShowDeleteModal] = useState(false);\n\n  const handleSendMessage = async () =&gt; {\n    if (input.trim() === '') return;\n\n    const prompt = input;\n    setInput('');\n\n    setIsThinking(true);\n\n    // Add the user's message to the local mesage history\n    const userMessage = { role: 'user', content: prompt };\n    setMessages([...messages, userMessage]);\n\n    setError('');\n\n    try {\n      // Get the completion from the API\n      const output = await api.completions.chat(sessionId, prompt);\n\n      // make sure request for a different session doesn't update the messages\n      if (sessionId === output.session_id) {\n        // Add the assistant's response to the messages\n        const assistantMessage = { role: 'assistant', content: output.content };\n        setMessages([...messages, userMessage, assistantMessage]);\n      }\n\n      // only update the messages if the session ID is the same\n      // This keeps a processing completion from updating messages after a new session is created\n      if (sessionId === -1 || sessionId !== output.session_id) {\n        // Update the session ID\n        setSessionId(output.session_id);\n      }\n    } catch (error) {\n      console.error('Error sending message:', error);\n      setError('Error sending message. Please try again.');\n    } finally {\n        setIsThinking(false);\n    }\n\n  };\n\n  const createNewSession = async () =&gt; {\n    setSessionId(-1);\n    setMessages([]);\n    setIsThinking(false);\n    setError('');\n  };\n\n  const refreshSessionList = async () =&gt; {\n    try {\n      const data = await api.completions.getSessions();\n      setSessions(data);\n    } catch (error) {\n      console.error('Error loading session history:', error);\n      setError('Error loading session history. Please try again.');\n    }\n  }\n\n  const loadSessionHistory = async () =&gt; {\n    if (!sessionId || sessionId &lt;= 0) {\n      setMessages([]);\n      return;\n    }\n    try {\n      const data = await api.completions.getHistory(sessionId);\n      setMessages(data);\n    } catch (error) {\n      console.error('Error loading session history:', error);\n      setError('Error loading session history. Please try again.');\n    }\n  }\n\n  useEffect(() =&gt; {\n    refreshSessionList();\n    loadSessionHistory();\n  }, [sessionId]);\n\n  useEffect(() =&gt; {\n    if (messagesEndRef.current) {\n      messagesEndRef.current.scrollIntoView({ behavior: 'smooth' });\n    }\n  }, [messages]);\n\n  useEffect(() =&gt; {\n    refreshSessionList();\n  }, []);\n\n  const handleDelete = async () =&gt; {\n    if (!sessionToDelete) return;\n\n    setError(null);\n    try {\n      await api.completions.deleteSession(sessionToDelete);\n\n      console.log('Session deleted:', sessionToDelete);\n      console.log('Current session:', sessionId);\n      if (sessionId === sessionToDelete) {\n        setSessionId(-1);\n      }\n    } catch (err) {\n      console.error('Error deleting session:', err);\n      setError('Error deleting session. Please try again.');\n    }\n    setShowDeleteModal(false);\n    refreshSessionList();\n  };\n\n  return (\n    &lt;div className=\"ai-chat container mt-4\"&gt;\n      &lt;Row&gt;\n        &lt;Col style={{ width: '10%', maxWidth: '10em' }}&gt;\n          &lt;Row&gt;\n            &lt;Button area-label=\"New Session\" alt=\"New Session\" onClick={createNewSession}&gt;\n              &lt;i className=\"fas fa-plus\"&gt;&lt;/i&gt; Chat\n            &lt;/Button&gt;\n          &lt;/Row&gt;\n          &lt;Row className=\"mt-3\"&gt;\n            &lt;strong&gt;Chat History&lt;/strong&gt;\n            {!sessions || sessions.length === 0 &amp;&amp; &lt;p&gt;No sessions&lt;/p&gt;}\n            {sessions &amp;&amp; sessions.length &gt; 0 &amp;&amp; &lt;ul className=\"session-list\"&gt;\n              {sessions.map((session, index) =&gt; (\n                  &lt;li key={index}\n                    className={`session ${sessionId === session.id ? 'selected' : ''}`}\n                    style={{ display: 'flex', justifyContent: 'space-between', alignItems: 'center', padding: '10px', borderBottom: '1px solid #ccc', cursor: 'pointer' }}\n                    onClick={() =&gt; setSessionId(session.id)}\n                  &gt;\n                    &lt;OverlayTrigger\n                      placement=\"top\"\n                      delay={{ show: 250, hide: 400 }}\n                      overlay={&lt;Tooltip id={`tooltip-${index}`}&gt;{session.name.substring(0, 300)}&lt;/Tooltip&gt;}\n                    &gt;\n                      &lt;a alt={session.name}&gt;{session.name}&lt;/a&gt;\n                    &lt;/OverlayTrigger&gt;\n                    &lt;div&gt;\n                      &lt;OverlayTrigger\n                        placement=\"top\"\n                        delay={{ show: 250, hide: 400 }}\n                        overlay={&lt;Tooltip id={`delete-tooltip-${index}`}&gt;Delete Session&lt;/Tooltip&gt;}\n                      &gt;\n                        &lt;Button className=\"btn-danger\" style={{ marginRight: '10px' }}\n                          title=\"Delete Session\"\n                          onClick={(e) =&gt; { setSessionToDelete(session.id); setShowDeleteModal(true); e.stopPropagation(); }}&gt;\n                          &lt;i className=\"fas fa-trash\"&gt;&lt;/i&gt;\n                        &lt;/Button&gt;\n                      &lt;/OverlayTrigger&gt;\n                    &lt;/div&gt;\n                  &lt;/li&gt;\n                ))}\n              &lt;/ul&gt;}\n          &lt;/Row&gt;\n        &lt;/Col&gt;\n        &lt;Col&gt;\n          &lt;div className=\"messages mb-3 border p-3\" style={{ minHeight: '20em', maxHeight: '50em', overflowY: 'scroll' }}&gt;\n            {messages.map((msg, index) =&gt; (\n              &lt;div key={index} className={`message ${msg.role} mb-2 d-flex ${msg.role === 'user' ? 'justify-content-end' : 'justify-content-start'}`}&gt;\n                {!error &amp;&amp; index === messages.length - 1 &amp;&amp; &lt;div ref={messagesEndRef} /&gt;}\n                &lt;div className={`alert ${msg.role === 'user' ? 'alert-primary' : 'alert-secondary'}`} style={{ maxWidth: '90%' }} role=\"alert\"&gt;\n                  &lt;ReactMarkdown&gt;{msg.content}&lt;/ReactMarkdown&gt;\n                &lt;/div&gt;\n              &lt;/div&gt;\n            ))}\n            {error &amp;&amp; &lt;div className=\"alert alert-danger\" role=\"alert\"&gt;{error}&lt;div ref={messagesEndRef} /&gt;&lt;/div&gt;}\n            {isThinking &amp;&amp; &lt;div className=\"d-flex justify-content-center\"&gt;\n                &lt;div className=\"spinner-border text-info\" role=\"status\"&gt;\n                  &lt;span className=\"visually-hidden\"&gt;Thinking...&lt;/span&gt;\n                &lt;/div&gt;\n                &lt;div ref={messagesEndRef} /&gt;\n              &lt;/div&gt;}\n          &lt;/div&gt;\n          &lt;div className=\"input-container d-flex\"&gt;\n            &lt;textarea className=\"form-control me-2\"\n              value={input}\n              onChange={(e) =&gt; setInput(e.target.value)}\n              onKeyDown={(e) =&gt; { if (e.key === 'Enter') { handleSendMessage(e); e.preventDefault(); return false; } }}\n              placeholder=\"Type a message...\"\n            &gt;&lt;/textarea&gt;\n            &lt;Button onClick={handleSendMessage}&gt;Send&lt;/Button&gt;\n          &lt;/div&gt;\n        &lt;/Col&gt;\n      &lt;/Row&gt;\n\n      &lt;ConfirmModal\n        show={showDeleteModal}\n        handleClose={() =&gt; setShowDeleteModal(false)}\n        handleConfirm={handleDelete}\n        message=\"Are you sure you want to delete this session?\"\n      /&gt;\n    &lt;/div&gt;\n  );\n};\n\nexport default CopilotChat;\n</code></pre> <ol> <li> <p>Import components and libraries (lines 1-6): Required components and libraries are imported.</p> </li> <li> <p>Define the <code>CopilotChat</code> functional component (line 8). Components in React.js are created using a functional component or class component.</p> </li> <li> <p>Declare state variables (lines 9-18): State variables are used to maintain the chat session state, including message history.</p> </li> <li> <p>Provide function for sending messages to the API (lines 20-58): The <code>handleSendMessage</code> function sends messages asynchronously to the Woodgrove Bank API. This function handles the UI's interaction with the backend <code>/completions/chat</code> endpoint, sending the user query and session id to the API.</p> </li> <li> <p>Provide functions for handling loading chat sessions (lines 60-89): The <code>createNewSession</code> function sets up the state variables to start a new chat session, the <code>refreshSessionList</code> function handles the UI's interaction with the backend <code>/completions</code> endpoint for loading previous chat sessions, and the <code>loadSessionHistory</code> function handles the UI's interaction with the backend <code>/completions</code> endpoint for loading the chat history for the selected chat session.</p> </li> <li> <p>Handle changes in the Session ID (lines 91-94): The <code>useEffect</code> hook is used to run code in response to changes in the sessionId to refresh the chat session list and load the chat history for the newly selected chat session.</p> </li> <li> <p>Handle changes in the messages collection (lines 96-100): The <code>useEffect</code> hook is used to run code in response to changes in the messages array.</p> </li> <li> <p>Handle loading chat session list on page load (lines 102-104): The <code>useEffect</code> hook is used to run code that loads the list of previous chat sessions into the UI on page load.</p> </li> <li> <p>Provide function for deleting chat session (lines 106-124): The <code>handleDelete</code> function handles the UI's interaction with the backend <code>/completions</code> endpoint for deleting the chosen chat session.</p> </li> <li> <p>Return the component (lines 126-208): The return statement renders the component's JSX, defining how it is presented in the web browser.</p> </li> <li> <p>Export the <code>CopilotChat</code> component (line 210): Exports the <code>CopilotChat</code> component as the default export.</p> </li> </ol>"},{"location":"workshop/docs/05-Customize-Solution/06/#enable-the-copilot-chat-ui","title":"Enable the Copilot Chat UI","text":"<p>To enable the copilot chat feature in the Woodgrove Bank Contract Management Portal UI, you will add a reference to the component on the <code>Dashboard</code> page of the UI. The dashboard page is defined in the <code>src/userportal/src/pages/dashboard/dashboard.jsx</code> file. Expand the section below to review the code for the page below.</p> Dashboard page code src/userportal/src/pages/dashboard/dashboard.jsx<pre><code>import React from 'react';\n\nconst Dashboard = () =&gt; {\n  return (\n    &lt;div className=\"table-responsive\"&gt;\n      &lt;div className=\"d-flex justify-content-between flex-wrap flex-md-nowrap align-items-center pt-3 pb-2 mb-3 border-bottom\"&gt;\n        &lt;h1 className=\"h2\"&gt;Dashboard&lt;/h1&gt;\n      &lt;/div&gt;\n\n    &lt;/div&gt;\n  );\n};\n\nexport default Dashboard;\n</code></pre> <ol> <li> <p>In the VS Code Explorer, navigate to the <code>src/userportal/src/pages/dashboard</code> folder and open the <code>dashboard.jsx</code> file.</p> </li> <li> <p>To import the AI chat component, insert the following <code>import</code> statement directly below the <code>import React from 'react';</code> line at the top of the file.</p> <p>Paste the following import statement into <code>dashboard.jsx</code>!</p> JavaScript<pre><code>import CopilotChat from '../../components/CopilotChat';\n</code></pre> </li> <li> <p>Insert the following code below the closing tag of <code>&lt;div&gt;</code> containing the Dashboard header (line 9). This will insert the component within the <code>const Dashboard =() =&gt; {}</code> functional component block of the dashboard page.</p> <p>Paste the following component code into <code>dashboard.jsx</code>!</p> JavaScript<pre><code>&lt;CopilotChat /&gt;\n</code></pre> </li> <li> <p>The final <code>Dashboard</code> code should look like the following:</p> src/userportal/src/pages/dashboard/dashboard.jsx<pre><code>import React from 'react';\nimport CopilotChat from '../../components/CopilotChat';\n\nconst Dashboard = () =&gt; {\n  return (\n    &lt;div className=\"table-responsive\"&gt;\n      &lt;div className=\"d-flex justify-content-between flex-wrap flex-md-nowrap align-items-center pt-3 pb-2 mb-3 border-bottom\"&gt;\n        &lt;h1 className=\"h2\"&gt;Dashboard&lt;/h1&gt;\n      &lt;/div&gt;\n      &lt;CopilotChat /&gt;\n    &lt;/div&gt;\n  );\n};\n\nexport default Dashboard;\n</code></pre> </li> </ol>"},{"location":"workshop/docs/05-Customize-Solution/07/","title":"5.7 Test the UI Copilot Chat","text":"<p>You are now ready to test the end-to-end copilot chat feature. You must run the FastAPI server and the React SPA locally from VS Code debug sessions. In this next section, you will see how to do this locally for rapid prototyping and testing.</p>"},{"location":"workshop/docs/05-Customize-Solution/07/#test-with-vs-code","title":"Test with VS Code","text":"<p>Visual Studio Code provides the ability to run applications locally, allowing for debugging and rapid testing.</p>"},{"location":"workshop/docs/05-Customize-Solution/07/#start-the-api","title":"Start the API","text":"<p>The UI relies on the Woodgrove Bank API to be running. As you did to test the API via its Swagger UI, follow the steps below to start a debug session for the API in VS Code.</p> <ol> <li> <p>In Visual Studio Code Run and Debug panel, select the API Debugger option for your OS from the debug configurations dropdown list.</p> <p></p> </li> <li> <p>Select the Start Debugging button (or press F5 on your keyboard).</p> <p></p> </li> <li> <p>Wait for the API application to start completely, indicated by an <code>Application startup complete.</code> message in the terminal output.</p> <p></p> </li> </ol>"},{"location":"workshop/docs/05-Customize-Solution/07/#start-the-portal","title":"Start the Portal","text":"<p>With the API running, you can start a second debug session in VS Code for the Portal project.</p> <ol> <li> <p>Return to the Run and Debug panel in Visual Studio Code and select the Portal Debugger option from the debug configurations dropdown list.</p> <p></p> </li> <li> <p>Select the Start Debugging button (or press F5 on your keyboard).</p> <p></p> </li> <li> <p>This should launch the Woodgrove Bank Contract Management Portal in a new browser window (http://localhost:3000/).</p> </li> <li> <p>On the Dashboard page, enter the following message into the chat and send it:</p> <p>Paste the following prompt in the copilot chat box!</p> <pre><code>What IT vendors are we working with?\n</code></pre> <p></p> </li> <li> <p>Next, ask the following question about vendor invoicing accuracy:</p> <p>Paste the following prompt in the copilot chat box!</p> <pre><code>What vendor has had the most invoicing issues?\n</code></pre> <p></p> </li> </ol> <p>The response to the previous question may be accurate, but the copilot did not provide any insights about why this vendor was specified as having the most invoicing issues. You may also have received an error because the copilot lacks guidance on correctly selecting the proper function for augmenting the prompt. In either case, prompt engineering can help improve your copilot's response!</p> <p>In the next task, you will iterate on your copilot's prompt to refine it and improve the quality and groundedness of the responses it provides.</p> <p>Leave the API and Portal debug sessions running for the next task.</p>"},{"location":"workshop/docs/05-Customize-Solution/08/","title":"5.8 Refine the Copilot Prompt","text":"<p>Prompt engineering is the art of crafting precise and effective prompts to guide AI models in generating desired outputs, making them respond in a way that aligns closely with the user's intentions and context. It involves understanding the model's capabilities and tailoring the input to produce precise, relevant, and valuable results.</p> <p>The initial prompt used by your copilot looks like this:</p> Starter prompt<pre><code>You are an intelligent copilot for Woodgrove Bank, designed to provide insights about the bank's vendors, active Statements of Work (SOWs), and the accuracy of invoices submitted in relation to those SOWs. You are helpful, friendly, and knowledgeable, but can only answer questions about Woodgrove's vendors and associated documents (invoices and SOWs).\n</code></pre> <p>This is a very basic prompt that does little to provide guidance to your copilot in how it interacts with users, leverages the tools available to it, or responds to questions. Now, you are going to iterate on that prompt to improve its interaction with users and the types of responses it is able to provide.</p> <p>Prompt engineering typically involves an iterative process of making minor changes to the prompt and then testing their impact. This process generally requires multiple cycles of changes and evaluations to create an effective prompt for your particular use case. Typicall, the iterative follows steps similar to the following:</p> <ol> <li>Ask questions</li> <li>Review responses</li> <li>Modify prompt</li> <li>Restart the API</li> <li>Repeat the above steps until you are satisfied with the quality and relevance of the responses you are receiving.</li> </ol>"},{"location":"workshop/docs/05-Customize-Solution/08/#iterate-to-improve-your-prompt","title":"Iterate to improve your prompt","text":"<p>You will use the active VS Code debug sessions for both the API and the Portal you started in the last task to test a few changes to the copilot's prompt and evaluate the responses you are receiving.</p> <ol> <li> <p>Open the copilot prompt by navigating to the <code>src/api/app/prompts</code> folder in the VS Code Explorer and opening the <code>copilot.txt</code> file.</p> <p>Prompts are loaded by the API's lifespan manager</p> <p>In the Woodgrove Bank API project, prompts are stored in text files and loaded into memory by the <code>lifespan_manager</code> used by the FastAPI application.</p> </li> <li> <p>To get started, add the following language to the bottom of the prompt to provide some guidelines it should follow when responding to questions. This guidance contains details about the information that should be included in the response based on the type of information the user query is about:</p> <p>Update the prompt with response guidelines!</p> Provide response guidelines<pre><code>Use the following guidelines when responding:\n- If asked about a specific vendor, provide information about the vendor, their SOWs, and invoices. Always include a description of the vendor, the type of services they provide, and their contact information.\n- If asked about a specific SOW, always provide information about the SOW, its milestones, and deliverables. Include the name of the vendor performing the work.\n- If asked about a specific invoice, always provide information about the invoice and its line items.\n</code></pre> </li> <li> <p>Your prompt should now resemble this:</p> Intermediate prompt<pre><code>You are an intelligent copilot for Woodgrove Bank, designed to provide insights about the bank's vendors, active Statements of Work (SOWs), and the accuracy of invoices submitted in relation to those SOWs. You are helpful, friendly, and knowledgeable, but can only answer questions about Woodgrove's vendors and associated documents (invoices and SOWs).\n\nUse the following guidelines when responding:\n- If asked about a specific vendor, provide information about the vendor, their SOWs, and invoices. Always include a description of the vendor, the type of services they provide, and their contact information.\n- If asked about a specific SOW, always provide information about the SOW, its milestones, and deliverables. Include the name of the vendor performing the work.\n- If asked about a specific invoice, always provide information about the invoice and its line items.\n</code></pre> </li> <li> <p>In VS Code, restart the API Debugger session using the floating debugger toolbar. Ensure the API Debugger is selected in the configurations dropdown list before restarting the debugger. Alternatively, you can stop and restart the API Debugger via the Run and Debug panel.</p> <p></p> </li> <li> <p>In the Woodgrove Bank Contract Management Portal, ask a few questions about specific vendors, SOWs, and invoices and evaluate the responses.</p> </li> <li> <p>Now, make another change to the copilot prompt by adding the following guidance on using validation results to assess a vendor's performance and billing accuracy.</p> <p>Update the prompt with additional instructions!</p> Provide instructions about assessing performance and accuracy<pre><code>When asked about a vendor's performance or billing accuracy:\n1. Use validation results for SOWs and invoices to perform your analysis.\n2. Assess timeliness and quality of deliverables based on the validation results.\n3. Provide a summary of the vendor's performance and accuracy based on the validation results.\n4. Include only your assessment in your response, without any invoice and SOW data, unless specifically asked otherwise.\n</code></pre> </li> <li> <p>Your updated prompt should look like:</p> Final prompt<pre><code>You are an intelligent copilot for Woodgrove Bank, designed to provide insights about the bank's vendors, active Statements of Work (SOWs), and the accuracy of invoices submitted in relation to those SOWs. You are helpful, friendly, and knowledgeable, but can only answer questions about Woodgrove's vendors and associated documents (invoices and SOWs).\n\nUse the following guidelines when responding:\n- If asked about a specific vendor, provide information about the vendor, their SOWs, and invoices. Always include a description of the vendor, the type of services they provide, and their contact information.\n- If asked about a specific SOW, always provide information about the SOW, its milestones, and deliverables. Include the name of the vendor performing the work.\n- If asked about a specific invoice, always provide information about the invoice and its line items.\n\nWhen asked about a vendor's performance or billing accuracy:\n1. Use validation results for SOWs and invoices to perform your analysis.\n2. Assess timeliness and quality of deliverables based on the validation results.\n3. Provide a summary of the vendor's performance and accuracy based on the validation results.\n4. Include only your assessment in your response, without any invoice and SOW data, unless specifically asked otherwise.\n</code></pre> </li> <li> <p>Restart the API Dugger session again.</p> </li> <li> <p>Return to the Woodgrove Bank Contract Management Portal, and ask some questions about vendor performance, accuracy, and billing issues. Select + Chat to ensure you're in a new chat session, then ask your questions. For example, ask, \"What vendor has had the most invoicing issues?\"</p> <p></p> </li> <li> <p>Review the response, comparing it to the more direct response from your initial prompt. Ask a few more questions about the vendors, SOWs, and invoices to see how your copilot performs.</p> </li> <li> <p>Continue to iterate on the prompt until you are happy with the responses you are receiving.</p> </li> </ol> <p>Congratulations! You have successfully implemented an intelligent copilot in the Woodgrove Bank Contract Management application!</p>"},{"location":"workshop/docs/06-Video-Processing/","title":"Video Processing","text":"Using your own data? <p>To effectively integrate your own data into the solution accelerator, it's essential to adapt the existing components to align with your specific data structures and relationships.</p> <p>1. Customize the Semantic Ranker The semantic ranker enhances search relevance by reordering search results based on contextual understanding. To tailor it to your data:</p> <ul> <li>Understand the Existing Semantic Ranker: Review the current implementation to comprehend how it processes and ranks data.</li> <li>Adapt Ranking Functions: Modify the ranking algorithms to consider the unique attributes and relationships within your data. This may involve adjusting weightings or incorporating additional data fields to improve relevance.</li> <li>Test and Validate: After modifications, rigorously test the ranker's performance to ensure it accurately reflects the importance and context of your data elements.</li> </ul> <p>2. Modify GraphRAG Function Calls Graph Retrieval-Augmented Generation (GraphRAG) combines knowledge graphs with AI models to provide contextually rich responses. To align GraphRAG with your data:</p> <ul> <li>Analyze Your Data Structure: Identify the nodes (entities) and edges (relationships) that are pertinent to your domain.</li> <li>Adjust Function Calls: Modify existing GraphRAG function calls to navigate your specific graph schema effectively. This includes updating queries to traverse the correct nodes and edges that mirror your data's relationships.</li> <li>Integrate with AI Models: Ensure that the data retrieved via GraphRAG is appropriately fed into your AI models to enhance response generation with accurate context.</li> </ul> <p>3. Revise Data Export Procedures Exporting data accurately is crucial for maintaining the integrity of your knowledge graph. To adapt the export process:</p> <ul> <li>Map Data Relationships: Clearly define how your data entities relate to one another, establishing the nodes and edges that will form your knowledge graph.</li> <li>Update Export Scripts: Modify data export scripts to extract information from the appropriate tables and fields that correspond to your defined nodes and edges.</li> <li>Validate Exported Data: After exporting, verify that the data correctly represents the intended relationships and entities within your knowledge graph framework.</li> </ul> <p>As Generative AI (GenAI) becomes increasingly integral to modern enterprises, users' trust in these applications is paramount and heavily reliant on the accuracy of their responses. The productivity loss from incorrect answers and the consequent erosion of user trust are issues that cannot be overlooked. For many organizations, the decision to deploy GenAI apps hinges on their ability to elevate the accuracy of the app's responses to an acceptable level.</p> <p>In this section, you will use Semantic Ranking and GraphRAG to enhance the accuracy of responses from the Woodgrove Bank Contract Management copilot. Here's what you will accomplish:</p> <ul> <li> Review Semantic Ranking</li> <li> Use Semantic Ranking to rerank hybrid search results</li> <li> Review GraphRAG and the Apache AGE extension</li> <li> Implement the AGE extension to enable graph queries against your PostgreSQL database</li> </ul>"},{"location":"workshop/docs/06-Video-Processing/#the-accuracy-problem","title":"The Accuracy Problem","text":"<p>Despite significant advancements, accuracy remains a challenge for GenAI. Retrieval Augmented Generation (RAG) helps by grounding responses in factual data. Still, as datasets expand or when documents become too similar, vector search techniques can falter, leading to a loss of user trust and diminished productivity. Enhancing accuracy requires optimizing the information retrieval pipeline through various techniques. General methods include chunking, larger embeddings, and hybrid search, while advanced, dataset-specific approaches like semantic ranking and GraphRAG are essential.</p>"},{"location":"workshop/docs/06-Video-Processing/#enhance-genai-accuracy-with-advanced-techniques","title":"Enhance GenAI Accuracy with Advanced Techniques","text":"<p>Innovations like Semantic Ranking and GraphRAG are crucial to address the accuracy problem. These techniques enhance the RAG approach by improving how grounding data is gathered and integrated into AI model responses, thereby increasing precision and reliability. Optimizing information retrieval pipelines through advanced techniques ensures that GenAI applications deliver accurate, trustworthy, and insightful responses, thereby maintaining user trust and productivity.</p> <p>Select the tabs below to understand how Semantic Ranking and GraphRAG can improve RAG accuracy.</p> Semantic RankingGraphRAG <p>Semantic ranking is an advanced technique used in information retrieval systems to enhance the relevance of search results by understanding the context and meaning of queries and documents rather than relying solely on keyword matching. By leveraging natural language processing and machine learning algorithms, semantic ranking can analyze the relationships between words and concepts to provide more accurate and meaningful results. This approach allows search engines to better comprehend the intent behind user queries and deliver results that are contextually aligned with what users are looking for. The benefits of semantic ranking include improved accuracy in search results, enhanced user satisfaction, and more efficient retrieval of information, making it a powerful tool for modern search engines and recommendation systems.</p> <p>GraphRAG, developed by Microsoft Research, is an innovative approach to information retrieval and generation, combining the power of knowledge graphs with large language models (LLMs) to enhance the accuracy and relevance of generated content. GraphRAG allows LLMs to provide more contextually aware and insightful responses by integrating structured data from knowledge graphs into the retrieval process. This method enhances the model's ability to understand complex queries and synthesize information from various sources, leading to more accurate and informative outputs. GraphRAG's benefits include improved information retrieval, enhanced reasoning capabilities, and the ability to deliver precise answers even when dealing with intricate or multi-faceted questions.</p>"},{"location":"workshop/docs/06-Video-Processing/01/","title":"6.1 Understand Semantic Ranking","text":"<p>Semantic ranking improves accuracy of the vector search by re-ranking results using a semantic ranker model, which brings more relevant items to the top of the ranked list. In the context of Azure Database for PostgreSQL, semantic ranking can enhance the accuracy and relevance of search results within the database, significantly improving the information retrieval pipelines in Generative AI (GenAI) applications. Unlike vector search, which primarily measures the similarity between two vector embeddings, semantic ranking delves deeper by analyzing the semantic relevance between text strings at the actual text level. This approach ensures that search results are more contextually aligned with user queries, improving information retrieval and higher user satisfaction.</p>"},{"location":"workshop/docs/06-Video-Processing/01/#reranking-search-results-with-semantic-ranker-models","title":"Reranking Search Results with Semantic Ranker Models","text":"<p>Semantic ranker models compare two text strings: the search query and the text of one of the items being searched. The ranker produces a relevance score that indicates how well the text string matches the query, essentially determining if the text holds an answer to the query. Typically, the semantic ranker is a machine learning model, often a variant of the BERT language model fine-tuned for ranking tasks, but it can also be a large language model (LLM). These ranker models, also called cross-encoder models, take two text strings as input and output a relevance score, usually ranging from 0 to 1.</p>"},{"location":"workshop/docs/06-Video-Processing/01/#semantic-ranker-model-inference-from-postgresql","title":"Semantic Ranker Model Inference from PostgreSQL","text":"<p>The <code>azure_ai</code> extension enables the invocation of machine learning models deployed on Azure Machine Learning online endpoints directly from within PostgreSQL.</p> <p></p> <p>Using the <code>azure_ml.invoke()</code> method, inference with a semantic ranker model can be performed through SQL queries. This allows for the reranking of vector search results based on the semantic relevance of the text, rather than relying solely on keyword matching.</p>"},{"location":"workshop/docs/06-Video-Processing/02/","title":"6.2 Use Semantic Ranking in PostgreSQL","text":"<p>By leveraging the power of semantic re-ranking, you can improve the accuracy of data retrieval and help ensure the success of your Generative AI applications. In this task, you will create a user-defined function (UDF) in your database that utilizes model inferencing capabilities of the Azure AI extension. Specifically, the <code>azure_ml.invoke()</code> method of the extension will be called from within the UDF to seamlessly invoke a semantic ranking model directly within SQL queries.</p> Your Semantic Ranker model was deployed to Azure ML. <p>Depending on what semantic re-ranker setting you chose during azd up deployment, the deployment of this solution accelerator included executing a post-deployment script to download either the <code>mini</code> model: ms-marco-MiniLM-L6-v2 or the <code>bge</code> model: BGE-reranker-v2-m3 cross encoder model and deploy it as an Azure Machine Learning (AML) inference endpoint in your subscription. These models were selected because they are lightweight, easy to deploy, and provides fast inference.</p>"},{"location":"workshop/docs/06-Video-Processing/02/#create-a-reranking-function","title":"Create a reranking function","text":"<p>To simplify the integration of semantic ranking and invocation of the semantic ranker model from Azure ML, you will create a user-defined function (UDF). The function uses the <code>azure_ai</code> extension to directly make remote calls to the AML inference endpoint from a SQL query. Using the <code>azure_ml.invoke()</code> function within a UDF, you can make calls to the semantic ranker model.</p> You already configured the extension's connection to Azure ML <p>Recall that you already configured the Azure AI extension's connection to Azure ML in task 3.2 Configure the Azure AI extension using the following commands:</p> <pre><code>SELECT azure_ai.set_setting('azure_ml.scoring_endpoint','&lt;endpoint&gt;');\nSELECT azure_ai.set_setting('azure_ml.endpoint_key', '&lt;api-key&gt;');\n</code></pre> <p>The UDF will be defined within your SQL database, allowing you to seamlessly incorporate semantic ranking into your queries. You will use pgAdmin to create it.</p> <ol> <li> <p>Return to the open instance of pgAdmin on your local machine and ensure it is connected to your PostgreSQL database.</p> </li> <li> <p>In the pgAdmin Object Explorer, expand databases under your PostgreSQL server.</p> </li> <li> <p>Right-click the contracts database and select Query Tool from the context menu.</p> </li> <li> <p>Run one of the following queries to create the <code>semantic_reranking</code> function:</p> <ul> <li>Run the first one if you chose the <code>mini</code> model</li> <li>Run the second one if you chose the <code>bge</code> model</li> </ul> <p>Execute one of the following SQL commands below in pgAdmin!</p> For mini model - Create Semantic Reranking UDF<pre><code>CREATE OR REPLACE FUNCTION semantic_reranking(query TEXT, vector_search_results TEXT[])\nRETURNS TABLE (content TEXT, relevance NUMERIC) AS $$\nBEGIN\n    RETURN QUERY\n    WITH\n    json_pairs AS (\n        SELECT jsonb_build_object(\n            'pairs', jsonb_agg(jsonb_build_array(query, content_))\n        ) AS json_pairs_data\n        FROM (\n            SELECT a.content AS content_\n            FROM unnest(vector_search_results) AS a(content)\n        )\n    ),\n    relevance_scores_raw AS (\n        SELECT azure_ml.invoke(\n            (SELECT json_pairs_data FROM json_pairs),\n            deployment_name =&gt; 'msmarco-minilm-deployment-6',\n            timeout_ms =&gt; 120000\n        ) AS response_json\n    ),\n    relevance_scores AS (\n    SELECT jsonb_array_elements(response_json) AS item\n    FROM relevance_scores_raw\n    )\n    SELECT\n        item -&gt;&gt; 'content' AS content,\n        (item -&gt;&gt; 'score')::NUMERIC AS relevance\n    FROM relevance_scores;\nEND $$ LANGUAGE plpgsql;\n</code></pre> For bge model - Create Semantic Reranking UDF<pre><code>CREATE OR REPLACE FUNCTION semantic_reranking(query TEXT, vector_search_results TEXT[])\nRETURNS TABLE (content TEXT, relevance NUMERIC) AS $$\nBEGIN\n    RETURN QUERY\n        WITH\n        json_pairs AS(\n            SELECT jsonb_build_object(\n                    'pairs', \n                    jsonb_agg(\n                        jsonb_build_array(query, content_)\n                    )\n                ) AS json_pairs_data\n            FROM (\n                SELECT a.content as content_\n                FROM unnest(vector_search_results) as a(content)\n            )\n        ), \n        relevance_scores AS(\n            SELECT jsonb_array_elements(invoke.invoke) as relevance_results\n            FROM azure_ml.invoke(\n                    (SELECT json_pairs_data FROM json_pairs),\n                    deployment_name=&gt;'bgev2m3-v1', timeout_ms =&gt; 120000)\n        ),\n        relevance_scores_rn AS (\n            SELECT *, ROW_NUMBER() OVER () AS idx\n            FROM relevance_scores\n        )\n        SELECT a.content,\n               (r.relevance_results::TEXT)::NUMERIC AS relevance\n            FROM\n                unnest(vector_search_results) WITH ORDINALITY AS a(content, idx2)\n            JOIN\n                relevance_scores_rn AS r(relevance_results, idx)\n            ON\n                a.idx2 = r.idx;\nEND $$ LANGUAGE plpgsql;\n</code></pre> How does the `semantic_reranking function work? <p>The <code>semantic_reranking</code> function enhances search results by re-ranking them based on their semantic relevance to a given query. Here's a breakdown of how it works:</p> <p>Input Parameters</p> <ul> <li><code>query</code>: A text string representing the search query.</li> <li><code>vector_search_results</code>: An array of text strings representing the initial search results obtained from a vector search.</li> </ul> <p>Return Value</p> <ul> <li>The function returns a table with two columns: <code>content</code> (the original search result content) and <code>relevance</code> (a NUMERIC object representing the relevance score).</li> </ul> <p>Steps</p> <ul> <li>Json Pairs Construction: The function starts by constructing a JSON object that pairs the query with each initial search result.</li> <li>Relevance Scoring: It then calls the <code>azure_ml.invoke</code> function to send the JSON pairs to an Azure Machine Learning endpoint, which computes the relevance scores for each pair. The results are returned as a JSON array.</li> <li>Row Number Assignment: The relevance scores are assigned row numbers to maintain their order.</li> <li>Combining Results: Finally, the function combines the original search results with corresponding relevance scores using the row numbers, ensuring that each result is paired with the correct relevance score.</li> </ul> <p>This function's overall purpose is to improve the relevance of search results by leveraging a semantic model hosted on Azure ML. This ensures that the results returned are more contextually relevant to the user's query.</p> </li> </ol>"},{"location":"workshop/docs/06-Video-Processing/02/#test-the-udf","title":"Test the UDF","text":"<p>To see how semantic ranking works in your queries, you can execute a test query from pgAdmin. The example below shows how to perform semantic reranking of vector search results for the query \"cost management and optimization\" over the <code>sow_chunks</code> table using the <code>semantic_reranking</code> UDF you created.</p> <ol> <li> <p>In the pgAdmin query window, run the following query and observe the results.</p> <p>Execute the following SQL command in pgAdmin! (Works for both <code>mini</code> and <code>bge</code> models)</p> Semantic ranking query<pre><code>WITH vector_results AS (\n    SELECT content FROM sow_chunks c\n    ORDER BY embedding &lt;=&gt; azure_openai.create_embeddings('embeddings', 'cost management and optimization')::vector\n    LIMIT 10\n)\nSELECT content, relevance\nFROM semantic_reranking('cost management and optimization',  ARRAY(SELECT content from vector_results))\nORDER BY relevance DESC\nLIMIT 3;\n</code></pre> </li> </ol> <p>Next, you will update the <code>get_sow_chunks</code> function used by your copilot to use semantic ranking to improve the accuracy and quality of the copilot's responses.</p>"},{"location":"workshop/docs/06-Video-Processing/03/","title":"6.3 Update Copilot With Semantic Ranking","text":"<p>The next step is to update your API to use the semantic ranking capability. For this, you will update how it finds and retrieves SOW chunks, which are the blocks of content extracted from each SOW uploaded into the application.</p>"},{"location":"workshop/docs/06-Video-Processing/03/#review-the-function","title":"Review the function","text":"<p>Following the function calling pattern used by your LangChain agent to retrieve data from the database, you will use a Python function to execute semantic ranking queries from your copilot. Within the <code>src/api/app/functions/chat_functions.py</code> file, the <code>find_sow_chunks_with_semantic_ranking</code> function has been provided for executing queries using the <code>semantic_reranking</code> UDF you added to your database in the previous step. Open it now in Visual Studio Code and explore the code with the function. You can also expand the section below to see the code inline.</p> Find SOW Chunks with Semantic Ranking code src/api/app/functions/chat_functions.py<pre><code>async def find_sow_chunks_with_semantic_ranking(self, user_query: str, vendor_id: int = None, sow_id: int = None, max_results: int = 3):\n        \"\"\"\n        Retrieves content chunks similar to the user query for the specified SOW.\n        \"\"\"\n\n        # Get the embeddings for the user query\n        query_embeddings = await self.__create_query_embeddings(user_query)\n\n        # Create a vector search query\n        cte_query = f\"SELECT content FROM sow_chunks\"\n        cte_query += f\" WHERE sow_id = {sow_id}\" if sow_id is not None else f\" WHERE vendor_id = {vendor_id}\" if vendor_id is not None else \"\"\n        cte_query += f\" ORDER BY embedding &lt;=&gt; '{query_embeddings}'\"\n        cte_query += f\" LIMIT 10\"\n\n        # Create the semantic ranker query\n        query = f\"\"\"\n        WITH vector_results AS (\n            {cte_query}\n        )\n        SELECT content, relevance\n        FROM semantic_reranking('{user_query}',  ARRAY(SELECT content from vector_results))\n        ORDER BY relevance DESC\n        LIMIT {max_results};\n        \"\"\"\n\n        rows = await self.__execute_query(f'{query};')\n        return [dict(row) for row in rows]\n</code></pre> <ol> <li> <p>Generate embeddings (line 18): Azure OpenAI generates embeddings representing the user query.</p> </li> <li> <p>Create vector search query (lines 21-24): The UDF expects vector search results as input, so a vector search query is created. The query selects the <code>content</code> field from the <code>sow_chunks</code> table and orders them by semantic similarity.</p> </li> <li> <p>Create a semantic ranker query (lines 27-35): The results of the vector search query are required to call the <code>semantic_reranking</code> UDF.</p> <ul> <li>Create CTE (lines 28-30): A common table expression (CTE) executes the vector search query and extracts the content values and relevancy scores from that query.</li> <li>Execute the <code>semantic_reranking</code> UDF (lines 31-33): Using the results of the CTE, the results are reranked using the <code>semantic_reranking</code> UDF.</li> <li>Limit results (line 34): The number of results is limited to ensure the more relevant records are sent to the LLM.</li> </ul> </li> <li> <p>Return the results (lines 37-38): The query results are extracted and returned to the LLM.</p> </li> </ol>"},{"location":"workshop/docs/06-Video-Processing/03/#implement-semantic-ranker","title":"Implement semantic ranker","text":"<p>To use the semantic ranking functionality instead of the vector search to retrieve SOW chunks, you must replace the function assigned to your LangChain agent's <code>tools</code> collection. You will replace the <code>find_sow_chunks</code> tool with the <code>find_sow_chunks_with_semantic_ranking</code> function.</p> <ol> <li> <p>In the VS Code Explorer, navigate to the <code>src/api/app/routers</code> folder and open the <code>completions.py</code> file.</p> </li> <li> <p>Within the <code>tools</code> array, locate the following line:</p> Python<pre><code>StructuredTool.from_function(coroutine=cf.find_sow_chunks),\n</code></pre> </li> <li> <p>Replace that line with the following:</p> <p>Insert the code to use the semantic ranking function!</p> Python<pre><code>StructuredTool.from_function(coroutine=cf.find_sow_chunks_with_semantic_ranking),\n</code></pre> </li> <li> <p>Your new <code>tools</code> array should look like this:</p> Python<pre><code># Define tools for the agent to retrieve data from the database\ntools = [\n    # Hybrid search functions\n    StructuredTool.from_function(coroutine=cf.find_invoice_line_items),\n    StructuredTool.from_function(coroutine=cf.find_invoice_validation_results),\n    StructuredTool.from_function(coroutine=cf.find_milestone_deliverables),\n    StructuredTool.from_function(coroutine=cf.find_sow_chunks_with_semantic_ranking),\n    StructuredTool.from_function(coroutine=cf.find_sow_validation_results),\n    # Get invoice data functions\n    StructuredTool.from_function(coroutine=cf.get_invoice_id),\n    StructuredTool.from_function(coroutine=cf.get_invoice_line_items),\n    StructuredTool.from_function(coroutine=cf.get_invoice_validation_results),\n    StructuredTool.from_function(coroutine=cf.get_invoices),\n    # Get SOW data functions\n    StructuredTool.from_function(coroutine=cf.get_sow_chunks),\n    StructuredTool.from_function(coroutine=cf.get_sow_id),\n    StructuredTool.from_function(coroutine=cf.get_sow_milestones),\n    StructuredTool.from_function(coroutine=cf.get_milestone_deliverables),\n    StructuredTool.from_function(coroutine=cf.get_sow_validation_results),\n    StructuredTool.from_function(coroutine=cf.get_sows),\n    # Get vendor data functions\n    StructuredTool.from_function(coroutine=cf.get_vendors)\n]\n</code></pre> </li> <li> <p>Save the <code>completions.py</code> file.</p> </li> </ol>"},{"location":"workshop/docs/06-Video-Processing/03/#test-with-vs-code","title":"Test with VS Code","text":"<p>As you have done previously, you will test your updates using Visual Studio Code.</p>"},{"location":"workshop/docs/06-Video-Processing/03/#start-the-api","title":"Start the API","text":"<p>Follow the steps below to start a debug session for the API in VS Code.</p> <ol> <li> <p>In Visual Studio Code Run and Debug panel, select the API Debugger option for your OS from the debug configurations dropdown list.</p> <p></p> </li> <li> <p>Select the Start Debugging button (or press F5 on your keyboard).</p> <p></p> </li> <li> <p>Wait for the API application to start completely, indicated by an <code>Application startup complete.</code> message in the terminal output.</p> <p></p> </li> </ol>"},{"location":"workshop/docs/06-Video-Processing/03/#start-the-portal","title":"Start the Portal","text":"<p>With the API running, you can start a second debug session in VS Code for the Portal project.</p> <ol> <li> <p>Return to the Run and Debug panel in Visual Studio Code and select the Portal Debugger option from the debug configurations dropdown list.</p> <p></p> </li> <li> <p>Select the Start Debugging button (or press F5 on your keyboard).</p> <p></p> </li> <li> <p>This should launch the Woodgrove Bank Contract Management Portal in a new browser window (http://localhost:3000/).</p> </li> <li> <p>On the Dashboard page, enter the following message into the chat and send it:</p> <p>Paste the following prompt into the copilot chat box!</p> <pre><code>Show me SOWs pertaining to cost management and optimization.\n</code></pre> </li> <li> <p>Observe the results provided using your UDF and semantic ranking.</p> </li> </ol> <p>Congratulations! You just learned how to leverage the semantic ranking capabilities in Azure Database for PostgreSQL!</p>"},{"location":"workshop/docs/06-Video-Processing/04/","title":"6.4 Review GraphRAG","text":"<p>GraphRAG (Graph Retrieval-Augmented Generation) is an innovative technique developed by Microsoft Research. It significantly enhances the accuracy and relevance of responses generated by Retrieval-Augmented Generation (RAG) systems. Graph refers to a data structure representing entities and their relationships. This structure is often visualized as nodes (vertices) and edges, where nodes represent entities and edges represent the relationships or connections between these entities.</p> <p>In the context of Azure Database for PostgreSQL, GraphRAG leverages the structure of knowledge graphs extracted from source data to provide better context and improve the quality of responses from language models. By integrating GraphRAG with PostgreSQL, you can enhance the information retrieval pipeline, making it more accurate and context-aware. This is particularly useful for applications where the accuracy and relevance of information are critical.</p> Want a more extensive example of GraphRAG with Azure Database for PostgreSQL? <p>This solution accelerator presents a simplified GraphRAG implementation to show you how you can improve the accuracy of RAG by leveraging a graph database. For a more comprehensive example that combines the results of semantic ranking and GraphRAG, please refer to the Introducting the GraphRAG Solution for Azure Database for PostgreSQL blob post and associated GraphRAG Solution Accelerator for Azure Database for PostgreSQL.</p>"},{"location":"workshop/docs/06-Video-Processing/04/#graphrag-with-age","title":"GraphRAG with AGE","text":"<p>Apache Graph Extension (AGE) is a PostgreSQL extension developed under the Apache Incubator project. It offers a significant advancement that provides graph processing capabilities within the PostgreSQL ecosystem, enabling users to store efficiently and query graph data. This new extension brings a powerful toolset for developers looking to leverage a graph database with the robust enterprise features of Azure Database for PostgreSQL.</p> <p></p> <p>With AGE, you can manage and analyze complex relationships within your data, uncovering insights that traditional relational databases and even semantic search might miss.</p>"},{"location":"workshop/docs/06-Video-Processing/04/#key-features","title":"Key Features","text":"<ul> <li>Graph and relational data integration: AGE allows seamless integration of graph data with existing relational data in PostgreSQL. This hybrid approach lets you benefit from both graph and relational models simultaneously.</li> <li>openCypher query language: AGE incorporates openCypher, a powerful and user-friendly query language designed explicitly for graph databases. This feature simplifies the process of writing and executing graph queries.</li> <li>High performance: AGE is optimized for performance, ensuring efficient storage and retrieval of graph data thanks to support for indexing graph properties using GIN indices.</li> <li>Scalability: Built on PostgreSQL's proven architecture, AGE inherits its scalability and reliability, allowing it to handle growing datasets and increasing workloads.</li> </ul>"},{"location":"workshop/docs/06-Video-Processing/04/#benefits","title":"Benefits","text":"<p>The integration of AGE in Azure Database for PostgreSQL brings numerous practical benefits to developers and businesses looking to leverage graph processing capabilities:</p> <ul> <li>Simplified data management: AGE's ability to integrate graph and relational data simplifies data management tasks, reducing the need for separate graph database solutions.</li> <li>Enhanced data analysis: With AGE, you can perform complex graph analyses directly within your PostgreSQL database, gaining deeper insights into relationships and patterns in your data.</li> <li>Cost efficiency: By utilizing AGE within Azure Database for PostgreSQL, you can consolidate your database infrastructure, lowering overall costs and reducing the complexity of your data architecture.</li> <li>Security and compliance: AGE leverages Azure's industry-leading security and compliance features to ensure your graph data is protected and meets regulatory requirements, providing peace of mind.</li> </ul>"},{"location":"workshop/docs/06-Video-Processing/05/","title":"6.5 Enable AGE","text":"<p>The Apache AGE (<code>age</code>) extension enhances PostgreSQL by allowing it to be used as a graph database, providing a comprehensive solution for analyzing interconnected data. With <code>age</code>, you can define and query complex data relationships using graph structures.</p> <p>To simplify the extraction of data from your PostgreSQL database into CSV files hosted in Azure Blob Storage, you will also use the Azure Storage extension (<code>azure_storage</code>). This extension allows you to connect directly to an Azure Storage account from your PostgreSQL database, copy data out of the database, and write it into files in blob storage. From your storage account, the CSV files will be used as the data source for your graph database.</p>"},{"location":"workshop/docs/06-Video-Processing/05/#allowlist-the-extensions","title":"Allowlist the extensions","text":"<p>Before using <code>age</code> and <code>azure_storage</code> extensions, add them to the PostgreSQL server's allowlist, configure them as shared preloaded libraries, and install them in your database.</p> <p>Select the tab for the method you want to use for allowlisting the extensions and follow the instructions provided.</p> Azure CLIAzure portal <ol> <li> <p>Open a new integrated terminal window in VS Code and execute the following Azure CLI command at the prompt.</p> <p>Ensure you replace the tokens in the command below with the appropriate values from your Azure environment.</p> <ul> <li>[YOUR_RESOURCE_GROUP]: The name of the resource group hosting your Azure Database for PostgreSQL flexible server.</li> <li>[YOUR_POSTGRESQL_SERVER]: The name of your Azure Database for PostgreSQL server.</li> <li>[YOUR_SUBSCRIPTION_ID]: Your Azure subscription ID.</li> </ul> <p>Execute the following Azure CLI command!</p> Bash<pre><code>az postgres flexible-server parameter set --resource-group [YOUR_RESOURCE_GROUP] --server-name [YOUR_POSTGRESQL_SERVER] --subscription [YOUR_SUBSCRIPTION_ID] --name azure.extensions --value azure_ai,pg_diskann,vector,age,azure_storage\n</code></pre> </li> </ol> <ol> <li> <p>Navigate to your Azure Database for PostgreSQL flexible server instance in the Azure portal.</p> </li> <li> <p>From the left-hand resource menu:</p> <ol> <li>Expand the Settings section and select Server parameters.</li> <li>Enter \"azure.extensions\" into the search filter.</li> <li>Expand the VALUE dropdown list.</li> <li>Select the AGE and AZURE_STORAGE extensions by checking the box for each in the VALUE dropdown list.</li> </ol> <p></p> </li> </ol>"},{"location":"workshop/docs/06-Video-Processing/05/#load-extensions-on-server-start","title":"Load extensions on server start","text":"<p>Some Postgres libraries need to perform certain operations that can only take place at postmaster start, such as allocating shared memory, reserving lightweight locks, or starting background workers. <code>AGE</code> relies on shared memory for its operations, so it must be loaded at server start. The <code>shared_preload_libraries</code> parameter in PostgreSQL is used to specify libraries that should be loaded at server startup, enabling additional functionalities or extensions before any connections are made.</p> <p>Select the tab for the method you want to use to update the <code>shared_preload_libraries</code> parameter and follow the instructions provided.</p> Azure CLIAzure portal <ol> <li> <p>In the VS Code integrated terminal window, execute the following Azure CLI command at the prompt.</p> <p>Ensure you replace the tokens in the command below with the appropriate values from your Azure environment.</p> <ul> <li>[YOUR_RESOURCE_GROUP]: The name of the resource group hosting your Azure Database for PostgreSQL flexible server.</li> <li>[YOUR_POSTGRESQL_SERVER]: The name of your Azure Database for PostgreSQL server.</li> <li>[YOUR_SUBSCRIPTION_ID]: Your Azure subscription ID.</li> </ul> <p>Execute the following Azure CLI command!</p> Bash<pre><code>az postgres flexible-server parameter set --resource-group [YOUR_RESOURCE_GROUP] --server-name [YOUR_POSTGRESQL_SERVER] --subscription [YOUR_SUBSCRIPTION_ID] --name shared_preload_libraries --value age,azure_storage,pg_cron,pg_stat_statements\n</code></pre> <p><code>pg_cron</code> and <code>pg_stat_statements</code> are set by default, so they are included in the above command to avoid removing them from the <code>shared_preload_libraries</code> parameter.</p> </li> <li> <p>The above command sets the parameter, but your PostgreSQL flexible server requires a restart for the setting to take effect. Run the following command to restart your server:</p> <p>Ensure you replace the tokens in the command below with the appropriate values from your Azure environment.</p> <ul> <li>[YOUR_RESOURCE_GROUP]: The name of the resource group hosting your Azure Database for PostgreSQL flexible server.</li> <li>[YOUR_POSTGRESQL_SERVER]: The name of your Azure Database for PostgreSQL server.</li> <li>[YOUR_SUBSCRIPTION_ID]: Your Azure subscription ID.</li> </ul> <p>Execute the following Azure CLI command!</p> Bash<pre><code>az postgres flexible-server restart --resource-group [YOUR_RESOURCE_GROUP] --name [YOUR_POSTGRESQL_SERVER] --subscription [YOUR_SUBSCRIPTION_ID]\n</code></pre> </li> </ol> <ol> <li> <p>On the Server parameters page of your Azure Database for PostgreSQL flexible server instance in the Azure portal:</p> <ol> <li>Enter \"shared_preload\" into the search filter.</li> <li>Expand the VALUE dropdown list.</li> <li>Select the AGE and AZURE_STORAGE extensions by checking the box for each in the VALUE dropdown list.</li> <li>Select Save on the toolbar.</li> </ol> <p></p> </li> <li> <p>Select Save will trigger a restart of the PostgreSQL server and will take a few seconds to complete.</p> </li> <li> <p>In the Save server parameter dialog that appears, select Save and Restart.</p> <p></p> </li> </ol>"},{"location":"workshop/docs/06-Video-Processing/05/#install-extensions","title":"Install extensions","text":"<p>With the <code>AGE</code> and <code>AZURE_STORAGE</code> extensions added to the allowlist and loaded on your PostgreSQL server, you can install them in your database using the CREATE EXTENSION command.</p> <p>At this time, the AGE extension is in preview and will only be available for newly created Azure Database for PostgreSQL Flexible Server instances running at least PG13 up to PG16.</p> <p>You will use pgAdmin to install the extension by executing a SQL command against your database.</p> <ol> <li> <p>On your local machine, return to the open instance of pgAdmin (or open it if you closed it after the setup tasks) and ensure it is connected to your PostgreSQL database.</p> </li> <li> <p>In the pgAdmin Object Explorer, expand databases under your PostgreSQL server.</p> </li> <li> <p>Right-click the contracts database and select Query Tool from the context menu.</p> </li> <li> <p>Install the <code>age</code> and <code>azure_storage</code> extensions by running the following <code>CREATE EXTENSION</code> commands in the pgAdmin query window:</p> <p>Execute the following SQL commands in pgAdmin!</p> <pre><code>CREATE EXTENSION IF NOT EXISTS age;\nCREATE EXTENSION IF NOT EXISTS azure_storage;\n</code></pre> </li> </ol>"},{"location":"workshop/docs/06-Video-Processing/06/","title":"6.6 Export Graph Data","text":"<p>You must export data from your PostgreSQL database to populate your graph database. Using the Azure Storage (<code>azure_storage</code>) extension for Azure Database for PostgreSQL provides a streamlined method for copying data out of your PostgreSQL database into CSV files in Azure Blob Storage. In the context of <code>AGE</code> and loading data into a graph database, the <code>azure_storage</code> extension facilitates the extraction of relational data from your PostgreSQL database, enabling efficient transfer to Blob Storage. This process ensures that the data needed for constructing and querying your graph database is readily available and can be seamlessly integrated into your data workflows.</p>"},{"location":"workshop/docs/06-Video-Processing/06/#connect-your-database-to-azure-storage","title":"Connect your database to Azure Storage","text":"<p>You will use pgAdmin to configure the <code>azure_storage</code> extension's connection to your storage account by executing SQL commands against your database.</p> <p>Ensure you replace the token in the commands below with the appropriate values from your Azure environment.</p> <p>Each SQL statement you will execute below contains a <code>[YOUR_STORAGE_ACCOUNT_NAME]</code> token. Before running any of the queries, you must replace this token with the name of your Storage account resource, which you can copy from the Azure portal.</p> <ol> <li> <p>On your local machine, return to the open instance of pgAdmin (or open it if you have closed it) and ensure it is connected to your PostgreSQL database.</p> </li> <li> <p>In the pgAdmin Object Explorer, expand databases under your PostgreSQL server.</p> </li> <li> <p>Right-click the contracts database and select Query Tool from the context menu.</p> </li> <li> <p>Run the following command to use the <code>azure_storage.account_add()</code> function to define a connection between your storage account and your PostgreSQL database. Replace the account name and key tokens with values for your storage account.</p> <p>Execute the following SQL commands in pgAdmin!</p> SQL<pre><code>SELECT azure_storage.account_add('[YOUR_STORAGE_ACCOUNT_NAME]', '[YOUR_STORAGE_ACCOUNT_KEY]');\n</code></pre> Need help finding your storage account key? <p>To get your storage account's access key:</p> <ol> <li>Navigate to your storage account in the Azure portal.</li> <li>Select the Access keys menu under Security + networking in the navigation menu.</li> <li>Select Show next to the Key value under key1.</li> <li>Select the Copy to clipboard button that appears on the right-hand side of the Key box.</li> <li> <p>Paste the copied key as the <code>[YOUR_STORAGE_ACCOUNT_KEY]</code> value in the above SQL statement.</p> <p></p> </li> </ol> </li> </ol>"},{"location":"workshop/docs/06-Video-Processing/06/#export-data-to-blob-storage","title":"Export data to blob storage","text":"<p>As part of the data export process, you will use queries to reshape the source data into the format required to efficiently define nodes and edges in your graph database. Traditional relational databases organize data in tables, while graph databases use nodes and edges to represent entities and their relationships. Converting tabular data into nodes and edges aligns with the graph structure, making relationship analysis more efficient. This transformation enables natural modeling of real-world entities, optimizes query performance, and allows for complex relationship analysis, such as evaluating the connections between vendors, SOWs, and associated invoices. By reshaping your data, you can fully leverage the strengths of <code>AGE</code> and Azure Database for PostgreSQL for deeper insights and sophisticated analyses.</p> <p>You will define two nodes and one edge in your graph database. The nodes will contain data vendor, and SOW data. The edge will define the relationship between these.</p> <p>You will use pgAdmin to execute data export queries leveraging the <code>azure_storage</code> extension.</p> <ol> <li> <p>Return to the open Query Tool in pgAdmin.</p> </li> <li> <p>Run the following query using the <code>azure_storage.blob_put()</code> function to write all data from the <code>vendors</code> table into a CSV file named <code>vendors.csv</code> into your storage account's <code>graph</code> container. This data will define the <code>vendor</code> node in your graph database.</p> <p>Execute the following SQL commands in pgAdmin!</p> SQL<pre><code>-- Extract data for the vendors node\nSELECT azure_storage.blob_put(\n    '[YOUR_STORAGE_ACCOUNT_NAME]',\n    'graph',\n    'vendors.csv',\n    vendors,\n    'csv',\n    'none',\n    azure_storage.options_csv_put(header:=true)\n)\nFROM (\n    SELECT * FROM vendors\n) AS vendors;\n</code></pre> </li> <li> <p>Execute this query to extract <code>sow</code> node data from the <code>sows</code> table and write it into a CSV file named <code>sows.csv</code> into your storage account's <code>graph</code> container. The query excludes a few columns from the <code>sows</code> table, including the <code>embedding</code> column, as they are unnecessary in the graph database and can cause errors.</p> <p>Execute the following SQL commands in pgAdmin!</p> SQL<pre><code>-- Extract data for the SOWs node\nSELECT azure_storage.blob_put(\n    '[YOUR_STORAGE_ACCOUNT_NAME]',\n    'graph',\n    'sows.csv',\n    sows,\n    'csv',\n    'none',\n    azure_storage.options_csv_put(header:=true)\n)\nFROM (\n    SELECT id, number, vendor_id, start_date, end_date, budget FROM sows\n) AS sows;\n</code></pre> </li> <li> <p>Finally, run the following query to extract <code>has_invoices</code> edge data from the <code>invoices</code> table and write it into a CSV file named <code>has_invoices.csv</code> into the <code>graph</code> container in your storage account:</p> <p>Execute the following SQL commands in pgAdmin!</p> SQL<pre><code>-- Create the has_invoices edge\nSELECT azure_storage.blob_put(\n    '[YOUR_STORAGE_ACCOUNT_NAME]',\n    'graph',\n    'has_invoices.csv',\n    invoices,\n    'csv',\n    'none',\n    azure_storage.options_csv_put(header:=true)\n)\nFROM (\n    SELECT id, vendor_id as start_id, 'vendor' AS start_vertex_type, sow_id AS end_id, 'sow' AS end_vertex_type, number, amount, invoice_date, payment_status FROM invoices\n) AS invoices;\n</code></pre> <p>Edge definition details</p> <p>When using <code>AGE</code>, edges must contain details about the relationships between nodes. These are defined in the above query by specifying the <code>start_id</code>, <code>start_vertex_type</code>, <code>end_id</code>, and <code>end_vertex_type</code> columns. The '_id' columns are mapped to the <code>vendor_id</code> and <code>sow_id</code>, respectively, and the start and end vertex types are strings specifying the node type associated with the ID.</p> </li> </ol>"},{"location":"workshop/docs/06-Video-Processing/06/#verify-csv-files","title":"Verify CSV files","text":"<p>You can use the <code>azure_storage</code> extension to verify the CSV files were successfully written into the <code>graph</code> container in your storage account.</p> <ol> <li> <p>Execute the following query in the Query Tool in pgAdmin. Ensure you replace the <code>[YOUR_STORAGE_ACCOUNT_NAME]</code> token with the name of your storage account.</p> <p>Execute the following SQL commands in pgAdmin!</p> SQL<pre><code>-- Verify the CSV files were written into blob storage\nSELECT azure_storage.blob_list('[YOUR_STORAGE_ACCOUNT_NAME]', 'graph');\n</code></pre> </li> <li> <p>You should see a list of blobs in the Data output panel in pgAdmin that includes the three CSV files you exported above.</p> </li> </ol> <p>Congratulations! You have successfully exported data to create your graph database!</p>"},{"location":"workshop/docs/06-Video-Processing/07/","title":"6.7 Create Graph Database","text":"<p>Hosting graph databases in Azure Database for PostgreSQL using the Apache AGE extension offers a powerful way to analyze relationships within your data. AGE combines graph and relational data seamlessly, leveraging the openCypher query language for efficient graph processing. This integration brings PostgreSQL's scalability, performance, and security to the table while enabling advanced data analysis and management. When incorporated into a copilot, this setup empowers you to evaluate vendor performance of SOW deliverables through invoice validation, ensuring your data-driven decisions are robust and insightful.</p> <p>Once you have created your graph database, it will be incorporated into the Woodgrove Bank API as an extra tool available to the copilot, allowing you to improve RAG accuracy when retrieving relevant data. By using another function call, the copilot will be able to execute cypher queries against the graph database hosted in Azure Database for PostgreSQL using AGE while maintaining the existing capabilities of querying data in Postgres.</p>"},{"location":"workshop/docs/06-Video-Processing/07/#woodgrove-graph","title":"Woodgrove Graph","text":"<p>Graph databases allow you to model complex relationships between data using nodes and edges, making it easier to represent and query these relationships. You will build a simple graph using the data you extracted from the Woodgrove Bank database. You will define <code>vendors</code> and <code>sows</code> as nodes in your graph and use <code>invoices</code> as the edge between them. Edges in graph databases define a one-to-one relationship between two entities, defined as nodes. The diagram below provides a high-level representation of the graph database.</p> <p></p> <p>Edges must include a mapping of related entities through ID mapping and can also include properties, which allow the relationship to be filtered at query time.</p>"},{"location":"workshop/docs/06-Video-Processing/07/#create-graph-database-with-agefreighter","title":"Create Graph Database with AGEFreighter","text":"<p>AGEFreighter is a Python library designed to simplify the process of creating and loading graph databases in Azure Database for PostgreSQL, allowing data to be ingested from various sources, including CSV file, Azure Cosmos DB, Neo4j, and Azure Database for PostgreSQL.</p>"},{"location":"workshop/docs/06-Video-Processing/07/#review-code","title":"Review code","text":"<p>The solution accelerator includes the <code>graph_loader.py</code> file in the <code>src/api/app</code> folder, which allows you to quickly run a Python script to create a graph database and populate it with data from CSV files.</p> <p>The graph loader is implemented in the <code>src/api/app/graph_loader.py</code> file. Open it now in Visual Studio Code and explore the code in sections. You can also expand the section below to see the code inline and review explanations for the code.</p> Graph Loader code src/api/app/graph_loader.py<pre><code>import os\nfrom agefreighter import Factory\nfrom dotenv import load_dotenv\nfrom azure.identity import DefaultAzureCredential\nfrom azure.storage.blob.aio import BlobServiceClient\n\nasync def main():\n    \"\"\"Load data into Azure Database for PostgreSQL Graph Database.\"\"\"\n    # Load environment variables from the .env file\n    load_dotenv()\n    print(\"Loading environment variables...\")\n\n    # Get environment variables\n    server = os.getenv(\"POSTGRESQL_SERVER_NAME\")\n    database = 'contracts'\n    username = os.getenv(\"ENTRA_ID_USERNAME\")\n    account_name = os.getenv(\"STORAGE_ACCOUNT_NAME\")\n\n    # Create an AGEFreigher factory instance to load data from multiple CSV files.\n    print(\"Creating AGEFreighter factory instance...\")\n    factory = Factory.create_instance('MultiCSVFreighter')\n\n    # Connect to the PostgreSQL database.\n    print(\"Connecting to the PostgreSQL database...\")\n    await factory.connect(\n        dsn=get_connection_string(server, database, username),\n        max_connections=64\n    )\n\n    local_data_dir = 'graph_data/'\n\n    # Download CSV data files from Azure Blob Storage\n    print(\"Downloading CSV files from Azure Blob Storage...\")\n    await download_csvs(account_name, local_data_dir)\n\n    # Load data into the graph database\n    print(\"Loading data into the graph database...\")\n    await factory.load(\n        graph_name='vendor_graph',\n        vertex_csv_paths = [\n            f'{local_data_dir}vendors.csv',\n            f'{local_data_dir}sows.csv'\n        ],\n        vertex_labels = ['vendor', 'sow'],\n        edge_csv_paths = [f'{local_data_dir}has_invoices.csv'],\n        edge_types = [\"has_invoices\"],\n        use_copy=True,\n        drop_graph=True,\n        create_graph=True,\n        progress=True\n    )\n\n    print(\"Graph data loaded successfully!\")\n\ndef get_connection_string(server_name: str, database_name: str, username: str):\n    \"\"\"Get the connection string for the PostgreSQL database.\"\"\"\n\n    # Get a token for the Azure Database for PostgreSQL server\n    credential = DefaultAzureCredential()\n    token = credential.get_token(\"https://ossrdbms-aad.database.windows.net\")\n    port = 5432\n\n    conn_str = \"host={} port={} dbname={} user={} password={}\".format(\n        server_name, port, database_name, username, token.token\n    )\n    return conn_str\n\nasync def download_csvs(account_name:str, local_data_directory: str):\n    \"\"\"Download CSV files from Azure Blob Storage.\"\"\"\n\n    # Create connection to the blob storage account\n    account_blob_endpoint = f\"https://{account_name}.blob.core.windows.net/\"\n    # Connect to the blob service client using Entra ID authentication\n    client = BlobServiceClient(account_url=account_blob_endpoint, credential=DefaultAzureCredential())\n\n    # List the blobs in the graph container with a CSV extension\n    async with client:\n        async for blob in client.get_container_client('graph').list_blobs():\n            if blob.name.endswith('.csv'):\n                # Download the CSV file to a local directory\n                await download_csv(client, blob.name, local_data_directory)\n\nasync def download_csv(client: BlobServiceClient, blob_path: str, local_data_dir: str):\n    \"\"\"Download a CSV file from Azure Blob Storage.\"\"\"\n    # Get the blob\n    blob_client = client.get_blob_client(container='graph', blob=blob_path)\n\n    async with blob_client:\n        # Download the CSV file\n        if await blob_client.exists():\n            # create a local directory if it does not exist\n            if not os.path.exists(local_data_dir):\n                os.makedirs(local_data_dir)\n\n            with open(f'{local_data_dir}{blob_path.split('/')[-1]}', 'wb') as file:\n                stream = await blob_client.download_blob()\n                result = await stream.readall()\n                # Save the CSV file to a local directory\n                file.write(result)\n\nif __name__ == \"__main__\":\n    import asyncio\n    import sys\n\n    if sys.platform == \"win32\":\n        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n\n    asyncio.run(main())\n</code></pre> <ol> <li> <p>Import libraries (lines 1-5): Required classes and functions are imported from various libraries.</p> </li> <li> <p>Define <code>main</code> function (line 7): The <code>main</code> function is the entry point of the graph loader. This function serves as the orchestrator for executing the code within the file.</p> </li> <li> <p>Load environment variables (lines 10-17): The <code>load_dotenv()</code> method from the <code>dotenv</code> Python library allows variables from the <code>.env</code> file within the API project to be loaded as environment variables in the project. Note the names of the variables here, as you will be adding those to your <code>.env</code> file in the next step.</p> </li> <li> <p>Create an AGEFreighter factory (line 21): The entry point for the <code>agefreighter</code> package in the <code>factory</code> class. This method creates an instance of the library using the type specified. You are loading your graph using multiple CSV files, so the <code>MultiCSVFreighter</code> class type is indicated.</p> </li> <li> <p>Connect to PostgreSQL (lines 25-28): The <code>connect</code> method of the <code>factory</code> opens a connection to your Azure Database for PostgreSQL flexible server.</p> <ol> <li>The <code>get_connection_string()</code> function uses values from your environment variables to define the connection string the <code>factory</code> will use to connect to your database.</li> <li>The <code>get_connection_string()</code> function is defined on lines 55-66.</li> </ol> </li> <li> <p>Download CSV files from blob storage (line 34): The CSV files you created in the previous task are downloaded from blob storage and written into a local folder, where the graph loader can easily access them.</p> <ol> <li>The <code>download_csvs()</code> function is defined on lines 68-81. This function creates a <code>BlobServiceClient</code> instance, which is used to retrieve the blobs in your storage account's <code>graph</code> container.</li> <li>For each blob with the extension of <code>.csv</code>, the <code>download_csv</code> function defined on lines 83-99 is used to retrieve the blob's contents and write them into a local file.</li> </ol> </li> <li> <p>Create and load the graph database (lines 38-51): The <code>load</code> method of the <code>factory</code> does the following:</p> <ol> <li>Creates a graph named <code>vendor_graph</code>.</li> <li>Defines vertex (node) data and labels and inserts the nodes into the graph.</li> <li>Specifies edges using labels and inserts them to establish the relationships between the nodes.</li> </ol> </li> <li> <p>Define the main guard (lines 101-108): The main guard defines how the <code>graph_loader</code> is executed when called directly. This code block lets you run the script from a command line or VS Code debugging session.</p> </li> </ol>"},{"location":"workshop/docs/06-Video-Processing/07/#update-env-file","title":"Update <code>.env</code> file","text":"<p>The <code>graph_loader.py</code> file references environment variables to retrieve information about your Azure Database for PostgreSQL flexible server instance, your Entra ID username, and the storage account from which to pull CSV files. Before executing the graph loader script, you must update your project's <code>.env</code> file with these values. The <code>.env</code> file can be found in the <code>src\\api\\app</code> folder of the repo.</p> <ol> <li> <p>In VS Code, navigate to the <code>src\\api\\app</code> folder in the Explorer panel.</p> </li> <li> <p>Open the <code>.env</code> file and add the following lines:</p> <p>Update your <code>.env</code> file!</p> <pre><code>ENTRA_ID_USERNAME=\"{YOUR_ENTRA_ID_USERNAME}\"\nPOSTGRESQL_SERVER_NAME=\"{YOUR_POSTGRESQL_SERVER_NAME}\"\nSTORAGE_ACCOUNT_NAME=\"{YOUR_STORAGE_ACCOUNT_NAME}\"\n</code></pre> Follow these steps to retrieve the necessary values <ol> <li> <p>Replace the <code>{YOUR_ENTRA_ID_USERNAME}</code> token in the <code>ENTRA_ID_USERNAME</code> variable's value with your Microsoft Entra ID, which should be the email address of the account you are using for this solution accelerator.</p> </li> <li> <p>Replace the <code>{YOUR_POSTGRESQL_SERVER_NAME}</code> token with the name of your PostgreSQL server. To get your server name:</p> </li> <li> <p>Navigate to your Azure Database for PostgreSQL flexible server resource in the Azure portal.</p> </li> <li> <p>In the Essentials panel of the PostgreSQL flexible server's Overview page, copy the Server name value and paste it into your <code>.env</code> file as the <code>POSTGRESQL_SERVER_NAME</code> value.</p> <p></p> </li> <li> <p>Replace the <code>{YOUR_STORAGE_ACCOUNT_NAME}</code> token with the name of your storage account. To retrieve your storage account name:</p> </li> <li> <p>In the Azure portal, navigate to the Storage account resource in your resource group.</p> </li> <li> <p>On the Storage account page, copy the storage account name and paste it into your <code>.env</code> file as the <code>STORAGE_ACCOUNT_NAME</code> value.</p> <p></p> </li> </ol> </li> <li> <p>Save the <code>.env</code> file.</p> </li> </ol>"},{"location":"workshop/docs/06-Video-Processing/07/#load-the-graph-database","title":"Load the graph database","text":"<p>You will use a VS Code debugging session to locally execute the <code>graph_loader.py</code> script. Follow the steps below to start a Graph Loader debug session in VS Code.</p> <ol> <li> <p>In Visual Studio Code Run and Debug panel, select the Graph Loader option from the debug configurations dropdown list.</p> <p></p> </li> <li> <p>Select the Start Debugging button (or press F5 on your keyboard).</p> <p></p> </li> <li> <p>Wait for the graph loader to finish running, indicated by the <code>Graph data loaded successfully!</code> message in the terminal output.</p> <p></p> </li> </ol>"},{"location":"workshop/docs/06-Video-Processing/07/#verify-data-load","title":"Verify data load","text":"<p>You will execute openCypher queries using pgAdmin to verify the data load and explore relationships in your graph database.</p> <ol> <li> <p>Return to pgAdmin and ensure it is connected to your PostgreSQL database.</p> </li> <li> <p>In the pgAdmin Object Explorer, expand databases under your PostgreSQL server.</p> </li> <li> <p>Right-click the contracts database and select Query Tool from the context menu.</p> </li> <li> <p>Before you can run cypher queries, you must set the <code>ag_catalog</code> schema in your path:</p> <p>Execute the following SQL commands in pgAdmin!</p> SQL<pre><code>SET search_path = ag_catalog, \"$user\", public;\n</code></pre> </li> <li> <p>Now, run the following cypher query to view vendors with open invoices, the details of those invoices, and verify your graph database was loaded correctly:</p> <p>Execute the following SQL commands in pgAdmin!</p> SQL<pre><code>-- View vendors and SOWs, along with invoice details from edge properties\nSELECT * FROM ag_catalog.cypher('vendor_graph', $$\nMATCH (v:vendor)-[rel:has_invoices]-&gt;(s:sow)\nRETURN v.id AS vendor_id, v.name AS vendor_name, s.id AS sow_id, s.number AS sow_number, rel.payment_status AS payment_status, rel.amount AS invoice_amount\n$$) as graph_query(vendor_id BIGINT, vendor_name TEXT, sow_id BIGINT, sow_number TEXT, payment_status TEXT, invoice_amount FLOAT);\n</code></pre> </li> </ol> <p>Congratulations! You have successfully loaded your graph database with data from PostgreSQL.</p>"},{"location":"workshop/docs/06-Video-Processing/08/","title":"6.8 Update Copilot With GraphRAG","text":"<p>The next step is to update your API to use GraphRAG for data retrieval. You will update how your copilot finds and retrieves unpaid invoices for this.</p>"},{"location":"workshop/docs/06-Video-Processing/08/#review-the-function","title":"Review the function","text":"<p>Following the function calling pattern used by your LangChain agent to retrieve data from the database, you will use a Python function to execute openCypher queries against your graph database from your copilot. Within the <code>src/api/app/functions/chat_functions.py</code> file, the <code>get_unpaid_invoices_for_vendor</code> function has been provided for executing cypher queries. Open it now in Visual Studio Code and explore the code with the function. You can also expand the section below to see the code inline.</p> GraphRAG code src/api/app/functions/chat_functions.py<pre><code>async def get_unpaid_invoices_for_vendor(self, vendor_id: int):\n    \"\"\"\n    Retrieves a list of unpaid invoices for a specific vendor using a graph query.\n    \"\"\"\n    # Define the graph query\n    graph_query = f\"\"\"SELECT * FROM ag_catalog.cypher('vendor_graph', $$\n    MATCH (v:vendor {{id: '{vendor_id}'}})-[rel:has_invoices]-&gt;(s:sow)\n    WHERE rel.payment_status &lt;&gt; 'Paid'\n    RETURN v.id AS vendor_id, v.name AS vendor_name, s.id AS sow_id, s.number AS sow_number, rel.id AS invoice_id, rel.number AS invoice_number, rel.payment_status AS payment_status\n    $$) as (vendor_id BIGINT, vendor_name TEXT, sow_id BIGINT, sow_number TEXT, invoice_id BIGINT, invoice_number TEXT, payment_status TEXT);\n    \"\"\"\n    rows = await self.__execute_graph_query(graph_query)\n    return [dict(row) for row in rows]\n</code></pre> <ol> <li> <p>Define grapy query (line 6): Creates the cypher query that will be used to look up unpaid invoices for the specified <code>vendor_id</code></p> </li> <li> <p>Execute cypher query (lines 12): The cyper query is sent to the database for execution, using the <code>__execute_graph_query()</code> function.</p> </li> <li> <p>The <code>__execute_graph_query()</code> function, starting on line 25 on the <code>chat_functions.py</code> file, runs the query against the <code>ag_catalog</code> schema, which contains the graph database. To enable this, it also includes a <code>SET</code> query prior to running the graph query to add <code>ag_catalog</code> to the <code>search_path</code> in the connection.</p> </li> <li> <p>Return the results (lines 13): The query results are extracted and returned to the LLM.</p> </li> </ol>"},{"location":"workshop/docs/06-Video-Processing/08/#implement-graphrag","title":"Implement GraphRAG","text":"<p>To implement GraphRAG functionality in your copilot, you must include the <code>get_unpaid_invoices_for_vendor</code> function in your LangChain agent's <code>tools</code> collection. You will add this function to the list of available tools to your agent.</p> <ol> <li> <p>In the VS Code Explorer, navigate to the <code>src/api/app/routers</code> folder and open the <code>completions.py</code> file.</p> </li> <li> <p>Within the <code>tools</code> array, locate the following line (line 75):</p> Python<pre><code>StructuredTool.from_function(coroutine=cf.get_invoices),\n</code></pre> </li> <li> <p>Insert the following code on the line just below that:</p> <p>Insert the following Python code to add the GraphRAG function!</p> Python<pre><code>StructuredTool.from_function(coroutine=cf.get_unpaid_invoices_for_vendor),\n</code></pre> </li> <li> <p>Your new <code>tools</code> array should look like this:</p> Python<pre><code># Define tools for the agent to retrieve data from the database\ntools = [\n    # Hybrid search functions\n    StructuredTool.from_function(coroutine=cf.find_invoice_line_items),\n    StructuredTool.from_function(coroutine=cf.find_invoice_validation_results),\n    StructuredTool.from_function(coroutine=cf.find_milestone_deliverables),\n    StructuredTool.from_function(coroutine=cf.find_sow_chunks_with_semantic_ranking),\n    StructuredTool.from_function(coroutine=cf.find_sow_validation_results),\n    # Get invoice data functions\n    StructuredTool.from_function(coroutine=cf.get_invoice_id),\n    StructuredTool.from_function(coroutine=cf.get_invoice_line_items),\n    StructuredTool.from_function(coroutine=cf.get_invoice_validation_results),\n    StructuredTool.from_function(coroutine=cf.get_invoices),\n    StructuredTool.from_function(coroutine=cf.get_unpaid_invoices_for_vendor),\n    # Get SOW data functions\n    StructuredTool.from_function(coroutine=cf.get_sow_chunks),\n    StructuredTool.from_function(coroutine=cf.get_sow_id),\n    StructuredTool.from_function(coroutine=cf.get_sow_milestones),\n    StructuredTool.from_function(coroutine=cf.get_milestone_deliverables),\n    StructuredTool.from_function(coroutine=cf.get_sow_validation_results),\n    StructuredTool.from_function(coroutine=cf.get_sows),\n    # Get vendor data functions\n    StructuredTool.from_function(coroutine=cf.get_vendors)\n]\n</code></pre> </li> <li> <p>Save the <code>completions.py</code> file.</p> </li> </ol>"},{"location":"workshop/docs/06-Video-Processing/08/#test-with-vs-code","title":"Test with VS Code","text":"<p>As you have done previously, you will test your updates using Visual Studio Code.</p>"},{"location":"workshop/docs/06-Video-Processing/08/#start-the-api","title":"Start the API","text":"<p>Follow the steps below to start a debug session for the API in VS Code.</p> <ol> <li> <p>In Visual Studio Code Run and Debug panel, select the API Debugger option for your OS from the debug configurations dropdown list.</p> <p></p> </li> <li> <p>Select the Start Debugging button (or press F5 on your keyboard).</p> <p></p> </li> <li> <p>Wait for the API application to start completely, indicated by an <code>Application startup complete.</code> message in the terminal output.</p> <p></p> </li> </ol>"},{"location":"workshop/docs/06-Video-Processing/08/#start-the-portal","title":"Start the Portal","text":"<p>With the API running, you can start a second debug session in VS Code for the Portal project.</p> <ol> <li> <p>Return to the Run and Debug panel in Visual Studio Code and select the Portal Debugger option from the debug configurations dropdown list.</p> <p></p> </li> <li> <p>Select the Start Debugging button (or press F5 on your keyboard).</p> <p></p> </li> <li> <p>This should launch the Woodgrove Bank Contract Management Portal in a new browser window (http://localhost:3000/).</p> </li> <li> <p>In the copilot chat on the Dashboard page, enter the following message and send it:</p> <p>Paste the following prompt into the copilot chat box!</p> <pre><code>Tell me about the accuracy of unpaid invoices from Adatum.\n</code></pre> </li> <li> <p>Observe the results provided using GraphRAG.</p> <p>GraphRAG improves accuracy</p> <p>Add a breakpoint in the <code>get_unpaid_invoices_for_vendor</code> function in the <code>chat_functions.py</code> file. The breakpoint will allow you to see the graph query executing and enable you to step through the remaining function calls to observe that the invoice validation results are only retrieved for the unpaid invoices. This precision reduces the data returned from the database and allows the RAG pattern to only receive the data it needs to generate a response.</p> </li> </ol> <p>Congratulations! You just learned how to leverage the GraphRAG capabilities of Azure Database for PostgreSQL and AGE!</p>"},{"location":"workshop/docs/07-Evaluation/","title":"Application Evaluation","text":"Using your own data? <p>To effectively integrate your own data into the solution accelerator, it's essential to adapt the existing components to align with your specific data structures and relationships.</p> <p>1. Customize the Semantic Ranker The semantic ranker enhances search relevance by reordering search results based on contextual understanding. To tailor it to your data:</p> <ul> <li>Understand the Existing Semantic Ranker: Review the current implementation to comprehend how it processes and ranks data.</li> <li>Adapt Ranking Functions: Modify the ranking algorithms to consider the unique attributes and relationships within your data. This may involve adjusting weightings or incorporating additional data fields to improve relevance.</li> <li>Test and Validate: After modifications, rigorously test the ranker's performance to ensure it accurately reflects the importance and context of your data elements.</li> </ul> <p>2. Modify GraphRAG Function Calls Graph Retrieval-Augmented Generation (GraphRAG) combines knowledge graphs with AI models to provide contextually rich responses. To align GraphRAG with your data:</p> <ul> <li>Analyze Your Data Structure: Identify the nodes (entities) and edges (relationships) that are pertinent to your domain.</li> <li>Adjust Function Calls: Modify existing GraphRAG function calls to navigate your specific graph schema effectively. This includes updating queries to traverse the correct nodes and edges that mirror your data's relationships.</li> <li>Integrate with AI Models: Ensure that the data retrieved via GraphRAG is appropriately fed into your AI models to enhance response generation with accurate context.</li> </ul> <p>3. Revise Data Export Procedures Exporting data accurately is crucial for maintaining the integrity of your knowledge graph. To adapt the export process:</p> <ul> <li>Map Data Relationships: Clearly define how your data entities relate to one another, establishing the nodes and edges that will form your knowledge graph.</li> <li>Update Export Scripts: Modify data export scripts to extract information from the appropriate tables and fields that correspond to your defined nodes and edges.</li> <li>Validate Exported Data: After exporting, verify that the data correctly represents the intended relationships and entities within your knowledge graph framework.</li> </ul> <p>As Generative AI (GenAI) becomes increasingly integral to modern enterprises, users' trust in these applications is paramount and heavily reliant on the accuracy of their responses. The productivity loss from incorrect answers and the consequent erosion of user trust are issues that cannot be overlooked. For many organizations, the decision to deploy GenAI apps hinges on their ability to elevate the accuracy of the app's responses to an acceptable level.</p> <p>In this section, you will use Semantic Ranking and GraphRAG to enhance the accuracy of responses from the Woodgrove Bank Contract Management copilot. Here's what you will accomplish:</p> <ul> <li> Review Semantic Ranking</li> <li> Use Semantic Ranking to rerank hybrid search results</li> <li> Review GraphRAG and the Apache AGE extension</li> <li> Implement the AGE extension to enable graph queries against your PostgreSQL database</li> </ul>"},{"location":"workshop/docs/07-Evaluation/#the-accuracy-problem","title":"The Accuracy Problem","text":"<p>Despite significant advancements, accuracy remains a challenge for GenAI. Retrieval Augmented Generation (RAG) helps by grounding responses in factual data. Still, as datasets expand or when documents become too similar, vector search techniques can falter, leading to a loss of user trust and diminished productivity. Enhancing accuracy requires optimizing the information retrieval pipeline through various techniques. General methods include chunking, larger embeddings, and hybrid search, while advanced, dataset-specific approaches like semantic ranking and GraphRAG are essential.</p>"},{"location":"workshop/docs/07-Evaluation/#enhance-genai-accuracy-with-advanced-techniques","title":"Enhance GenAI Accuracy with Advanced Techniques","text":"<p>Innovations like Semantic Ranking and GraphRAG are crucial to address the accuracy problem. These techniques enhance the RAG approach by improving how grounding data is gathered and integrated into AI model responses, thereby increasing precision and reliability. Optimizing information retrieval pipelines through advanced techniques ensures that GenAI applications deliver accurate, trustworthy, and insightful responses, thereby maintaining user trust and productivity.</p> <p>Select the tabs below to understand how Semantic Ranking and GraphRAG can improve RAG accuracy.</p> Semantic RankingGraphRAG <p>Semantic ranking is an advanced technique used in information retrieval systems to enhance the relevance of search results by understanding the context and meaning of queries and documents rather than relying solely on keyword matching. By leveraging natural language processing and machine learning algorithms, semantic ranking can analyze the relationships between words and concepts to provide more accurate and meaningful results. This approach allows search engines to better comprehend the intent behind user queries and deliver results that are contextually aligned with what users are looking for. The benefits of semantic ranking include improved accuracy in search results, enhanced user satisfaction, and more efficient retrieval of information, making it a powerful tool for modern search engines and recommendation systems.</p> <p>GraphRAG, developed by Microsoft Research, is an innovative approach to information retrieval and generation, combining the power of knowledge graphs with large language models (LLMs) to enhance the accuracy and relevance of generated content. GraphRAG allows LLMs to provide more contextually aware and insightful responses by integrating structured data from knowledge graphs into the retrieval process. This method enhances the model's ability to understand complex queries and synthesize information from various sources, leading to more accurate and informative outputs. GraphRAG's benefits include improved information retrieval, enhanced reasoning capabilities, and the ability to deliver precise answers even when dealing with intricate or multi-faceted questions.</p>"},{"location":"workshop/docs/07-Evaluation/01/","title":"6.1 Understand Semantic Ranking","text":"<p>Semantic ranking improves accuracy of the vector search by re-ranking results using a semantic ranker model, which brings more relevant items to the top of the ranked list. In the context of Azure Database for PostgreSQL, semantic ranking can enhance the accuracy and relevance of search results within the database, significantly improving the information retrieval pipelines in Generative AI (GenAI) applications. Unlike vector search, which primarily measures the similarity between two vector embeddings, semantic ranking delves deeper by analyzing the semantic relevance between text strings at the actual text level. This approach ensures that search results are more contextually aligned with user queries, improving information retrieval and higher user satisfaction.</p>"},{"location":"workshop/docs/07-Evaluation/01/#reranking-search-results-with-semantic-ranker-models","title":"Reranking Search Results with Semantic Ranker Models","text":"<p>Semantic ranker models compare two text strings: the search query and the text of one of the items being searched. The ranker produces a relevance score that indicates how well the text string matches the query, essentially determining if the text holds an answer to the query. Typically, the semantic ranker is a machine learning model, often a variant of the BERT language model fine-tuned for ranking tasks, but it can also be a large language model (LLM). These ranker models, also called cross-encoder models, take two text strings as input and output a relevance score, usually ranging from 0 to 1.</p>"},{"location":"workshop/docs/07-Evaluation/01/#semantic-ranker-model-inference-from-postgresql","title":"Semantic Ranker Model Inference from PostgreSQL","text":"<p>The <code>azure_ai</code> extension enables the invocation of machine learning models deployed on Azure Machine Learning online endpoints directly from within PostgreSQL.</p> <p></p> <p>Using the <code>azure_ml.invoke()</code> method, inference with a semantic ranker model can be performed through SQL queries. This allows for the reranking of vector search results based on the semantic relevance of the text, rather than relying solely on keyword matching.</p>"},{"location":"workshop/docs/07-Evaluation/02/","title":"6.2 Use Semantic Ranking in PostgreSQL","text":"<p>By leveraging the power of semantic re-ranking, you can improve the accuracy of data retrieval and help ensure the success of your Generative AI applications. In this task, you will create a user-defined function (UDF) in your database that utilizes model inferencing capabilities of the Azure AI extension. Specifically, the <code>azure_ml.invoke()</code> method of the extension will be called from within the UDF to seamlessly invoke a semantic ranking model directly within SQL queries.</p> Your Semantic Ranker model was deployed to Azure ML. <p>Depending on what semantic re-ranker setting you chose during azd up deployment, the deployment of this solution accelerator included executing a post-deployment script to download either the <code>mini</code> model: ms-marco-MiniLM-L6-v2 or the <code>bge</code> model: BGE-reranker-v2-m3 cross encoder model and deploy it as an Azure Machine Learning (AML) inference endpoint in your subscription. These models were selected because they are lightweight, easy to deploy, and provides fast inference.</p>"},{"location":"workshop/docs/07-Evaluation/02/#create-a-reranking-function","title":"Create a reranking function","text":"<p>To simplify the integration of semantic ranking and invocation of the semantic ranker model from Azure ML, you will create a user-defined function (UDF). The function uses the <code>azure_ai</code> extension to directly make remote calls to the AML inference endpoint from a SQL query. Using the <code>azure_ml.invoke()</code> function within a UDF, you can make calls to the semantic ranker model.</p> You already configured the extension's connection to Azure ML <p>Recall that you already configured the Azure AI extension's connection to Azure ML in task 3.2 Configure the Azure AI extension using the following commands:</p> <pre><code>SELECT azure_ai.set_setting('azure_ml.scoring_endpoint','&lt;endpoint&gt;');\nSELECT azure_ai.set_setting('azure_ml.endpoint_key', '&lt;api-key&gt;');\n</code></pre> <p>The UDF will be defined within your SQL database, allowing you to seamlessly incorporate semantic ranking into your queries. You will use pgAdmin to create it.</p> <ol> <li> <p>Return to the open instance of pgAdmin on your local machine and ensure it is connected to your PostgreSQL database.</p> </li> <li> <p>In the pgAdmin Object Explorer, expand databases under your PostgreSQL server.</p> </li> <li> <p>Right-click the contracts database and select Query Tool from the context menu.</p> </li> <li> <p>Run one of the following queries to create the <code>semantic_reranking</code> function:</p> <ul> <li>Run the first one if you chose the <code>mini</code> model</li> <li>Run the second one if you chose the <code>bge</code> model</li> </ul> <p>Execute one of the following SQL commands below in pgAdmin!</p> For mini model - Create Semantic Reranking UDF<pre><code>CREATE OR REPLACE FUNCTION semantic_reranking(query TEXT, vector_search_results TEXT[])\nRETURNS TABLE (content TEXT, relevance NUMERIC) AS $$\nBEGIN\n    RETURN QUERY\n    WITH\n    json_pairs AS (\n        SELECT jsonb_build_object(\n            'pairs', jsonb_agg(jsonb_build_array(query, content_))\n        ) AS json_pairs_data\n        FROM (\n            SELECT a.content AS content_\n            FROM unnest(vector_search_results) AS a(content)\n        )\n    ),\n    relevance_scores_raw AS (\n        SELECT azure_ml.invoke(\n            (SELECT json_pairs_data FROM json_pairs),\n            deployment_name =&gt; 'msmarco-minilm-deployment-6',\n            timeout_ms =&gt; 120000\n        ) AS response_json\n    ),\n    relevance_scores AS (\n    SELECT jsonb_array_elements(response_json) AS item\n    FROM relevance_scores_raw\n    )\n    SELECT\n        item -&gt;&gt; 'content' AS content,\n        (item -&gt;&gt; 'score')::NUMERIC AS relevance\n    FROM relevance_scores;\nEND $$ LANGUAGE plpgsql;\n</code></pre> For bge model - Create Semantic Reranking UDF<pre><code>CREATE OR REPLACE FUNCTION semantic_reranking(query TEXT, vector_search_results TEXT[])\nRETURNS TABLE (content TEXT, relevance NUMERIC) AS $$\nBEGIN\n    RETURN QUERY\n        WITH\n        json_pairs AS(\n            SELECT jsonb_build_object(\n                    'pairs', \n                    jsonb_agg(\n                        jsonb_build_array(query, content_)\n                    )\n                ) AS json_pairs_data\n            FROM (\n                SELECT a.content as content_\n                FROM unnest(vector_search_results) as a(content)\n            )\n        ), \n        relevance_scores AS(\n            SELECT jsonb_array_elements(invoke.invoke) as relevance_results\n            FROM azure_ml.invoke(\n                    (SELECT json_pairs_data FROM json_pairs),\n                    deployment_name=&gt;'bgev2m3-v1', timeout_ms =&gt; 120000)\n        ),\n        relevance_scores_rn AS (\n            SELECT *, ROW_NUMBER() OVER () AS idx\n            FROM relevance_scores\n        )\n        SELECT a.content,\n               (r.relevance_results::TEXT)::NUMERIC AS relevance\n            FROM\n                unnest(vector_search_results) WITH ORDINALITY AS a(content, idx2)\n            JOIN\n                relevance_scores_rn AS r(relevance_results, idx)\n            ON\n                a.idx2 = r.idx;\nEND $$ LANGUAGE plpgsql;\n</code></pre> How does the `semantic_reranking function work? <p>The <code>semantic_reranking</code> function enhances search results by re-ranking them based on their semantic relevance to a given query. Here's a breakdown of how it works:</p> <p>Input Parameters</p> <ul> <li><code>query</code>: A text string representing the search query.</li> <li><code>vector_search_results</code>: An array of text strings representing the initial search results obtained from a vector search.</li> </ul> <p>Return Value</p> <ul> <li>The function returns a table with two columns: <code>content</code> (the original search result content) and <code>relevance</code> (a NUMERIC object representing the relevance score).</li> </ul> <p>Steps</p> <ul> <li>Json Pairs Construction: The function starts by constructing a JSON object that pairs the query with each initial search result.</li> <li>Relevance Scoring: It then calls the <code>azure_ml.invoke</code> function to send the JSON pairs to an Azure Machine Learning endpoint, which computes the relevance scores for each pair. The results are returned as a JSON array.</li> <li>Row Number Assignment: The relevance scores are assigned row numbers to maintain their order.</li> <li>Combining Results: Finally, the function combines the original search results with corresponding relevance scores using the row numbers, ensuring that each result is paired with the correct relevance score.</li> </ul> <p>This function's overall purpose is to improve the relevance of search results by leveraging a semantic model hosted on Azure ML. This ensures that the results returned are more contextually relevant to the user's query.</p> </li> </ol>"},{"location":"workshop/docs/07-Evaluation/02/#test-the-udf","title":"Test the UDF","text":"<p>To see how semantic ranking works in your queries, you can execute a test query from pgAdmin. The example below shows how to perform semantic reranking of vector search results for the query \"cost management and optimization\" over the <code>sow_chunks</code> table using the <code>semantic_reranking</code> UDF you created.</p> <ol> <li> <p>In the pgAdmin query window, run the following query and observe the results.</p> <p>Execute the following SQL command in pgAdmin! (Works for both <code>mini</code> and <code>bge</code> models)</p> Semantic ranking query<pre><code>WITH vector_results AS (\n    SELECT content FROM sow_chunks c\n    ORDER BY embedding &lt;=&gt; azure_openai.create_embeddings('embeddings', 'cost management and optimization')::vector\n    LIMIT 10\n)\nSELECT content, relevance\nFROM semantic_reranking('cost management and optimization',  ARRAY(SELECT content from vector_results))\nORDER BY relevance DESC\nLIMIT 3;\n</code></pre> </li> </ol> <p>Next, you will update the <code>get_sow_chunks</code> function used by your copilot to use semantic ranking to improve the accuracy and quality of the copilot's responses.</p>"},{"location":"workshop/docs/07-Evaluation/03/","title":"6.3 Update Copilot With Semantic Ranking","text":"<p>The next step is to update your API to use the semantic ranking capability. For this, you will update how it finds and retrieves SOW chunks, which are the blocks of content extracted from each SOW uploaded into the application.</p>"},{"location":"workshop/docs/07-Evaluation/03/#review-the-function","title":"Review the function","text":"<p>Following the function calling pattern used by your LangChain agent to retrieve data from the database, you will use a Python function to execute semantic ranking queries from your copilot. Within the <code>src/api/app/functions/chat_functions.py</code> file, the <code>find_sow_chunks_with_semantic_ranking</code> function has been provided for executing queries using the <code>semantic_reranking</code> UDF you added to your database in the previous step. Open it now in Visual Studio Code and explore the code with the function. You can also expand the section below to see the code inline.</p> Find SOW Chunks with Semantic Ranking code src/api/app/functions/chat_functions.py<pre><code>async def find_sow_chunks_with_semantic_ranking(self, user_query: str, vendor_id: int = None, sow_id: int = None, max_results: int = 3):\n        \"\"\"\n        Retrieves content chunks similar to the user query for the specified SOW.\n        \"\"\"\n\n        # Get the embeddings for the user query\n        query_embeddings = await self.__create_query_embeddings(user_query)\n\n        # Create a vector search query\n        cte_query = f\"SELECT content FROM sow_chunks\"\n        cte_query += f\" WHERE sow_id = {sow_id}\" if sow_id is not None else f\" WHERE vendor_id = {vendor_id}\" if vendor_id is not None else \"\"\n        cte_query += f\" ORDER BY embedding &lt;=&gt; '{query_embeddings}'\"\n        cte_query += f\" LIMIT 10\"\n\n        # Create the semantic ranker query\n        query = f\"\"\"\n        WITH vector_results AS (\n            {cte_query}\n        )\n        SELECT content, relevance\n        FROM semantic_reranking('{user_query}',  ARRAY(SELECT content from vector_results))\n        ORDER BY relevance DESC\n        LIMIT {max_results};\n        \"\"\"\n\n        rows = await self.__execute_query(f'{query};')\n        return [dict(row) for row in rows]\n</code></pre> <ol> <li> <p>Generate embeddings (line 18): Azure OpenAI generates embeddings representing the user query.</p> </li> <li> <p>Create vector search query (lines 21-24): The UDF expects vector search results as input, so a vector search query is created. The query selects the <code>content</code> field from the <code>sow_chunks</code> table and orders them by semantic similarity.</p> </li> <li> <p>Create a semantic ranker query (lines 27-35): The results of the vector search query are required to call the <code>semantic_reranking</code> UDF.</p> <ul> <li>Create CTE (lines 28-30): A common table expression (CTE) executes the vector search query and extracts the content values and relevancy scores from that query.</li> <li>Execute the <code>semantic_reranking</code> UDF (lines 31-33): Using the results of the CTE, the results are reranked using the <code>semantic_reranking</code> UDF.</li> <li>Limit results (line 34): The number of results is limited to ensure the more relevant records are sent to the LLM.</li> </ul> </li> <li> <p>Return the results (lines 37-38): The query results are extracted and returned to the LLM.</p> </li> </ol>"},{"location":"workshop/docs/07-Evaluation/03/#implement-semantic-ranker","title":"Implement semantic ranker","text":"<p>To use the semantic ranking functionality instead of the vector search to retrieve SOW chunks, you must replace the function assigned to your LangChain agent's <code>tools</code> collection. You will replace the <code>find_sow_chunks</code> tool with the <code>find_sow_chunks_with_semantic_ranking</code> function.</p> <ol> <li> <p>In the VS Code Explorer, navigate to the <code>src/api/app/routers</code> folder and open the <code>completions.py</code> file.</p> </li> <li> <p>Within the <code>tools</code> array, locate the following line:</p> Python<pre><code>StructuredTool.from_function(coroutine=cf.find_sow_chunks),\n</code></pre> </li> <li> <p>Replace that line with the following:</p> <p>Insert the code to use the semantic ranking function!</p> Python<pre><code>StructuredTool.from_function(coroutine=cf.find_sow_chunks_with_semantic_ranking),\n</code></pre> </li> <li> <p>Your new <code>tools</code> array should look like this:</p> Python<pre><code># Define tools for the agent to retrieve data from the database\ntools = [\n    # Hybrid search functions\n    StructuredTool.from_function(coroutine=cf.find_invoice_line_items),\n    StructuredTool.from_function(coroutine=cf.find_invoice_validation_results),\n    StructuredTool.from_function(coroutine=cf.find_milestone_deliverables),\n    StructuredTool.from_function(coroutine=cf.find_sow_chunks_with_semantic_ranking),\n    StructuredTool.from_function(coroutine=cf.find_sow_validation_results),\n    # Get invoice data functions\n    StructuredTool.from_function(coroutine=cf.get_invoice_id),\n    StructuredTool.from_function(coroutine=cf.get_invoice_line_items),\n    StructuredTool.from_function(coroutine=cf.get_invoice_validation_results),\n    StructuredTool.from_function(coroutine=cf.get_invoices),\n    # Get SOW data functions\n    StructuredTool.from_function(coroutine=cf.get_sow_chunks),\n    StructuredTool.from_function(coroutine=cf.get_sow_id),\n    StructuredTool.from_function(coroutine=cf.get_sow_milestones),\n    StructuredTool.from_function(coroutine=cf.get_milestone_deliverables),\n    StructuredTool.from_function(coroutine=cf.get_sow_validation_results),\n    StructuredTool.from_function(coroutine=cf.get_sows),\n    # Get vendor data functions\n    StructuredTool.from_function(coroutine=cf.get_vendors)\n]\n</code></pre> </li> <li> <p>Save the <code>completions.py</code> file.</p> </li> </ol>"},{"location":"workshop/docs/07-Evaluation/03/#test-with-vs-code","title":"Test with VS Code","text":"<p>As you have done previously, you will test your updates using Visual Studio Code.</p>"},{"location":"workshop/docs/07-Evaluation/03/#start-the-api","title":"Start the API","text":"<p>Follow the steps below to start a debug session for the API in VS Code.</p> <ol> <li> <p>In Visual Studio Code Run and Debug panel, select the API Debugger option for your OS from the debug configurations dropdown list.</p> <p></p> </li> <li> <p>Select the Start Debugging button (or press F5 on your keyboard).</p> <p></p> </li> <li> <p>Wait for the API application to start completely, indicated by an <code>Application startup complete.</code> message in the terminal output.</p> <p></p> </li> </ol>"},{"location":"workshop/docs/07-Evaluation/03/#start-the-portal","title":"Start the Portal","text":"<p>With the API running, you can start a second debug session in VS Code for the Portal project.</p> <ol> <li> <p>Return to the Run and Debug panel in Visual Studio Code and select the Portal Debugger option from the debug configurations dropdown list.</p> <p></p> </li> <li> <p>Select the Start Debugging button (or press F5 on your keyboard).</p> <p></p> </li> <li> <p>This should launch the Woodgrove Bank Contract Management Portal in a new browser window (http://localhost:3000/).</p> </li> <li> <p>On the Dashboard page, enter the following message into the chat and send it:</p> <p>Paste the following prompt into the copilot chat box!</p> <pre><code>Show me SOWs pertaining to cost management and optimization.\n</code></pre> </li> <li> <p>Observe the results provided using your UDF and semantic ranking.</p> </li> </ol> <p>Congratulations! You just learned how to leverage the semantic ranking capabilities in Azure Database for PostgreSQL!</p>"},{"location":"workshop/docs/07-Evaluation/04/","title":"6.4 Review GraphRAG","text":"<p>GraphRAG (Graph Retrieval-Augmented Generation) is an innovative technique developed by Microsoft Research. It significantly enhances the accuracy and relevance of responses generated by Retrieval-Augmented Generation (RAG) systems. Graph refers to a data structure representing entities and their relationships. This structure is often visualized as nodes (vertices) and edges, where nodes represent entities and edges represent the relationships or connections between these entities.</p> <p>In the context of Azure Database for PostgreSQL, GraphRAG leverages the structure of knowledge graphs extracted from source data to provide better context and improve the quality of responses from language models. By integrating GraphRAG with PostgreSQL, you can enhance the information retrieval pipeline, making it more accurate and context-aware. This is particularly useful for applications where the accuracy and relevance of information are critical.</p> Want a more extensive example of GraphRAG with Azure Database for PostgreSQL? <p>This solution accelerator presents a simplified GraphRAG implementation to show you how you can improve the accuracy of RAG by leveraging a graph database. For a more comprehensive example that combines the results of semantic ranking and GraphRAG, please refer to the Introducting the GraphRAG Solution for Azure Database for PostgreSQL blob post and associated GraphRAG Solution Accelerator for Azure Database for PostgreSQL.</p>"},{"location":"workshop/docs/07-Evaluation/04/#graphrag-with-age","title":"GraphRAG with AGE","text":"<p>Apache Graph Extension (AGE) is a PostgreSQL extension developed under the Apache Incubator project. It offers a significant advancement that provides graph processing capabilities within the PostgreSQL ecosystem, enabling users to store efficiently and query graph data. This new extension brings a powerful toolset for developers looking to leverage a graph database with the robust enterprise features of Azure Database for PostgreSQL.</p> <p></p> <p>With AGE, you can manage and analyze complex relationships within your data, uncovering insights that traditional relational databases and even semantic search might miss.</p>"},{"location":"workshop/docs/07-Evaluation/04/#key-features","title":"Key Features","text":"<ul> <li>Graph and relational data integration: AGE allows seamless integration of graph data with existing relational data in PostgreSQL. This hybrid approach lets you benefit from both graph and relational models simultaneously.</li> <li>openCypher query language: AGE incorporates openCypher, a powerful and user-friendly query language designed explicitly for graph databases. This feature simplifies the process of writing and executing graph queries.</li> <li>High performance: AGE is optimized for performance, ensuring efficient storage and retrieval of graph data thanks to support for indexing graph properties using GIN indices.</li> <li>Scalability: Built on PostgreSQL's proven architecture, AGE inherits its scalability and reliability, allowing it to handle growing datasets and increasing workloads.</li> </ul>"},{"location":"workshop/docs/07-Evaluation/04/#benefits","title":"Benefits","text":"<p>The integration of AGE in Azure Database for PostgreSQL brings numerous practical benefits to developers and businesses looking to leverage graph processing capabilities:</p> <ul> <li>Simplified data management: AGE's ability to integrate graph and relational data simplifies data management tasks, reducing the need for separate graph database solutions.</li> <li>Enhanced data analysis: With AGE, you can perform complex graph analyses directly within your PostgreSQL database, gaining deeper insights into relationships and patterns in your data.</li> <li>Cost efficiency: By utilizing AGE within Azure Database for PostgreSQL, you can consolidate your database infrastructure, lowering overall costs and reducing the complexity of your data architecture.</li> <li>Security and compliance: AGE leverages Azure's industry-leading security and compliance features to ensure your graph data is protected and meets regulatory requirements, providing peace of mind.</li> </ul>"},{"location":"workshop/docs/07-Evaluation/05/","title":"6.5 Enable AGE","text":"<p>The Apache AGE (<code>age</code>) extension enhances PostgreSQL by allowing it to be used as a graph database, providing a comprehensive solution for analyzing interconnected data. With <code>age</code>, you can define and query complex data relationships using graph structures.</p> <p>To simplify the extraction of data from your PostgreSQL database into CSV files hosted in Azure Blob Storage, you will also use the Azure Storage extension (<code>azure_storage</code>). This extension allows you to connect directly to an Azure Storage account from your PostgreSQL database, copy data out of the database, and write it into files in blob storage. From your storage account, the CSV files will be used as the data source for your graph database.</p>"},{"location":"workshop/docs/07-Evaluation/05/#allowlist-the-extensions","title":"Allowlist the extensions","text":"<p>Before using <code>age</code> and <code>azure_storage</code> extensions, add them to the PostgreSQL server's allowlist, configure them as shared preloaded libraries, and install them in your database.</p> <p>Select the tab for the method you want to use for allowlisting the extensions and follow the instructions provided.</p> Azure CLIAzure portal <ol> <li> <p>Open a new integrated terminal window in VS Code and execute the following Azure CLI command at the prompt.</p> <p>Ensure you replace the tokens in the command below with the appropriate values from your Azure environment.</p> <ul> <li>[YOUR_RESOURCE_GROUP]: The name of the resource group hosting your Azure Database for PostgreSQL flexible server.</li> <li>[YOUR_POSTGRESQL_SERVER]: The name of your Azure Database for PostgreSQL server.</li> <li>[YOUR_SUBSCRIPTION_ID]: Your Azure subscription ID.</li> </ul> <p>Execute the following Azure CLI command!</p> Bash<pre><code>az postgres flexible-server parameter set --resource-group [YOUR_RESOURCE_GROUP] --server-name [YOUR_POSTGRESQL_SERVER] --subscription [YOUR_SUBSCRIPTION_ID] --name azure.extensions --value azure_ai,pg_diskann,vector,age,azure_storage\n</code></pre> </li> </ol> <ol> <li> <p>Navigate to your Azure Database for PostgreSQL flexible server instance in the Azure portal.</p> </li> <li> <p>From the left-hand resource menu:</p> <ol> <li>Expand the Settings section and select Server parameters.</li> <li>Enter \"azure.extensions\" into the search filter.</li> <li>Expand the VALUE dropdown list.</li> <li>Select the AGE and AZURE_STORAGE extensions by checking the box for each in the VALUE dropdown list.</li> </ol> <p></p> </li> </ol>"},{"location":"workshop/docs/07-Evaluation/05/#load-extensions-on-server-start","title":"Load extensions on server start","text":"<p>Some Postgres libraries need to perform certain operations that can only take place at postmaster start, such as allocating shared memory, reserving lightweight locks, or starting background workers. <code>AGE</code> relies on shared memory for its operations, so it must be loaded at server start. The <code>shared_preload_libraries</code> parameter in PostgreSQL is used to specify libraries that should be loaded at server startup, enabling additional functionalities or extensions before any connections are made.</p> <p>Select the tab for the method you want to use to update the <code>shared_preload_libraries</code> parameter and follow the instructions provided.</p> Azure CLIAzure portal <ol> <li> <p>In the VS Code integrated terminal window, execute the following Azure CLI command at the prompt.</p> <p>Ensure you replace the tokens in the command below with the appropriate values from your Azure environment.</p> <ul> <li>[YOUR_RESOURCE_GROUP]: The name of the resource group hosting your Azure Database for PostgreSQL flexible server.</li> <li>[YOUR_POSTGRESQL_SERVER]: The name of your Azure Database for PostgreSQL server.</li> <li>[YOUR_SUBSCRIPTION_ID]: Your Azure subscription ID.</li> </ul> <p>Execute the following Azure CLI command!</p> Bash<pre><code>az postgres flexible-server parameter set --resource-group [YOUR_RESOURCE_GROUP] --server-name [YOUR_POSTGRESQL_SERVER] --subscription [YOUR_SUBSCRIPTION_ID] --name shared_preload_libraries --value age,azure_storage,pg_cron,pg_stat_statements\n</code></pre> <p><code>pg_cron</code> and <code>pg_stat_statements</code> are set by default, so they are included in the above command to avoid removing them from the <code>shared_preload_libraries</code> parameter.</p> </li> <li> <p>The above command sets the parameter, but your PostgreSQL flexible server requires a restart for the setting to take effect. Run the following command to restart your server:</p> <p>Ensure you replace the tokens in the command below with the appropriate values from your Azure environment.</p> <ul> <li>[YOUR_RESOURCE_GROUP]: The name of the resource group hosting your Azure Database for PostgreSQL flexible server.</li> <li>[YOUR_POSTGRESQL_SERVER]: The name of your Azure Database for PostgreSQL server.</li> <li>[YOUR_SUBSCRIPTION_ID]: Your Azure subscription ID.</li> </ul> <p>Execute the following Azure CLI command!</p> Bash<pre><code>az postgres flexible-server restart --resource-group [YOUR_RESOURCE_GROUP] --name [YOUR_POSTGRESQL_SERVER] --subscription [YOUR_SUBSCRIPTION_ID]\n</code></pre> </li> </ol> <ol> <li> <p>On the Server parameters page of your Azure Database for PostgreSQL flexible server instance in the Azure portal:</p> <ol> <li>Enter \"shared_preload\" into the search filter.</li> <li>Expand the VALUE dropdown list.</li> <li>Select the AGE and AZURE_STORAGE extensions by checking the box for each in the VALUE dropdown list.</li> <li>Select Save on the toolbar.</li> </ol> <p></p> </li> <li> <p>Select Save will trigger a restart of the PostgreSQL server and will take a few seconds to complete.</p> </li> <li> <p>In the Save server parameter dialog that appears, select Save and Restart.</p> <p></p> </li> </ol>"},{"location":"workshop/docs/07-Evaluation/05/#install-extensions","title":"Install extensions","text":"<p>With the <code>AGE</code> and <code>AZURE_STORAGE</code> extensions added to the allowlist and loaded on your PostgreSQL server, you can install them in your database using the CREATE EXTENSION command.</p> <p>At this time, the AGE extension is in preview and will only be available for newly created Azure Database for PostgreSQL Flexible Server instances running at least PG13 up to PG16.</p> <p>You will use pgAdmin to install the extension by executing a SQL command against your database.</p> <ol> <li> <p>On your local machine, return to the open instance of pgAdmin (or open it if you closed it after the setup tasks) and ensure it is connected to your PostgreSQL database.</p> </li> <li> <p>In the pgAdmin Object Explorer, expand databases under your PostgreSQL server.</p> </li> <li> <p>Right-click the contracts database and select Query Tool from the context menu.</p> </li> <li> <p>Install the <code>age</code> and <code>azure_storage</code> extensions by running the following <code>CREATE EXTENSION</code> commands in the pgAdmin query window:</p> <p>Execute the following SQL commands in pgAdmin!</p> <pre><code>CREATE EXTENSION IF NOT EXISTS age;\nCREATE EXTENSION IF NOT EXISTS azure_storage;\n</code></pre> </li> </ol>"},{"location":"workshop/docs/07-Evaluation/06/","title":"6.6 Export Graph Data","text":"<p>You must export data from your PostgreSQL database to populate your graph database. Using the Azure Storage (<code>azure_storage</code>) extension for Azure Database for PostgreSQL provides a streamlined method for copying data out of your PostgreSQL database into CSV files in Azure Blob Storage. In the context of <code>AGE</code> and loading data into a graph database, the <code>azure_storage</code> extension facilitates the extraction of relational data from your PostgreSQL database, enabling efficient transfer to Blob Storage. This process ensures that the data needed for constructing and querying your graph database is readily available and can be seamlessly integrated into your data workflows.</p>"},{"location":"workshop/docs/07-Evaluation/06/#connect-your-database-to-azure-storage","title":"Connect your database to Azure Storage","text":"<p>You will use pgAdmin to configure the <code>azure_storage</code> extension's connection to your storage account by executing SQL commands against your database.</p> <p>Ensure you replace the token in the commands below with the appropriate values from your Azure environment.</p> <p>Each SQL statement you will execute below contains a <code>[YOUR_STORAGE_ACCOUNT_NAME]</code> token. Before running any of the queries, you must replace this token with the name of your Storage account resource, which you can copy from the Azure portal.</p> <ol> <li> <p>On your local machine, return to the open instance of pgAdmin (or open it if you have closed it) and ensure it is connected to your PostgreSQL database.</p> </li> <li> <p>In the pgAdmin Object Explorer, expand databases under your PostgreSQL server.</p> </li> <li> <p>Right-click the contracts database and select Query Tool from the context menu.</p> </li> <li> <p>Run the following command to use the <code>azure_storage.account_add()</code> function to define a connection between your storage account and your PostgreSQL database. Replace the account name and key tokens with values for your storage account.</p> <p>Execute the following SQL commands in pgAdmin!</p> SQL<pre><code>SELECT azure_storage.account_add('[YOUR_STORAGE_ACCOUNT_NAME]', '[YOUR_STORAGE_ACCOUNT_KEY]');\n</code></pre> Need help finding your storage account key? <p>To get your storage account's access key:</p> <ol> <li>Navigate to your storage account in the Azure portal.</li> <li>Select the Access keys menu under Security + networking in the navigation menu.</li> <li>Select Show next to the Key value under key1.</li> <li>Select the Copy to clipboard button that appears on the right-hand side of the Key box.</li> <li> <p>Paste the copied key as the <code>[YOUR_STORAGE_ACCOUNT_KEY]</code> value in the above SQL statement.</p> <p></p> </li> </ol> </li> </ol>"},{"location":"workshop/docs/07-Evaluation/06/#export-data-to-blob-storage","title":"Export data to blob storage","text":"<p>As part of the data export process, you will use queries to reshape the source data into the format required to efficiently define nodes and edges in your graph database. Traditional relational databases organize data in tables, while graph databases use nodes and edges to represent entities and their relationships. Converting tabular data into nodes and edges aligns with the graph structure, making relationship analysis more efficient. This transformation enables natural modeling of real-world entities, optimizes query performance, and allows for complex relationship analysis, such as evaluating the connections between vendors, SOWs, and associated invoices. By reshaping your data, you can fully leverage the strengths of <code>AGE</code> and Azure Database for PostgreSQL for deeper insights and sophisticated analyses.</p> <p>You will define two nodes and one edge in your graph database. The nodes will contain data vendor, and SOW data. The edge will define the relationship between these.</p> <p>You will use pgAdmin to execute data export queries leveraging the <code>azure_storage</code> extension.</p> <ol> <li> <p>Return to the open Query Tool in pgAdmin.</p> </li> <li> <p>Run the following query using the <code>azure_storage.blob_put()</code> function to write all data from the <code>vendors</code> table into a CSV file named <code>vendors.csv</code> into your storage account's <code>graph</code> container. This data will define the <code>vendor</code> node in your graph database.</p> <p>Execute the following SQL commands in pgAdmin!</p> SQL<pre><code>-- Extract data for the vendors node\nSELECT azure_storage.blob_put(\n    '[YOUR_STORAGE_ACCOUNT_NAME]',\n    'graph',\n    'vendors.csv',\n    vendors,\n    'csv',\n    'none',\n    azure_storage.options_csv_put(header:=true)\n)\nFROM (\n    SELECT * FROM vendors\n) AS vendors;\n</code></pre> </li> <li> <p>Execute this query to extract <code>sow</code> node data from the <code>sows</code> table and write it into a CSV file named <code>sows.csv</code> into your storage account's <code>graph</code> container. The query excludes a few columns from the <code>sows</code> table, including the <code>embedding</code> column, as they are unnecessary in the graph database and can cause errors.</p> <p>Execute the following SQL commands in pgAdmin!</p> SQL<pre><code>-- Extract data for the SOWs node\nSELECT azure_storage.blob_put(\n    '[YOUR_STORAGE_ACCOUNT_NAME]',\n    'graph',\n    'sows.csv',\n    sows,\n    'csv',\n    'none',\n    azure_storage.options_csv_put(header:=true)\n)\nFROM (\n    SELECT id, number, vendor_id, start_date, end_date, budget FROM sows\n) AS sows;\n</code></pre> </li> <li> <p>Finally, run the following query to extract <code>has_invoices</code> edge data from the <code>invoices</code> table and write it into a CSV file named <code>has_invoices.csv</code> into the <code>graph</code> container in your storage account:</p> <p>Execute the following SQL commands in pgAdmin!</p> SQL<pre><code>-- Create the has_invoices edge\nSELECT azure_storage.blob_put(\n    '[YOUR_STORAGE_ACCOUNT_NAME]',\n    'graph',\n    'has_invoices.csv',\n    invoices,\n    'csv',\n    'none',\n    azure_storage.options_csv_put(header:=true)\n)\nFROM (\n    SELECT id, vendor_id as start_id, 'vendor' AS start_vertex_type, sow_id AS end_id, 'sow' AS end_vertex_type, number, amount, invoice_date, payment_status FROM invoices\n) AS invoices;\n</code></pre> <p>Edge definition details</p> <p>When using <code>AGE</code>, edges must contain details about the relationships between nodes. These are defined in the above query by specifying the <code>start_id</code>, <code>start_vertex_type</code>, <code>end_id</code>, and <code>end_vertex_type</code> columns. The '_id' columns are mapped to the <code>vendor_id</code> and <code>sow_id</code>, respectively, and the start and end vertex types are strings specifying the node type associated with the ID.</p> </li> </ol>"},{"location":"workshop/docs/07-Evaluation/06/#verify-csv-files","title":"Verify CSV files","text":"<p>You can use the <code>azure_storage</code> extension to verify the CSV files were successfully written into the <code>graph</code> container in your storage account.</p> <ol> <li> <p>Execute the following query in the Query Tool in pgAdmin. Ensure you replace the <code>[YOUR_STORAGE_ACCOUNT_NAME]</code> token with the name of your storage account.</p> <p>Execute the following SQL commands in pgAdmin!</p> SQL<pre><code>-- Verify the CSV files were written into blob storage\nSELECT azure_storage.blob_list('[YOUR_STORAGE_ACCOUNT_NAME]', 'graph');\n</code></pre> </li> <li> <p>You should see a list of blobs in the Data output panel in pgAdmin that includes the three CSV files you exported above.</p> </li> </ol> <p>Congratulations! You have successfully exported data to create your graph database!</p>"},{"location":"workshop/docs/07-Evaluation/07/","title":"6.7 Create Graph Database","text":"<p>Hosting graph databases in Azure Database for PostgreSQL using the Apache AGE extension offers a powerful way to analyze relationships within your data. AGE combines graph and relational data seamlessly, leveraging the openCypher query language for efficient graph processing. This integration brings PostgreSQL's scalability, performance, and security to the table while enabling advanced data analysis and management. When incorporated into a copilot, this setup empowers you to evaluate vendor performance of SOW deliverables through invoice validation, ensuring your data-driven decisions are robust and insightful.</p> <p>Once you have created your graph database, it will be incorporated into the Woodgrove Bank API as an extra tool available to the copilot, allowing you to improve RAG accuracy when retrieving relevant data. By using another function call, the copilot will be able to execute cypher queries against the graph database hosted in Azure Database for PostgreSQL using AGE while maintaining the existing capabilities of querying data in Postgres.</p>"},{"location":"workshop/docs/07-Evaluation/07/#woodgrove-graph","title":"Woodgrove Graph","text":"<p>Graph databases allow you to model complex relationships between data using nodes and edges, making it easier to represent and query these relationships. You will build a simple graph using the data you extracted from the Woodgrove Bank database. You will define <code>vendors</code> and <code>sows</code> as nodes in your graph and use <code>invoices</code> as the edge between them. Edges in graph databases define a one-to-one relationship between two entities, defined as nodes. The diagram below provides a high-level representation of the graph database.</p> <p></p> <p>Edges must include a mapping of related entities through ID mapping and can also include properties, which allow the relationship to be filtered at query time.</p>"},{"location":"workshop/docs/07-Evaluation/07/#create-graph-database-with-agefreighter","title":"Create Graph Database with AGEFreighter","text":"<p>AGEFreighter is a Python library designed to simplify the process of creating and loading graph databases in Azure Database for PostgreSQL, allowing data to be ingested from various sources, including CSV file, Azure Cosmos DB, Neo4j, and Azure Database for PostgreSQL.</p>"},{"location":"workshop/docs/07-Evaluation/07/#review-code","title":"Review code","text":"<p>The solution accelerator includes the <code>graph_loader.py</code> file in the <code>src/api/app</code> folder, which allows you to quickly run a Python script to create a graph database and populate it with data from CSV files.</p> <p>The graph loader is implemented in the <code>src/api/app/graph_loader.py</code> file. Open it now in Visual Studio Code and explore the code in sections. You can also expand the section below to see the code inline and review explanations for the code.</p> Graph Loader code src/api/app/graph_loader.py<pre><code>import os\nfrom agefreighter import Factory\nfrom dotenv import load_dotenv\nfrom azure.identity import DefaultAzureCredential\nfrom azure.storage.blob.aio import BlobServiceClient\n\nasync def main():\n    \"\"\"Load data into Azure Database for PostgreSQL Graph Database.\"\"\"\n    # Load environment variables from the .env file\n    load_dotenv()\n    print(\"Loading environment variables...\")\n\n    # Get environment variables\n    server = os.getenv(\"POSTGRESQL_SERVER_NAME\")\n    database = 'contracts'\n    username = os.getenv(\"ENTRA_ID_USERNAME\")\n    account_name = os.getenv(\"STORAGE_ACCOUNT_NAME\")\n\n    # Create an AGEFreigher factory instance to load data from multiple CSV files.\n    print(\"Creating AGEFreighter factory instance...\")\n    factory = Factory.create_instance('MultiCSVFreighter')\n\n    # Connect to the PostgreSQL database.\n    print(\"Connecting to the PostgreSQL database...\")\n    await factory.connect(\n        dsn=get_connection_string(server, database, username),\n        max_connections=64\n    )\n\n    local_data_dir = 'graph_data/'\n\n    # Download CSV data files from Azure Blob Storage\n    print(\"Downloading CSV files from Azure Blob Storage...\")\n    await download_csvs(account_name, local_data_dir)\n\n    # Load data into the graph database\n    print(\"Loading data into the graph database...\")\n    await factory.load(\n        graph_name='vendor_graph',\n        vertex_csv_paths = [\n            f'{local_data_dir}vendors.csv',\n            f'{local_data_dir}sows.csv'\n        ],\n        vertex_labels = ['vendor', 'sow'],\n        edge_csv_paths = [f'{local_data_dir}has_invoices.csv'],\n        edge_types = [\"has_invoices\"],\n        use_copy=True,\n        drop_graph=True,\n        create_graph=True,\n        progress=True\n    )\n\n    print(\"Graph data loaded successfully!\")\n\ndef get_connection_string(server_name: str, database_name: str, username: str):\n    \"\"\"Get the connection string for the PostgreSQL database.\"\"\"\n\n    # Get a token for the Azure Database for PostgreSQL server\n    credential = DefaultAzureCredential()\n    token = credential.get_token(\"https://ossrdbms-aad.database.windows.net\")\n    port = 5432\n\n    conn_str = \"host={} port={} dbname={} user={} password={}\".format(\n        server_name, port, database_name, username, token.token\n    )\n    return conn_str\n\nasync def download_csvs(account_name:str, local_data_directory: str):\n    \"\"\"Download CSV files from Azure Blob Storage.\"\"\"\n\n    # Create connection to the blob storage account\n    account_blob_endpoint = f\"https://{account_name}.blob.core.windows.net/\"\n    # Connect to the blob service client using Entra ID authentication\n    client = BlobServiceClient(account_url=account_blob_endpoint, credential=DefaultAzureCredential())\n\n    # List the blobs in the graph container with a CSV extension\n    async with client:\n        async for blob in client.get_container_client('graph').list_blobs():\n            if blob.name.endswith('.csv'):\n                # Download the CSV file to a local directory\n                await download_csv(client, blob.name, local_data_directory)\n\nasync def download_csv(client: BlobServiceClient, blob_path: str, local_data_dir: str):\n    \"\"\"Download a CSV file from Azure Blob Storage.\"\"\"\n    # Get the blob\n    blob_client = client.get_blob_client(container='graph', blob=blob_path)\n\n    async with blob_client:\n        # Download the CSV file\n        if await blob_client.exists():\n            # create a local directory if it does not exist\n            if not os.path.exists(local_data_dir):\n                os.makedirs(local_data_dir)\n\n            with open(f'{local_data_dir}{blob_path.split('/')[-1]}', 'wb') as file:\n                stream = await blob_client.download_blob()\n                result = await stream.readall()\n                # Save the CSV file to a local directory\n                file.write(result)\n\nif __name__ == \"__main__\":\n    import asyncio\n    import sys\n\n    if sys.platform == \"win32\":\n        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n\n    asyncio.run(main())\n</code></pre> <ol> <li> <p>Import libraries (lines 1-5): Required classes and functions are imported from various libraries.</p> </li> <li> <p>Define <code>main</code> function (line 7): The <code>main</code> function is the entry point of the graph loader. This function serves as the orchestrator for executing the code within the file.</p> </li> <li> <p>Load environment variables (lines 10-17): The <code>load_dotenv()</code> method from the <code>dotenv</code> Python library allows variables from the <code>.env</code> file within the API project to be loaded as environment variables in the project. Note the names of the variables here, as you will be adding those to your <code>.env</code> file in the next step.</p> </li> <li> <p>Create an AGEFreighter factory (line 21): The entry point for the <code>agefreighter</code> package in the <code>factory</code> class. This method creates an instance of the library using the type specified. You are loading your graph using multiple CSV files, so the <code>MultiCSVFreighter</code> class type is indicated.</p> </li> <li> <p>Connect to PostgreSQL (lines 25-28): The <code>connect</code> method of the <code>factory</code> opens a connection to your Azure Database for PostgreSQL flexible server.</p> <ol> <li>The <code>get_connection_string()</code> function uses values from your environment variables to define the connection string the <code>factory</code> will use to connect to your database.</li> <li>The <code>get_connection_string()</code> function is defined on lines 55-66.</li> </ol> </li> <li> <p>Download CSV files from blob storage (line 34): The CSV files you created in the previous task are downloaded from blob storage and written into a local folder, where the graph loader can easily access them.</p> <ol> <li>The <code>download_csvs()</code> function is defined on lines 68-81. This function creates a <code>BlobServiceClient</code> instance, which is used to retrieve the blobs in your storage account's <code>graph</code> container.</li> <li>For each blob with the extension of <code>.csv</code>, the <code>download_csv</code> function defined on lines 83-99 is used to retrieve the blob's contents and write them into a local file.</li> </ol> </li> <li> <p>Create and load the graph database (lines 38-51): The <code>load</code> method of the <code>factory</code> does the following:</p> <ol> <li>Creates a graph named <code>vendor_graph</code>.</li> <li>Defines vertex (node) data and labels and inserts the nodes into the graph.</li> <li>Specifies edges using labels and inserts them to establish the relationships between the nodes.</li> </ol> </li> <li> <p>Define the main guard (lines 101-108): The main guard defines how the <code>graph_loader</code> is executed when called directly. This code block lets you run the script from a command line or VS Code debugging session.</p> </li> </ol>"},{"location":"workshop/docs/07-Evaluation/07/#update-env-file","title":"Update <code>.env</code> file","text":"<p>The <code>graph_loader.py</code> file references environment variables to retrieve information about your Azure Database for PostgreSQL flexible server instance, your Entra ID username, and the storage account from which to pull CSV files. Before executing the graph loader script, you must update your project's <code>.env</code> file with these values. The <code>.env</code> file can be found in the <code>src\\api\\app</code> folder of the repo.</p> <ol> <li> <p>In VS Code, navigate to the <code>src\\api\\app</code> folder in the Explorer panel.</p> </li> <li> <p>Open the <code>.env</code> file and add the following lines:</p> <p>Update your <code>.env</code> file!</p> <pre><code>ENTRA_ID_USERNAME=\"{YOUR_ENTRA_ID_USERNAME}\"\nPOSTGRESQL_SERVER_NAME=\"{YOUR_POSTGRESQL_SERVER_NAME}\"\nSTORAGE_ACCOUNT_NAME=\"{YOUR_STORAGE_ACCOUNT_NAME}\"\n</code></pre> Follow these steps to retrieve the necessary values <ol> <li> <p>Replace the <code>{YOUR_ENTRA_ID_USERNAME}</code> token in the <code>ENTRA_ID_USERNAME</code> variable's value with your Microsoft Entra ID, which should be the email address of the account you are using for this solution accelerator.</p> </li> <li> <p>Replace the <code>{YOUR_POSTGRESQL_SERVER_NAME}</code> token with the name of your PostgreSQL server. To get your server name:</p> </li> <li> <p>Navigate to your Azure Database for PostgreSQL flexible server resource in the Azure portal.</p> </li> <li> <p>In the Essentials panel of the PostgreSQL flexible server's Overview page, copy the Server name value and paste it into your <code>.env</code> file as the <code>POSTGRESQL_SERVER_NAME</code> value.</p> <p></p> </li> <li> <p>Replace the <code>{YOUR_STORAGE_ACCOUNT_NAME}</code> token with the name of your storage account. To retrieve your storage account name:</p> </li> <li> <p>In the Azure portal, navigate to the Storage account resource in your resource group.</p> </li> <li> <p>On the Storage account page, copy the storage account name and paste it into your <code>.env</code> file as the <code>STORAGE_ACCOUNT_NAME</code> value.</p> <p></p> </li> </ol> </li> <li> <p>Save the <code>.env</code> file.</p> </li> </ol>"},{"location":"workshop/docs/07-Evaluation/07/#load-the-graph-database","title":"Load the graph database","text":"<p>You will use a VS Code debugging session to locally execute the <code>graph_loader.py</code> script. Follow the steps below to start a Graph Loader debug session in VS Code.</p> <ol> <li> <p>In Visual Studio Code Run and Debug panel, select the Graph Loader option from the debug configurations dropdown list.</p> <p></p> </li> <li> <p>Select the Start Debugging button (or press F5 on your keyboard).</p> <p></p> </li> <li> <p>Wait for the graph loader to finish running, indicated by the <code>Graph data loaded successfully!</code> message in the terminal output.</p> <p></p> </li> </ol>"},{"location":"workshop/docs/07-Evaluation/07/#verify-data-load","title":"Verify data load","text":"<p>You will execute openCypher queries using pgAdmin to verify the data load and explore relationships in your graph database.</p> <ol> <li> <p>Return to pgAdmin and ensure it is connected to your PostgreSQL database.</p> </li> <li> <p>In the pgAdmin Object Explorer, expand databases under your PostgreSQL server.</p> </li> <li> <p>Right-click the contracts database and select Query Tool from the context menu.</p> </li> <li> <p>Before you can run cypher queries, you must set the <code>ag_catalog</code> schema in your path:</p> <p>Execute the following SQL commands in pgAdmin!</p> SQL<pre><code>SET search_path = ag_catalog, \"$user\", public;\n</code></pre> </li> <li> <p>Now, run the following cypher query to view vendors with open invoices, the details of those invoices, and verify your graph database was loaded correctly:</p> <p>Execute the following SQL commands in pgAdmin!</p> SQL<pre><code>-- View vendors and SOWs, along with invoice details from edge properties\nSELECT * FROM ag_catalog.cypher('vendor_graph', $$\nMATCH (v:vendor)-[rel:has_invoices]-&gt;(s:sow)\nRETURN v.id AS vendor_id, v.name AS vendor_name, s.id AS sow_id, s.number AS sow_number, rel.payment_status AS payment_status, rel.amount AS invoice_amount\n$$) as graph_query(vendor_id BIGINT, vendor_name TEXT, sow_id BIGINT, sow_number TEXT, payment_status TEXT, invoice_amount FLOAT);\n</code></pre> </li> </ol> <p>Congratulations! You have successfully loaded your graph database with data from PostgreSQL.</p>"},{"location":"workshop/docs/07-Evaluation/08/","title":"6.8 Update Copilot With GraphRAG","text":"<p>The next step is to update your API to use GraphRAG for data retrieval. You will update how your copilot finds and retrieves unpaid invoices for this.</p>"},{"location":"workshop/docs/07-Evaluation/08/#review-the-function","title":"Review the function","text":"<p>Following the function calling pattern used by your LangChain agent to retrieve data from the database, you will use a Python function to execute openCypher queries against your graph database from your copilot. Within the <code>src/api/app/functions/chat_functions.py</code> file, the <code>get_unpaid_invoices_for_vendor</code> function has been provided for executing cypher queries. Open it now in Visual Studio Code and explore the code with the function. You can also expand the section below to see the code inline.</p> GraphRAG code src/api/app/functions/chat_functions.py<pre><code>async def get_unpaid_invoices_for_vendor(self, vendor_id: int):\n    \"\"\"\n    Retrieves a list of unpaid invoices for a specific vendor using a graph query.\n    \"\"\"\n    # Define the graph query\n    graph_query = f\"\"\"SELECT * FROM ag_catalog.cypher('vendor_graph', $$\n    MATCH (v:vendor {{id: '{vendor_id}'}})-[rel:has_invoices]-&gt;(s:sow)\n    WHERE rel.payment_status &lt;&gt; 'Paid'\n    RETURN v.id AS vendor_id, v.name AS vendor_name, s.id AS sow_id, s.number AS sow_number, rel.id AS invoice_id, rel.number AS invoice_number, rel.payment_status AS payment_status\n    $$) as (vendor_id BIGINT, vendor_name TEXT, sow_id BIGINT, sow_number TEXT, invoice_id BIGINT, invoice_number TEXT, payment_status TEXT);\n    \"\"\"\n    rows = await self.__execute_graph_query(graph_query)\n    return [dict(row) for row in rows]\n</code></pre> <ol> <li> <p>Define grapy query (line 6): Creates the cypher query that will be used to look up unpaid invoices for the specified <code>vendor_id</code></p> </li> <li> <p>Execute cypher query (lines 12): The cyper query is sent to the database for execution, using the <code>__execute_graph_query()</code> function.</p> </li> <li> <p>The <code>__execute_graph_query()</code> function, starting on line 25 on the <code>chat_functions.py</code> file, runs the query against the <code>ag_catalog</code> schema, which contains the graph database. To enable this, it also includes a <code>SET</code> query prior to running the graph query to add <code>ag_catalog</code> to the <code>search_path</code> in the connection.</p> </li> <li> <p>Return the results (lines 13): The query results are extracted and returned to the LLM.</p> </li> </ol>"},{"location":"workshop/docs/07-Evaluation/08/#implement-graphrag","title":"Implement GraphRAG","text":"<p>To implement GraphRAG functionality in your copilot, you must include the <code>get_unpaid_invoices_for_vendor</code> function in your LangChain agent's <code>tools</code> collection. You will add this function to the list of available tools to your agent.</p> <ol> <li> <p>In the VS Code Explorer, navigate to the <code>src/api/app/routers</code> folder and open the <code>completions.py</code> file.</p> </li> <li> <p>Within the <code>tools</code> array, locate the following line (line 75):</p> Python<pre><code>StructuredTool.from_function(coroutine=cf.get_invoices),\n</code></pre> </li> <li> <p>Insert the following code on the line just below that:</p> <p>Insert the following Python code to add the GraphRAG function!</p> Python<pre><code>StructuredTool.from_function(coroutine=cf.get_unpaid_invoices_for_vendor),\n</code></pre> </li> <li> <p>Your new <code>tools</code> array should look like this:</p> Python<pre><code># Define tools for the agent to retrieve data from the database\ntools = [\n    # Hybrid search functions\n    StructuredTool.from_function(coroutine=cf.find_invoice_line_items),\n    StructuredTool.from_function(coroutine=cf.find_invoice_validation_results),\n    StructuredTool.from_function(coroutine=cf.find_milestone_deliverables),\n    StructuredTool.from_function(coroutine=cf.find_sow_chunks_with_semantic_ranking),\n    StructuredTool.from_function(coroutine=cf.find_sow_validation_results),\n    # Get invoice data functions\n    StructuredTool.from_function(coroutine=cf.get_invoice_id),\n    StructuredTool.from_function(coroutine=cf.get_invoice_line_items),\n    StructuredTool.from_function(coroutine=cf.get_invoice_validation_results),\n    StructuredTool.from_function(coroutine=cf.get_invoices),\n    StructuredTool.from_function(coroutine=cf.get_unpaid_invoices_for_vendor),\n    # Get SOW data functions\n    StructuredTool.from_function(coroutine=cf.get_sow_chunks),\n    StructuredTool.from_function(coroutine=cf.get_sow_id),\n    StructuredTool.from_function(coroutine=cf.get_sow_milestones),\n    StructuredTool.from_function(coroutine=cf.get_milestone_deliverables),\n    StructuredTool.from_function(coroutine=cf.get_sow_validation_results),\n    StructuredTool.from_function(coroutine=cf.get_sows),\n    # Get vendor data functions\n    StructuredTool.from_function(coroutine=cf.get_vendors)\n]\n</code></pre> </li> <li> <p>Save the <code>completions.py</code> file.</p> </li> </ol>"},{"location":"workshop/docs/07-Evaluation/08/#test-with-vs-code","title":"Test with VS Code","text":"<p>As you have done previously, you will test your updates using Visual Studio Code.</p>"},{"location":"workshop/docs/07-Evaluation/08/#start-the-api","title":"Start the API","text":"<p>Follow the steps below to start a debug session for the API in VS Code.</p> <ol> <li> <p>In Visual Studio Code Run and Debug panel, select the API Debugger option for your OS from the debug configurations dropdown list.</p> <p></p> </li> <li> <p>Select the Start Debugging button (or press F5 on your keyboard).</p> <p></p> </li> <li> <p>Wait for the API application to start completely, indicated by an <code>Application startup complete.</code> message in the terminal output.</p> <p></p> </li> </ol>"},{"location":"workshop/docs/07-Evaluation/08/#start-the-portal","title":"Start the Portal","text":"<p>With the API running, you can start a second debug session in VS Code for the Portal project.</p> <ol> <li> <p>Return to the Run and Debug panel in Visual Studio Code and select the Portal Debugger option from the debug configurations dropdown list.</p> <p></p> </li> <li> <p>Select the Start Debugging button (or press F5 on your keyboard).</p> <p></p> </li> <li> <p>This should launch the Woodgrove Bank Contract Management Portal in a new browser window (http://localhost:3000/).</p> </li> <li> <p>In the copilot chat on the Dashboard page, enter the following message and send it:</p> <p>Paste the following prompt into the copilot chat box!</p> <pre><code>Tell me about the accuracy of unpaid invoices from Adatum.\n</code></pre> </li> <li> <p>Observe the results provided using GraphRAG.</p> <p>GraphRAG improves accuracy</p> <p>Add a breakpoint in the <code>get_unpaid_invoices_for_vendor</code> function in the <code>chat_functions.py</code> file. The breakpoint will allow you to see the graph query executing and enable you to step through the remaining function calls to observe that the invoice validation results are only retrieved for the unpaid invoices. This precision reduces the data returned from the database and allows the RAG pattern to only receive the data it needs to generate a response.</p> </li> </ol> <p>Congratulations! You just learned how to leverage the GraphRAG capabilities of Azure Database for PostgreSQL and AGE!</p>"},{"location":"workshop/docs/Tear-Down/","title":"Cleanup Resources","text":""},{"location":"workshop/docs/Tear-Down/#give-us-a-on-github","title":"Give us a \u2b50\ufe0f on GitHub","text":"<p>FOUND THIS WORKSHOP AND SAMPLE USEFUL? MAKE SURE YOU GET UPDATES.</p> <p>The Build Your Own Advanced AI Copilot with Postgres sample is an actively updated project that will reflect the latest features and best practices for code-first development of RAG-based copilots on the Azure AI platform. Visit the repo or click the button below, to give us a \u2b50\ufe0f.</p> <p> Give the PostgreSQL Solution Accelerator a Star!</p>"},{"location":"workshop/docs/Tear-Down/#provide-feedback","title":"Provide Feedback","text":"<p>Have feedback that can help us make this lab better for others? Open an issue and let us know.</p>"},{"location":"workshop/docs/Tear-Down/#clean-up","title":"Clean-up","text":"<p>Once you have completed this workshop, delete the Azure resources you created. You are charged for the configured capacity, not how much the resources are used. Follow these instructions to delete your resource group and all resources you created for this solution accelerator.</p> <ol> <li> <p>In VS Code, open a new integrated terminal prompt.</p> </li> <li> <p>At the terminal prompt, execute the following command to delete the resources created by the deployment script:</p> <p>Execute the following Azure Developer CLI command to delete resources!</p> <pre><code>azd down --purge\n</code></pre> <p>The <code>--purge</code> flag purges the resources that provide soft-delete functionality in Azure, including Azure KeyVault and Azure OpenAI. This flag is required to remove all resources completely.</p> </li> <li> <p>In the terminal window, you will be shown a list of the resources that will be deleted and prompted about continuing. Enter \"y\" at the prompt to being the resource deletion.</p> </li> </ol>"},{"location":"workshop/docs/Tear-Down/#persist-changes-to-github","title":"Persist changes to GitHub","text":"<p>If you want to save any changes you have made to files, use the Source Control tool in VS Code to commit and push your changes to your fork of the GitHub repo.</p>"},{"location":"workshop/docs/support-docs/AppAuthentication/","title":"AppAuthentication","text":""},{"location":"workshop/docs/support-docs/AppAuthentication/#add-authentication-in-azure-app-service-configuration","title":"Add Authentication in Azure App Service configuration","text":"<ol> <li>Click on <code>Authentication</code> from left menu.</li> </ol> <ol> <li>Click on <code>+ Add Provider</code> to see a list of identity providers.</li> </ol> <ol> <li>Click on <code>+ Add Provider</code> to see a list of identity providers.</li> </ol> <ol> <li>Select the first option <code>Microsoft Entra Id</code> from the drop-down list.</li> </ol> <ol> <li>Accept the default values and click on <code>Add</code> button to go back to the previous page with the identity provider added.</li> </ol> <p>### Creating a new App Registration 1. Click on <code>Home</code> and select <code>Microsoft Entra ID</code>.</p> <p></p> <ol> <li>Click on <code>App registrations</code>.</li> </ol> <p></p> <ol> <li>Click on <code>+ New registration</code>.</li> </ol> <p></p> <ol> <li>Provide the <code>Name</code>, select supported account types as <code>Accounts in this organizational directory only(Contoso only - Single tenant)</code>, select platform as <code>Web</code>, enter/select the <code>URL</code> and register.</li> </ol> <p></p> <ol> <li>After application is created sucessfully, then click on <code>Add a Redirect URL</code>.</li> </ol> <p></p> <ol> <li>Click on <code>+ Add a platform</code>.</li> </ol> <p></p> <ol> <li>Click on <code>Web</code>.</li> </ol> <p></p> <ol> <li>Enter the <code>web app URL</code> (Provide the app service name in place of XXXX) and Save. Then go back to [Step 4] and follow from Point 4 choose <code>Pick an existing app registration in this directory</code> from the Add an Identity Provider page and provide the newly registered App Name.</li> </ol> <p></p>"},{"location":"workshop/docs/support-docs/AzureAccountSetUp/","title":"AzureAccountSetUp","text":""},{"location":"workshop/docs/support-docs/AzureAccountSetUp/#azure-account-setup","title":"Azure account setup","text":"<ol> <li>Sign up for a free Azure account and create an Azure Subscription.</li> <li>Check that you have the necessary permissions:<ul> <li>Your Azure account must have <code>Microsoft.Authorization/roleAssignments/write</code> permissions, such as Role Based Access Control Administrator, User Access Administrator, or Owner.</li> <li>Your Azure account also needs <code>Microsoft.Resources/deployments/write</code> permissions on the subscription level.</li> </ul> </li> </ol> <p>You can view the permissions for your account and subscription by following the steps below:  - Navigate to the Azure Portal and click on <code>Subscriptions</code> under 'Navigation'  - Select the subscription you are using for this accelerator from the list.      - If you try to search for your subscription and it does not come up, make sure no filters are selected. - Select <code>Access control (IAM)</code> and you can see the roles that are assigned to your account for this subscription.      - If you want to see more information about the roles, you can go to the <code>Role assignments</code>      tab and search by your account name and then click the role you want to view more information about.</p>"},{"location":"workshop/docs/support-docs/AzureGPTQuotaSettings/","title":"AzureGPTQuotaSettings","text":""},{"location":"workshop/docs/support-docs/AzureGPTQuotaSettings/#how-to-check-update-quota","title":"How to Check &amp; Update Quota","text":"<ol> <li>Navigate to the Azure AI Foundry portal.  </li> <li>Select the AI Project associated with this accelerator.  </li> <li>Go to the <code>Management Center</code> from the bottom-left navigation menu.  </li> <li>Select <code>Quota</code> </li> <li>Click on the <code>GlobalStandard</code> dropdown.  </li> <li>Select the required GPT model (<code>GPT-4, GPT-4o, GPT-4o Mini</code>) or Embeddings model (<code>text-embedding-ada-002</code>).  </li> <li>Choose the region where the deployment is hosted.  </li> <li>Request More Quota or delete any unused model deployments as needed.  </li> </ol>"},{"location":"workshop/docs/support-docs/AzureSemanticSearchRegion/","title":"AzureSemanticSearchRegion","text":""},{"location":"workshop/docs/support-docs/AzureSemanticSearchRegion/#select-a-region-where-semantic-search-availability-is-available-before-proceeding-with-the-deployment","title":"Select a region where Semantic Search Availability is available before proceeding with the deployment.","text":"<p>Steps to Check Semantic Search Availability 1. Open the Semantic Search Availability page. 2. Scroll down to the \"Availability by Region\" section. 3. Use the table to find supported regions for Azure AI Search and its Semantic Search feature. 4. If your target region is not listed, choose a supported region for deployment.</p>"},{"location":"workshop/docs/support-docs/ConversationalDataFormat/","title":"ConversationalDataFormat","text":""},{"location":"workshop/docs/support-docs/ConversationalDataFormat/#data-upload-format","title":"Data upload format","text":""},{"location":"workshop/docs/support-docs/ConversationalDataFormat/#audio-file-format","title":"Audio File Format","text":"<p>Azure AI Speech Service is utilized for transcription of conversation audio files. The code is currently configured to support WAV files only, but the code can be modified to support other formats supported by Azure Speech Service. Full details can be found here.</p> <p>We have seen successful transcription of files up to 15MB in size but some very large files may experience processing challenges.</p> <p>Contact center conversations may be uploaded directly as audio to the <code>cu_audio_files_all</code> folder in the Fabric lakehouse. o ensure proper processing, all audio files must follow the specified naming convention. Below is an example of the required format:</p> Text Only<pre><code>convo_03b0e193-5b55-42d3-a258-b0ff9336ae18_2024-12-05 18_00_00.wav\n</code></pre>"},{"location":"workshop/docs/support-docs/ConversationalDataFormat/#naming-convention-breakdown","title":"Naming Convention Breakdown","text":"<ol> <li><code>convo</code>: The prefix that indicates the file contains a contact center conversation.</li> <li>Conversation ID (GUID): A unique identifier for the conversation, represented as a globally unique identifier (GUID).</li> <li>Date and Timestamp: The date and time of the conversation, formatted as <code>YYYY-MM-DD HH_MM_SS</code>.</li> </ol>"},{"location":"workshop/docs/support-docs/ConversationalDataFormat/#json-file-format","title":"JSON File Format","text":"<p>Below is a sample structure of a conversation file. Each sentence or phrase is an individual node followed by summary information for the entire call. These formatted conversation files are smaller size, less costly to process, and faster to process. JSON<pre><code>{\n    \"ClientId\": \"10003\",\n    \"ConversationId\": \"0a7b112e-3e59-4132-82a9-b399f782e859\",\n    \"StartTime\": \"2024-11-22 03:00:00\",\n    \"EndTime\": \"2024-11-22 03:14:00\",\n    \"Content\": \" Agent: Good day, thank you for calling Contoso Inc. This is Chris speaking, how can I assist you today?\\n\\nCustomer (Susan): Hi Chris, I've been having some network coverage and connectivity issues lately. It's been a bit frustrating.\\n\\nAgent: I'm sorry to hear that, Susan. I understand how frustrating it can be when you have connectivity issues. Can you tell me more about the issue you are facing and when it first started?\\n\\nCustomer (Susan): Sure. It's been happening for about a week now. I mostly experience lose of signal when I'm at home. However, the issue doesn't affect my mobile data.\\n\\nAgent: Thank you for sharing that information, Susan. Just to confirm, this issue only occurs when you are at home, correct?\\n\\nCustomer (Susan): Yes, that's correct.\\n\\nAgent: Alright, let's try and see if we can find a resolution for you. Firstly, can you tell me if you have checked your router and modem? Sometimes, network issues can be due to router configuration or problems with the modem.\\n\\nCustomer (Susan): I haven't really checked on that. I'm not really tech-savvy, to be honest.\\n\\nAgent: No problem at all, Susan. I'll guide you through the steps to check your router and modem. Firstly, could you please ensure that your router and modem are properly plugged in and switched on?\\n\\nCustomer (Susan): Just a moment... Okay, everything seems to be plugged in properly, and both the modem and the router are turned on.\\n\\nAgent: Great! Now, could you try unplugging both the modem and router for 10 seconds and then plugging them back in? This process is called power cycling, and it can help reset your devices.\\n\\nCustomer (Susan): Alright, I've done that. Let me check if the internet is working... No, unfortunately, the situation hasn't improved.\\n\\nAgent: Thank you for trying that. Since the issue still persists, I will schedule a technician visit for you. Can you please provide your address?\\n\\nCustomer (Susan): My address is 123 Elm Street.\\n\\nAgent: Thank you, Susan. I have scheduled a technician to visit your location tomorrow between 9am and 11am. They will contact you 15 minutes before arrival.\\n\\nCustomer (Susan): Thank you for arranging that quickly, Chris.\\n\\nAgent: You're welcome, Susan. I apologize for the inconvenience caused. Is there anything else I can assist you with today?\\n\\nCustomer (Susan): No, that covers it. Thank you for your help, Chris.\\n\\nAgent: It's been a pleasure assisting you, Susan. We at Contoso Inc. value your satisfaction and apologize again for the inconvenience. Feel free to call us again if you need any more help. Have a great day!\\n\\nCustomer (Susan): Thank you, you too!\\n\\nAgent: This call is now complete. Thank you for choosing Contoso Inc. Goodbye, Susan!\"\n}\n</code></pre></p>"},{"location":"workshop/docs/support-docs/CustomizingAzdParameters/","title":"CustomizingAzdParameters","text":""},{"location":"workshop/docs/support-docs/CustomizingAzdParameters/#optional-customizing-resource-names","title":"[Optional]: Customizing resource names","text":"<p>By default this template will use the environment name as the prefix to prevent naming collisions within Azure. The parameters below show the default values. You only need to run the statements below if you need to change the values. </p> <p>To override any of the parameters, run <code>azd env set &lt;key&gt; &lt;value&gt;</code> before running <code>azd up</code>. On the first azd command, it will prompt you for the environment name. Be sure to choose 3-20 charaters alphanumeric unique name. </p> <p>Change the Content Understanding Location (allowed values: Sweden Central, Australia East)</p> Bash<pre><code>azd env set AZURE_ENV_CU_LOCATION 'swedencentral'\n</code></pre> <p>Change the Secondary Location (example: eastus2, westus2, etc.)</p> Bash<pre><code>azd env set AZURE_ENV_SECONDARY_LOCATION eastus2\n</code></pre> <p>Change the Model Deployment Type (allowed values: Standard, GlobalStandard)</p> Bash<pre><code>azd env set AZURE_ENV_MODEL_DEPLOYMENT_TYPE GlobalStandard\n</code></pre> <p>Set the Model Name (allowed values: gpt-4o-mini, gpt-4o, gpt-4)</p> Bash<pre><code>azd env set AZURE_ENV_MODEL_NAME gpt-4o-mini\n</code></pre> <p>Change the Model Capacity (choose a number based on available GPT model capacity in your subscription)</p> Bash<pre><code>azd env set AZURE_ENV_MODEL_CAPACITY 30\n</code></pre> <p>Change the Embedding Model </p> Bash<pre><code>azd env set AZURE_ENV_EMBEDDING_MODEL_NAME text-embedding-ada-002\n</code></pre> <p>Change the Embedding Deployment Capacity (choose a number based on available embedding model capacity in your subscription)</p> Bash<pre><code>azd env set AZURE_ENV_EMBEDDING_MODEL_CAPACITY 80\n</code></pre>"},{"location":"workshop/docs/support-docs/DeleteResourceGroup/","title":"Deleting Resources After a Failed Deployment in Azure Portal","text":"<p>If your deployment fails and you need to clean up the resources manually, follow these steps in the Azure Portal.</p>"},{"location":"workshop/docs/support-docs/DeleteResourceGroup/#1-navigate-to-the-azure-portal","title":"1. Navigate to the Azure Portal","text":"<ol> <li>Open Azure Portal.</li> <li>Sign in with your Azure account.</li> </ol>"},{"location":"workshop/docs/support-docs/DeleteResourceGroup/#2-find-the-resource-group","title":"2. Find the Resource Group","text":"<ol> <li>In the search bar at the top, type \"Resource groups\" and select it.</li> <li>Locate the resource group associated with the failed deployment.</li> </ol>"},{"location":"workshop/docs/support-docs/DeleteResourceGroup/#3-delete-the-resource-group","title":"3. Delete the Resource Group","text":"<ol> <li>Click on the resource group name to open it.</li> <li>Click the Delete resource group button at the top.</li> </ol> <ol> <li>Type the resource group name in the confirmation box and click Delete.</li> </ol> <p>\ud83d\udccc Note: Deleting a resource group will remove all resources inside it.</p>"},{"location":"workshop/docs/support-docs/DeleteResourceGroup/#4-delete-individual-resources-if-needed","title":"4. Delete Individual Resources (If Needed)","text":"<p>If you don\u2019t want to delete the entire resource group, follow these steps:</p> <ol> <li>Open Azure Portal and go to the Resource groups section.</li> <li>Click on the specific resource group.</li> <li>Select the resource you want to delete (e.g., App Service, Storage Account).</li> <li>Click Delete at the top.</li> </ol> <p></p>"},{"location":"workshop/docs/support-docs/DeleteResourceGroup/#5-verify-deletion","title":"5. Verify Deletion","text":"<ul> <li>After a few minutes, refresh the Resource groups page.</li> <li>Ensure the deleted resource or group no longer appears.</li> </ul> <p>\ud83d\udccc Tip: If a resource fails to delete, check if it's locked under the Locks section and remove the lock.</p>"},{"location":"workshop/docs/support-docs/Fabric_deployment/","title":"Fabric deployment","text":""},{"location":"workshop/docs/support-docs/Fabric_deployment/#how-to-customize","title":"How to customize","text":"<p>If you'd like to customize the solution accelerator, here are some ways you might do that: - Ingest your own audio conversation files by uploading them into the <code>cu_audio_files_all</code> lakehouse folder and run the data pipeline - Deploy with Microsoft Fabric by following the steps in Fabric_deployment.md</p> <ol> <li> <p>Create Fabric workspace</p> <ol> <li>Navigate to (Fabric Workspace)</li> <li>Click on Data Engineering experience</li> <li>Click on Workspaces from left Navigation</li> <li>Click on + New Workspace<ol> <li>Provide Name of Workspace </li> <li>Provide Description of Workspace (optional)</li> <li>Click Apply</li> </ol> </li> <li>Open Workspace</li> <li>Create Environment<ol> <li>Click <code>+ New Item</code> (in Workspace)</li> <li>Select Environment from list</li> <li>Provide name for Environment and click Create</li> <li>Select Public libraries in left panel</li> <li>Click Add from .yml</li> <li>Upload .yml from here</li> <li>Click Publish</li> </ol> </li> <li>Retrieve Workspace ID from URL, refer to documentation additional assistance (here)</li> </ol> <p>***Note: Wait until the Environment is finished publishing prior to proceeding witht the next steps.</p> </li> <li> <p>Deploy Fabric resources and artifacts</p> <ol> <li>Navigate to (Azure Portal)</li> <li>Click on Azure Cloud Shell in the top right of navigation Menu (add image)</li> <li>Run the run the following commands:  <ol> <li><code>az login</code> ***Follow instructions in Azure Cloud Shell for login instructions</li> <li><code>rm -rf ./Conversation-Knowledge-Mining-Solution-Accelerator</code></li> <li><code>git clone https://github.com/microsoft/Conversation-Knowledge-Mining-Solution-Accelerator</code></li> <li><code>cd ./Conversation-Knowledge-Mining-Solution-Accelerator/Deployment/scripts/fabric_scripts</code></li> <li><code>sh ./run_fabric_items_scripts.sh keyvault_param workspaceid_param solutionprefix_param</code><ol> <li>keyvault_param - the name of the keyvault that was created in Step 1</li> <li>workspaceid_param - the workspaceid created in Step 2</li> <li>solutionprefix_param - prefix used to append to lakehouse upon creation</li> </ol> </li> </ol> </li> </ol> </li> <li> <p>Add App Authentication</p> <p>Follow steps in App Authentication to configure authenitcation in app service.</p> </li> </ol>"},{"location":"workshop/docs/support-docs/Fabric_deployment/#upload-additional-files","title":"Upload additional files","text":"<p>All files WAV files can be uploaded in the corresponding Lakehouse in the data/Files folder:</p> <ul> <li>Audio (WAV files):   Upload Audio files in the cu_audio_files_all folder.</li> </ul>"},{"location":"workshop/docs/support-docs/Fabric_deployment/#post-deployment","title":"Post-deployment","text":"<ul> <li>To process additional files, manually execute the pipeline_notebook after uploading new files.</li> <li>The OpenAI prompt can be modified within the Fabric notebooks.</li> </ul>"},{"location":"workshop/docs/support-docs/quota_check/","title":"Quota check","text":""},{"location":"workshop/docs/support-docs/quota_check/#check-quota-availability-before-deployment","title":"Check Quota Availability Before Deployment","text":"<p>Before deploying the accelerator, ensure sufficient quota availability for the required model. Use one of the following scripts based on your needs:  </p> <ul> <li><code>quota_check_params.sh</code> \u2192 If you know the model and capacity required.  </li> <li><code>quota_check_all_regions.sh</code> \u2192 If you want to check available capacity across all regions for supported models.  </li> </ul>"},{"location":"workshop/docs/support-docs/quota_check/#if-using-azure-portal-and-cloud-shell","title":"If using Azure Portal and Cloud Shell","text":"<ol> <li>Navigate to the Azure Portal.</li> <li>Click on Azure Cloud Shell in the top right navigation menu.</li> <li>Run the appropriate command based on your requirement:  </li> </ol> <p>To check quota for a specific model and capacity: </p> Text Only<pre><code>```sh\ncurl -L -o quota_check_params.sh \"https://raw.githubusercontent.com/microsoft/Conversation-Knowledge-Mining-Solution-Accelerator/main/infra/scripts/quota_check_params.sh\"\nchmod +x quota_check_params.sh\n./quota_check_params.sh &lt;model_name:capacity&gt; [&lt;model_region&gt;] (e.g., gpt-4o-mini:30,text-embedding-ada-002:20 eastus)\n```\n</code></pre> <p>To check available quota across all regions for supported models: </p> Text Only<pre><code>```sh\ncurl -L -o quota_check_all_regions.sh \"https://raw.githubusercontent.com/microsoft/Conversation-Knowledge-Mining-Solution-Accelerator/main/infra/scripts/quota_check_all_regions.sh\"\nchmod +x quota_check_all_regions.sh\n./quota_check_all_regions.sh\n```\n</code></pre>"},{"location":"workshop/docs/support-docs/quota_check/#if-using-vs-code-or-codespaces","title":"If using VS Code or Codespaces","text":"<ol> <li>Run the appropriate script based on your requirement:  </li> </ol> <p>To check quota for a specific model and capacity: </p> Text Only<pre><code>```sh\n./quota_check_params.sh &lt;model_name:capacity&gt; [&lt;model_region&gt;] (e.g., gpt-4o-mini:30,text-embedding-ada-002:20 eastus)\n```\n</code></pre> <p>To check available quota across all regions for supported models: </p> Text Only<pre><code>```sh\n./quota_check_all_regions.sh\n```\n</code></pre> <ol> <li> <p>If you see the error <code>_bash: az: command not found_</code>, install Azure CLI:  </p> <p>Bash<pre><code>curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash\naz login\n</code></pre> 3. Rerun the script after installing Azure CLI.</p> <p>Parameters - <code>&lt;model_name:capacity&gt;</code>: The name and required capacity for each model, in the format model_name:capacity (e.g., gpt-4o-mini:30,text-embedding-ada-002:20). - <code>[&lt;model_region&gt;] (optional)</code>: The Azure region to check first. If not provided, all supported regions will be checked (e.g., eastus).</p> </li> </ol>"}]}